Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
booktitle = {3rd Int. Conf. Learn. Represent. ICLR 2015 - Conf. Track Proc.},
eprint = {1412.6980},
month = {dec},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Adam: A method for stochastic optimization}},
year = {2015}
}
@book{Cardinal2013,
abstract = {Chapter 1. Quick Summary -- 1.1. Overview of this book -- 1.2. Background knowledge -- 1.3. Supporting Web site -- 1.4. Quick summary: choosing and performing an ANOVA -- Chapter 2. Understanding the Basics -- 2.1. The basic logic and assumptions of ANOVA -- 2.1.1. A 'model' that describes and predicts some data -- 2.1.2. An example: data and a structural model -- 2.1.3. The null hypothesis -- 2.1.4. The assumptions of ANOVA -- 2.1.5. The logic of ANOVA -- 2.1.6. Expected mean squares (EMS) -- 2.2. The calculations behind a one-way ANOVA (one BS factor) -- 2.2.1. Calculations using means (preferred) or totals -- 2.2.2. Sums of squares: calculating SS[subscript total], SS[subscript treatment], and SS[subscript error] -- 2.2.3. Degrees of freedom -- 2.2.4. Mean squares -- 2.2.5. The F test -- 2.2.6. ANOVA summary table -- 2.2.7. SS[subscript treatment] for unequal sample sizes -- 2.2.8. Pictorial representation -- 2.3. Regression ANOVA: the other way to understand the logic -- 2.3.1. Linear regression in terms of sums of squares -- 2.3.2. Pictorial representation -- 2.3.3. Linear regression as an ANOVA -- 2.4. Factors versus covariates -- 2.5. Assumptions of ANOVA involving covariates -- 2.6. ANOVA with two between-subjects factors -- 2.6.1. Main effects, interactions, simple effects, and a structural model -- 2.6.2. Expected mean squares -- 2.6.3. Degrees of freedom -- 2.6.4. Sums of squares -- 2.6.5. Relating SS calculations to the structural model -- 2.6.6. ANOVA table -- 2.6.7. Pictorial representation -- 2.7. Within-subjects (repeated measures) ANOVA -- 2.7.1. Structural model -- 2.7.2. Degrees of freedom -- 2.7.3. Sums of squares -- 2.7.4. EMS and ANOVA summary table -- 2.8. Assumptions of within-subjects ANOVA: 'sphericity' -- 2.8.1. Short version -- 2.8.2. Long version -- 2.9. Missing data in designs involving within-subjects factors -- 2.10. Mixed ANOVA (with both BS and WS factors) -- 2.10.1. Structural model -- 2.10.2. Degrees of freedom -- 2.10.3. Sums of squares -- 2.10.4. ANOVA table -- 2.11. Fixed and random factors -- 2.12. Additional material (ADVANCED) -- 2.12.1. Notation for variances and mean squares in EMS expressions -- 2.12.2. Expected value of F -- 2.12.3. A X[superscript 2] distribution is the sum of squared z scores -- 2.12.4. Relationship between the sample variance and the x[superscript 2] distribution -- 2.12.5. The F distribution -- 2.12.6. Comparing two variances with an F test -- 2.12.7. ANOVA: comparing two mean-square values with an F test -- 2.12.8. Relating SS calculations to the model for one-way ANOVA -- Chapter 3. Practical Analysis -- 3.1. Reminder: assumptions of ANOVA -- 3.2. Reminder: assumption of ANOVA with WS factors -- 3.3. Consequences of violating the assumptions of ANOVA -- 3.4. Exploratory data analysis and transformations -- 3.4.1. Plot your data -- 3.4.2. Outliers -- 3.4.3. Transformations -- 3.5. Performing the ANOVA -- 3.6. Residuals -- 3.7. Further analysis: after the ANOVA has been run -- 3.7.1. Main effects, interactions, and simple effects revisited -- 3.7.2. Conducting simple-effects analysis -- 3.7.3. A fallacy to avoid: when A differs from C but B doesn't -- 3.7.4. A fallacy to avoid: simple effects without interactions -- 3.7.5. Determining the effects of a factor with {\textgreater}2 levels -- 3.7.6. Multiple comparisons: a problem -- 3.7.7. Post hoc tests: a problem -- 3.7.8. The special case of three groups: multiple t tests are OK -- 3.7.9. Otherwise: a variety of post hoc tests -- 3.7.10. Post hoc tests for within-subject factors -- 3.7.11. A priori tests: planned contrasts -- 3.7.12. Apparent inconsistency between the F test and post hoc tests -- 3.7.13. SPSS's default pairwise comparison post hoc tests -- 3.8. Drawing pictures: error bars for different comparisons -- 3.8.1. Error bars for t tests: between-subjects comparisons -- 3.8.2. Error bars for t tests: within-subjects comparisons -- 3.8.3. Error bars for an ANOVA: between-subjects designs -- 3.8.4. Error bars for an ANOVA: effects in mixed designs -- 3.9. Summarizing your methods: a writing guide -- 3.10. Additional material (ADVANCED) -- 3.10.1. Error bars for t tests: between-subjects comparisons: SEMs -- 3.10.2. Error bars for t tests: between-subjects comparisons: CIs -- 3.10.3. Error bars for t tests: between-subjects comparisons: SDs -- 3.10.4. Obtaining SEDs from an ANOVA table -- Chapter 4. Pitfalls and Common Issues -- 4.1. Time in within-subjects (repeated measures) designs -- 4.2. Analysis of pre-test versus post-test data -- 4.3. Observing subjects repeatedly to increase power -- 4.4. 'It's significant in this subject...' -- 4.5. Should I add/remove a factor? Full and reduced models -- 4.6. Should I add/remove/collapse over levels of a factor? -- 4.6.1. Adding and removing levels by adding new observations -- 4.6.2. Collapsing over or subdividing levels -- Chapter 5. Using SPSS for ANOVA -- 5.1. Running ANOVAs using SPSS -- 5.1.1. Analysis of variance -- 5.1.2. Organizing and reorganizing your data -- 5.1.3. Syntax -- 5.1.4. Plots -- 5.1.5. Options, including homogeneity-of-variance tests -- 5.1.6. Post hoc tests -- 5.2. Interpreting the output -- Tip: pairwise comparisons for interactions -- 5.3. Further analysis: selecting cases -- 5.4. The 'intercept', 'total', and 'corrected total' terms -- Chapter 6. Contrasts and Trends -- 6.1. Contrasts -- 6.1.1. About linear contrasts -- 6.1.2. Type I error rates with planned contrasts -- 6.1.3. Orthogonal contrasts -- 6.1.4. Linear contrasts in SPSS -- 6.1.5. Contrasts in multifactor designs-an overview -- 6.2. Trend analysis: the effects of quantitative factors -- 6.2.1. Trends -- 6.2.2. Trend analysis in SPSS -- 6.2.3. Trend analysis, multiple regression, and polynomial ANCOVA -- Chapter 7. Advanced Topics -- 7.1. Rules for calculating sums of squares -- 7.1.1. Partitioning sums of squares -- 7.1.2. General rule for calculating sums of squares -- 7.2. Rules for calculating degrees of freedom -- 7.3. Expected mean squares (EMS) and error terms -- 7.3.1. Rules for obtaining expected mean squares (EMS) -- 7.3.2. Choosing an error term -- 7.3.3. Error terms in models including random factors (complicated) -- 7.3.4. Pooling error terms -- 7.4. Unequal group sizes and non-orthogonal sums of squares -- 7.4.1. Proportional cell frequencies -- 7.4.2. Disproportionate cell frequencies-a problem -- 7.4.3. Correlated predictors in general-a problem -- 7.5. How computers perform ANOVA: general linear models -- 7.5.1. The basic idea of a GLM, illustrated with multiple regression -- 7.5.2. Using a GLM for simple ANOVA: the design matrix -- 7.5.3. Example of a GLM for a one-way ANOVA -- 7.5.4. GLM for two-way ANOVA and beyond -- 7.5.5. F statistics for GLMs: comparing full and reduced models -- 7.5.6. An overview of GLM designs -- 7.5.7. GLM designs involving random effects -- 7.5.8. A hint at multivariate analysis: MANOVA -- 7.5.9. Linear contrasts with a GLM -- 7.5.10. GLMs and custom contrasts in SPSS -- 7.6. Effect size -- 7.6.1. Effect size in the language of multiple regression -- 7.6.2. Effect size in the language of ANOVA -- Chapter 8. Specific Designs -- 8.1. One between-subjects (BS) factor -- 8.2. Two BS factors -- 8.3. Three BS factors -- 8.4. One within-subjects (WS) factor -- 8.5. Two WS factors -- 8.6. Three WS factors -- 8.7. One BS and one WS factor -- 8.8. Two BS factors and one WS factor -- 8.9. One BS factor and two WS factors -- 8.10. Other ANOVA designs with BS and/or WS factors -- 8.11. One BS covariate (linear regression) -- 8.12. One BS covariate and one BS factor -- 8.12.1. The covariate and factor do not interact -- 8.12.2. The covariate and factor interact -- 8.13. One BS covariate and two BS factors -- 8.14. Two or more BS covariates (multiple regression) -- 8.15. Two or more BS covariates and one or more BS factors -- 8.16. One WS covariate -- 8.17. One WS covariate and one BS factor -- 8.17.1. The covariate and factor do not interact -- 8.17.2. The covariate and factor interact -- 8.18. Hierarchical designs -- 8.18.1. Subjects within groups within treatments (S/G/A) -- 8.18.2. Groups versus individuals. 8.18.3. Adding a further within-group, BS variable (S/GB/A) -- 8.18.4. Adding a within-subjects variable (US/GB/A) -- 8.18.5. Nesting within-subjects variables, such as V/US/A -- 8.18.6. The split-split plot design -- 8.18.7. Three levels of relatedness -- 8.19. Latin square designs -- 8.19.1. Latin squares in experimental design -- 8.19.2. The analysis of a basic Latin square -- 8.19.3. A x B interactions in a single Latin square -- 8.19.4. More subjects than rows: (a) using several squares -- 8.19.5. More subjects than rows: (b) using one square several times -- 8.19.6. BS designs using Latin squares (fractional factorial designs) -- 8.19.7. Several-squares design with a BS factor -- 8.19.8. Replicated-squares design with a BS factor -- 8.20. Agricultural terminology and designs -- Chapter 9. Mathematics -- 9.1. Matrices -- 9.1.1. Matrix notation -- 9.1.2. Matrix algebra -- 9.1.3. The inverse of a matrix -- 9.1.4. Matrix transposition -- 9.2. Calculus -- 9.2.1. Derivatives -- 9.2.2. Simple, non-trigonometric derivatives -- 9.2.3. Rules for differentiation -- 9.2.4. Derivatives of a vector function -- 9.2.5. Partial derivatives -- 9.2.6. The chain rule for partial derivatives -- 9.2.7. Illustrations of partial derivatives -- 9.3. Solving a GLM (an overdetermined system of equations) -- 9.4. Singular value decomposition to solve GLMs -- 9.4.1. Eigenvectors and eigenvalues -- 9.4.2. Singular value decomposition -- 9.4.3. An underdetermined set of equations: the role of expectations -- 9.5. Random variables, means, and variances -- 9.5.1. Summation -- 9.5.2. Random variables; definition of mean and variance -- 9.5.3. Continuous random variables -- 9.5.4. Expected values -- 9.5.5. Variance laws -- 9.5.6. Distribution of a set of means: the standard error of the mean -- 9.5.7. The sample mean and SD are unbiased estimators of [mu] a},
author = {Cardinal, Rudolf N. and Aitken, Michael R.F.},
booktitle = {ANOVA Behav. Sci. Res.},
doi = {10.4324/9780203763933},
isbn = {9780203763933},
issn = {00225347},
month = {apr},
pages = {1--448},
publisher = {Psychology Press},
title = {{ANOVA for the behavioural sciences researcher}},
url = {https://www.taylorfrancis.com/books/9780203763933},
year = {2013}
}
@inproceedings{Reitmaier2010,
abstract = {We describe and reflect on a method we used to evaluate usability and give insights on situated use of a mobile digital storytelling prototype. We report on rich data we gained by implementing this method and argue that we were able to learn more about our prototype, users, their needs, and their context, than we would have through other evaluation methods. We look at the usability problems we uncovered and discuss how our flexibility in field- testing allowed us to observe unanticipated usage, from which we were able to motivate future design directions. Finally, we reflect on the importance of spending time in-situ during all stages of design, especially when designing across cultures.},
address = {New York, New York, USA},
author = {Reitmaier, Thomas and Bidwell, Nicola J. and Marsden, Gary},
booktitle = {Proc. 12th Int. Conf. Hum. Comput. Interact. with Mob. devices Serv. - MobileHCI '10},
doi = {10.1145/1851600.1851649},
isbn = {9781605588353},
keywords = {HCI4D,digital storytelling,evaluation,probe,rural},
pages = {283},
publisher = {ACM Press},
title = {{Field testing mobile digital storytelling software in rural Kenya}},
url = {http://portal.acm.org/citation.cfm?doid=1851600.1851649},
year = {2010}
}
@article{Lakkaraju2016,
abstract = {Decision makers, such as doctors and judges, make crucial decisions such as recommending treatments to patients, and granting bails to defendants on a daily basis. Such decisions typically involve weighting the potential benefits of taking an action against the costs involved. In this work, we aim to automate this task of learning {\{}cost-effective, interpretable and actionable treatment regimes. We formulate this as a problem of learning a decision list -- a sequence of if-then-else rules -- which maps characteristics of subjects (eg., diagnostic test results of patients) to treatments. We propose a novel objective to construct a decision list which maximizes outcomes for the population, and minimizes overall costs. We model the problem of learning such a list as a Markov Decision Process (MDP) and employ a variant of the Upper Confidence Bound for Trees (UCT) strategy which leverages customized checks for pruning the search space effectively. Experimental results on real world observational data capturing treatment recommendations for asthma patients demonstrate the effectiveness of our approach.}},
archivePrefix = {arXiv},
arxivId = {1611.07663},
author = {Lakkaraju, Himabindu and Rudin, Cynthia},
eprint = {1611.07663},
isbn = {0364-5134; 0364-5134},
issn = {1938-7228},
month = {nov},
title = {{Learning Cost-Effective and Interpretable Regimes for Treatment Recommendation}},
url = {http://arxiv.org/abs/1611.07663},
year = {2016}
}
@article{Szalma2010a,
abstract = {BACKGROUND: The growing consensus that most valuable data source for biomedical discoveries is derived from human samples is clearly reflected in the growing number of translational medicine and translational sciences departments across pharma as well as academic and government supported initiatives such as Clinical and Translational Science Awards (CTSA) in the US and the Seventh Framework Programme (FP7) of EU with emphasis on translating research for human health.$\backslash$n$\backslash$nMETHODS: The pharmaceutical companies of Johnson and Johnson have established translational and biomarker departments and implemented an effective knowledge management framework including building a data warehouse and the associated data mining applications. The implemented resource is built from open source systems such as i2b2 and GenePattern.$\backslash$n$\backslash$nRESULTS: The system has been deployed across multiple therapeutic areas within the pharmaceutical companies of Johnson and Johnsons and being used actively to integrate and mine internal and public data to support drug discovery and development decisions such as indication selection and trial design in a translational medicine setting. Our results show that the established system allows scientist to quickly re-validate hypotheses or generate new ones with the use of an intuitive graphical interface.$\backslash$n$\backslash$nCONCLUSIONS: The implemented resource can serve as the basis of precompetitive sharing and mining of studies involving samples from human subjects thus enhancing our understanding of human biology and pathophysiology and ultimately leading to more effective treatment of diseases which represent unmet medical needs.},
author = {Szalma, S{\'{a}}ndor and Koka, Venkata and Khasanova, Tatiana and Perakslis, Eric D},
doi = {10.1186/1479-5876-8-68},
isbn = {1479-5876 (Linking)},
issn = {14795876},
journal = {J. Transl. Med.},
month = {jul},
pages = {68},
pmid = {20642836},
publisher = {BioMed Central},
title = {{Effective knowledge management in translational medicine}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20642836 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2914663},
volume = {8},
year = {2010}
}
@article{Powles2017,
abstract = {Data-driven tools and techniques, particularly machine learning methods that underpin artificial intelligence, offer promise in improving healthcare systems and services. One of the companies aspiring to pioneer these advances is DeepMind Technologies Limited, a wholly-owned subsidiary of the Google conglomerate, Alphabet Inc. In 2016, DeepMind announced its first major health project: a collaboration with the Royal Free London NHS Foundation Trust, to assist in the management of acute kidney injury. Initially received with great enthusiasm, the collaboration has suffered from a lack of clarity and openness, with issues of privacy and power emerging as potent challenges as the project has unfolded. Taking the DeepMind-Royal Free case study as its pivot, this article draws a number of lessons on the transfer of population-derived datasets to large private prospectors, identifying critical questions for policy-makers, industry and individuals as healthcare moves into an algorithmic age.},
author = {Powles, Julia and Hodson, Hal},
doi = {10.1007/s12553-017-0179-1},
issn = {21907196},
journal = {Health Technol. (Berl).},
keywords = {Artificial intelligence,Clinical care,Consent,Data protection,Machine learning,Power,Privacy,Regulation},
month = {dec},
number = {4},
pages = {351--367},
title = {{Google DeepMind and healthcare in an age of algorithms}},
url = {http://link.springer.com/10.1007/s12553-017-0179-1},
volume = {7},
year = {2017}
}
@article{Otasek2019,
abstract = {Cytoscape is one of the most successful network biology analysis and visualization tools, but because of its interactive nature, its role in creating reproducible, scalable, and novel workflows has been limited. We describe Cytoscape Automation (CA), which marries Cytoscape to highly productive workflow systems, for example, Python/R in Jupyter/RStudio. We expose over 270 Cytoscape core functions and 34 Cytoscape apps as REST-callable functions with standardized JSON interfaces backed by Swagger documentation. Independent projects to create and publish Python/R native CA interface libraries have reached an advanced stage, and a number of automation workflows are already published.},
author = {Otasek, David and Morris, John H. and Bou{\c{c}}as, Jorge and Pico, Alexander R. and Demchak, Barry},
doi = {10.1186/s13059-019-1758-4},
issn = {1474760X},
journal = {Genome Biol.},
keywords = {Cytoscape,Interoperability,Microservice,REST,Reproducibility,Service-oriented architecture,Workflow},
month = {dec},
number = {1},
pages = {185},
pmid = {31477170},
title = {{Cytoscape Automation: Empowering workflow-based network analysis}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/31477170 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC6717989 https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1758-4},
volume = {20},
year = {2019}
}
@article{Shugay2018,
abstract = {The ability to decode antigen specificities encapsu-lated in the sequences of rearranged T-cell recep-tor (TCR) genes is critical for our understanding of the adaptive immune system and promises signifi-cant advances in the field of translational medicine. Recent developments in high-throughput sequenc-ing methods (immune repertoire sequencing tech-nology, or RepSeq) and single-cell RNA sequencing technology have allowed us to obtain huge numbers of TCR sequences from donor samples and link them to T-cell phenotypes. However, our ability to anno-tate these TCR sequences still lags behind, owing to the enormous diversity of the TCR repertoire and the scarcity of available data on T-cell specificities. In this paper, we present VDJdb, a database that stores and aggregates the results of published T-cell speci-ficity assays and provides a universal platform that couples antigen specificities with TCR sequences. We demonstrate that VDJdb is a versatile instrument for the annotation of TCR repertoire data, enabling a concatenated view of antigen-specific TCR sequence motifs. VDJdb can be accessed at https://vdjdb.cdr3. net and https://github.com/antigenomics/vdjdb-db.},
author = {Shugay, Mikhail and Bagaev, Dmitriy V and Zvyagin, Ivan V and Vroomans, Renske M and Crawford, Jeremy Chase and Dolton, Garry and Komech, Ekaterina A and Sycheva, Anastasiya L and Koneva, Anna E and Egorov, Evgeniy S and Eliseev, Alexey V and {Van Dyk}, Ewald and Dash, Pradyot and Attaf, Meriem and Rius, Cristina and Ladell, Kristin and McLaren, James E and Matthews, Katherine K and Clemens, E Bridie and Douek, Daniel C and Luciani, Fabio and {Van Baarle}, Debbie and Kedzierska, Katherine and Kesmir, Can and Thomas, Paul G and Price, David A and Sewell, Andrew K and Chudakov, Dmitriy M},
doi = {10.1093/nar/gkx760},
isbn = {1362-4962 (Electronic) 0305-1048 (Linking)},
issn = {13624962},
journal = {Nucleic Acids Res.},
keywords = {antigens,donors,rna,sequence analysis,t-lymphocyte},
month = {jan},
number = {D1},
pages = {D419--D427},
pmid = {28977646},
publisher = {Oxford University Press},
title = {{VDJdb: A curated database of T-cell receptor sequences with known antigen specificity}},
url = {http://academic.oup.com/nar/article/46/D1/D419/4101254},
volume = {46},
year = {2018}
}
@article{Davies2018,
abstract = {Mendelian randomisation uses genetic variation as a natural experiment to investigate the causal relations between potentially modifiable risk factors and health outcomes in observational data. As with all epidemiological approaches, findings from Mendelian randomisation studies depend on specific assumptions. We provide explanations of the information typically reported in Mendelian randomisation studies that can be used to assess the plausibility of these assumptions and guidance on how to interpret findings from Mendelian randomisation studies in the context of other sources of evidence.},
author = {Davies, Neil M and Holmes, Michael V and {Davey Smith}, George},
doi = {10.1136/bmj.k601},
issn = {17561833},
journal = {BMJ},
month = {jul},
pages = {k601},
pmid = {30002074},
publisher = {British Medical Journal Publishing Group},
title = {{Reading Mendelian randomisation studies: A guide, glossary, and checklist for clinicians}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/30002074 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC6041728},
volume = {362},
year = {2018}
}
@book{Higgins2008,
abstract = {Preparing a Cochrane review / edited by Sally Green and Julian P.T. Higgins -- Maintaining reviews : updates, amendments and feedback / Julian P.T. Higgins, Sally Green, and Rob J.P.M. Scholten -- Guide to the contents of a Cochrane protocol and review / edited by Julian P.T. Higgins and Sally Green -- Defining the review question and developing criteria for including studies / edited by Denise O'Connor, Sally Green, and Julian P.T. Higgins -- Searching for studies / Carol Lefebvre, Eric Manheimer, and Julie Glanville -- Selecting studies and collecting data / edited by Julian P.T. Higgins and Jonathan J. Deeks -- Assessing risk of bias in included studies / edited by Julian P.T. Higgins and Douglas G. Altman -- Analysing data and undertaking meta-analyses / edited by Jonathan J. Deeks, Julian P.T. Higgins, and Douglas G. Altman -- Addressing reporting biases / edied by Jonathan A.C. Sterne, Matthias Egger, and David Moher -- Presenting results and 'summary of findings' tables / Holger J. Schünemann [and others] -- Interpreting results and drawing conclusions / Holger J. Schünemann [and others]. Includes non-randomized studies / Barnaby C. Reeves [and others] -- Adverse effects / Yoon K. Loke, Deirdre Price, and Andrew Herxheimer -- Incorporating economics evidence / Ian Shemilt [and others] -- Special topics in statistics / edited by Julian P.T. Higgins, Jonathan J. Deeks, and Douglas G. Altman -- Patient-reported outcomes / Donald L. Patrick, Gordon H. Guyatt, and Catherine Acquadro -- Reviews of individual patient data / Lesley A. Stewart, Jayne F. Tierney, Mike Clarke -- Prospective meta-analysis / Davina Ghersi, Jesse Berlin, and Lisa Askie -- Qualitative research and Cochrane reviews / Jane Noyes [and others] -- Reviews in health promotion and public health / edited by Rebecca Armstrong, Elizabeth Waters, and Jodie Doyle -- Overviews of reviews / Lorne A. Becker and Andrew D. Oxman.},
author = {Higgins, Julian P. T. and Green, Sally (Sally Elizabeth) and {Cochrane Collaboration.}},
isbn = {9781119964797},
pages = {649},
publisher = {Wiley-Blackwell},
title = {{Cochrane handbook for systematic reviews of interventions}},
year = {2008}
}
@article{Hoang2011,
abstract = {OBJECTIVE: To investigate whether the mortality gap has reduced in recent years between people with schizophrenia or bipolar disorder and the general population.$\backslash$n$\backslash$nDESIGN: Record linkage study.$\backslash$n$\backslash$nSETTING: English hospital episode statistics and death registration data for patients discharged 1999-2006.$\backslash$n$\backslash$nPARTICIPANTS: People discharged from inpatient care with a diagnosis of schizophrenia or bipolar disorder, followed for a year after discharge.$\backslash$n$\backslash$nMAIN OUTCOME MEASURES: Age standardised mortality ratios at each time, comparing the mortality in people with schizophrenia or bipolar disorder with mortality in the general population. Poisson test of trend was used to investigate trend in ratios over time.$\backslash$n$\backslash$nRESULTS: By 2006 standardised mortality ratios in the psychiatric cohorts were about double the population average. The mortality gap widened over time. For people discharged with schizophrenia, the ratio was 1.6 (95{\%} confidence interval 1.5 to 1.8) in 1999 and 2.2 (2.0 to 2.4) in 2006 (P {\textless} 0.001 for trend). For bipolar disorder, the ratios were 1.3 (1.1 to 1.6) in 1999 and 1.9 (1.6 to 2.2) in 2006 (P = 0.06 for trend). Ratios were higher for unnatural than for natural causes. About three quarters of all deaths, however, were certified as natural, and increases in ratios for natural causes, especially circulatory disease and respiratory diseases, were the main components of the increase in all cause mortality.$\backslash$n$\backslash$nCONCLUSIONS: The total burden of premature deaths from natural causes in people with schizophrenia or bipolar disorder is substantial. There is a need for better understanding of the reasons for the persistent and increasing gap in mortality between discharged psychiatric patients and the general population, and for continued action to target risk factors for both natural and unnatural causes of death in people with serious mental illness.},
author = {Hoang, Uy and Stewart, Robert and Goldacre, Michael J},
doi = {10.1136/bmj.d5422},
isbn = {1756-1833 (Electronic)$\backslash$r0959-535X (Linking)},
issn = {09598146},
journal = {BMJ},
month = {sep},
number = {7824},
pages = {d5422},
pmid = {21914766},
publisher = {British Medical Journal Publishing Group},
title = {{Mortality after hospital discharge for people with schizophrenia or bipolar disorder: Retrospective study of linked English hospital episode statistics, 1999-2006}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21914766 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3172324},
volume = {343},
year = {2011}
}
@inproceedings{Clarke2011,
abstract = {Statistical Model Checking is useful in situations where it is either inconvenient or impossible to build a concise representation of the global transition relation. This happens frequently with cyber-physical systems: Two examples are verifying Stateflow-Simulink models and in reasoning about biochemical reactions in Systems Biology. The main problem with Statistical Model Checking is caused by rare events. We describe how Statistical Model Checking works and demonstrate the problem with rare events. We then describe how Importance Sampling with the Cross-Entropy Technique can be used to address this problem. {\textcopyright} 2011 Springer-Verlag.},
author = {Clarke, Edmund M. and Zuliani, Paolo},
booktitle = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
doi = {10.1007/978-3-642-24372-1_1},
isbn = {9783642243714},
issn = {03029743},
month = {oct},
pages = {1--12},
publisher = {Springer, Berlin, Heidelberg},
title = {{Statistical model checking for cyber-physical systems}},
url = {http://link.springer.com/10.1007/978-3-642-24372-1{\_}1},
volume = {6996 LNCS},
year = {2011}
}
@article{Schwartz2019,
abstract = {Humans rely on properties of the materials that make up objects to guide our interactions with them. Grasping smooth materials, for example, requires care, and softness is an ideal property for fabric used in bedding. Even when these properties are not visual (e.g. softness is a physical property), we may still infer their presence visually. We refer to such material properties as visual material attributes. Recognizing these attributes in images can contribute valuable information for general scene understanding and material recognition. Unlike well-known object and scene attributes, visual material attributes are local properties with no fixed shape or spatial extent. We show that given a set of images annotated with known material attributes, we may accurately recognize the attributes from small local image patches. Obtaining such annotations in a consistent fashion at scale, however, is challenging. To address this, we introduce a method that allows us to probe the human visual perception of materials by asking simple yes/no questions comparing pairs of image patches. This provides sufficient weak supervision to build a set of attributes and associated classifiers that, while unnamed, serve the same function as the named attributes we use to describe materials. Doing so allows us to recognize visual material attributes without resorting to exhaustive manual annotation of a fixed set of named attributes. Furthermore, we show that this method may be integrated in the end-to-end learning of a material classification CNN to simultaneously recognize materials and discover their visual attributes. Our experimental results show that visual material attributes, whether named or automatically discovered, provide a useful intermediate representation for known material categories themselves as well as a basis for transfer learning when recognizing previously-unseen categories.},
archivePrefix = {arXiv},
arxivId = {1801.03127},
author = {Schwartz, Gabriel and Nishino, Ko},
doi = {10.1109/tpami.2019.2907850},
eprint = {1801.03127},
issn = {0162-8828},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
month = {jan},
pages = {1--1},
title = {{Recognizing Material Properties from Images}},
url = {http://arxiv.org/abs/1801.03127},
year = {2019}
}
@article{Yosinski2015,
abstract = {Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.},
archivePrefix = {arXiv},
arxivId = {1506.06579},
author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
eprint = {1506.06579},
journal = {Int. Conf. Mach. Learn. - Deep Learn. Work. 2015},
month = {jun},
pages = {12},
title = {{Understanding Neural Networks Through Deep Visualization}},
url = {http://arxiv.org/abs/1506.06579},
year = {2015}
}
@article{Orlowski2016,
abstract = {Despite growing interest in the promise of e-mental and well-being interventions, little supporting literature exists to guide their design and the evaluation of their effectiveness. Both participatory design (PD) and design thinking (DT) have emerged as approaches that hold significant potential for supporting design in this space. Each approach is difficult to definitively circumscribe, and as such has been enacted as a process, a mind-set, specific practices/techniques, or a combination thereof. At its core, however, PD is a design research tradition that emphasizes egalitarian partnerships with end users. In contrast, DT is in the process of becoming a management concept tied to innovation with strong roots in business and education. From a health researcher viewpoint, while PD can be reduced to a number of replicable stages that involve particular methods, techniques, and outputs, projects often take vastly different forms and effective PD projects and practice have traditionally required technology-specific (eg, computer science) and domain-specific (eg, an application domain, such as patient support services) knowledge. In contrast, DT offers a practical off-the-shelf toolkit of approaches that at face value have more potential to have a quick impact and be successfully applied by novice practitioners (and those looking to include a more human-centered focus in their work). Via 2 case studies we explore the continuum of similarities and differences between PD and DT in order to provide an initial recommendation for what health researchers might reasonably expect from each in terms of process and outcome in the design of e-mental health interventions. We suggest that the sensibilities that DT shares with PD (ie, deep engagement and collaboration with end users and an inclusive and multidisciplinary practice) are precisely the aspects of DT that must be emphasized in any application to mental health provision and that any technology development process must prioritize empathy and understanding over innovation for the successful uptake of technology in this space.},
author = {Orlowski, Simone and Matthews, Ben and Bidargaddi, Niranjan and Jones, Gabrielle and Lawn, Sharon and Venning, Anthony and Collin, Philippa},
doi = {10.2196/humanfactors.4336},
issn = {2292-9495},
journal = {JMIR Hum. Factors},
keywords = {design thinking,mental health,participatory design,technology},
month = {jan},
number = {1},
pages = {e4},
pmid = {27026210},
publisher = {JMIR Human Factors},
title = {{Mental Health Technologies: Designing With Consumers}},
url = {http://humanfactors.jmir.org/2016/1/e4/ http://www.ncbi.nlm.nih.gov/pubmed/27026210 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4811665},
volume = {3},
year = {2016}
}
@article{PerezArribas2018,
abstract = {Mobile technologies offer opportunities for higher resolution monitoring of health conditions. This opportunity seems of particular promise in psychiatry where diagnoses often rely on retrospective and subjective recall of mood states. However, getting actionable information from these rather complex time series is challenging, and at present the implications for clinical care are largely hypothetical. This research demonstrates that, with well chosen cohorts (of bipolar disorder, borderline personality disorder, and control) and modern methods, it is possible to objectively learn to identify distinctive behaviour over short periods (20 reports) that effectively separate the cohorts. Participants with bipolar disorder or borderline personality disorder and healthy volunteers completed daily mood ratings using a bespoke smartphone app for up to a year. A signature-based machine learning model was used to classify participants on the basis of the interrelationship between the different mood items assessed and to predict subsequent mood. The signature methodology was significantly superior to earlier statistical approaches applied to this data in distinguishing the participant three groups, clearly placing 75{\%} into their original groups on the basis of their reports. Subsequent mood ratings were correctly predicted with greater than 70{\%} accuracy in all groups. Prediction of mood was most accurate in healthy volunteers (89-98{\%}) compared to bipolar disorder (82-90{\%}) and borderline personality disorder (70-78{\%}).},
archivePrefix = {arXiv},
arxivId = {1707.07124},
author = {{Perez Arribas}, Imanol and Goodwin, Guy M. and Geddes, John R. and Lyons, Terry and Saunders, Kate E.A.},
doi = {10.1038/s41398-018-0334-0},
eprint = {1707.07124},
issn = {21583188},
journal = {Transl. Psychiatry},
month = {dec},
number = {1},
pages = {274},
pmid = {30546013},
title = {{A signature-based machine learning model for distinguishing bipolar disorder and borderline personality disorder}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/30546013 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC6293318 http://www.nature.com/articles/s41398-018-0334-0},
volume = {8},
year = {2018}
}
@article{Parisotto2016,
abstract = {Recent years have seen the proposal of a number of neural architectures for the problem of Program Induction. Given a set of input-output examples, these architectures are able to learn mappings that generalize to new test inputs. While achieving impressive results, these approaches have a number of important limitations: (a) they are computationally expensive and hard to train, (b) a model has to be trained for each task (program) separately, and (c) it is hard to interpret or verify the correctness of the learnt mapping (as it is defined by a neural network). In this paper, we propose a novel technique, Neuro-Symbolic Program Synthesis, to overcome the above-mentioned problems. Once trained, our approach can automatically construct computer programs in a domain-specific language that are consistent with a set of input-output examples provided at test time. Our method is based on two novel neural modules. The first module, called the cross correlation I/O network, given a set of input-output examples, produces a continuous representation of the set of I/O examples. The second module, the Recursive-Reverse-Recursive Neural Network (R3NN), given the continuous representation of the examples, synthesizes a program by incrementally expanding partial programs. We demonstrate the effectiveness of our approach by applying it to the rich and complex domain of regular expression based string transformations. Experiments show that the R3NN model is not only able to construct programs from new input-output examples, but it is also able to construct new programs for tasks that it had never observed before during training.},
archivePrefix = {arXiv},
arxivId = {1611.01855},
author = {Parisotto, Emilio and Mohamed, Abdel-rahman and Singh, Rishabh and Li, Lihong and Zhou, Dengyong and Kohli, Pushmeet},
eprint = {1611.01855},
month = {nov},
title = {{Neuro-Symbolic Program Synthesis}},
url = {http://arxiv.org/abs/1611.01855},
year = {2016}
}
@incollection{Walsham1997,
abstract = {An increasing interest is being shown, not least by IS researchers, in the socio-technical approach known as actor-network theory. The purpose of this paper is to assess the current and potential future contribution of the theory to IS research. A brief review of key concepts of the theory is given, some IS literature which uses the theory is described, and significant criticisms of the theory are examined in some detail. Finally, implications are drawn on the potential value of the theory for IS research in the future, with the broad conclusion being that it has much to offer in both theoretical and methodological terms.},
address = {Boston, MA},
author = {Walsham, G.},
booktitle = {Inf. Syst. Qual. Res.},
doi = {10.1007/978-0-387-35309-8_23},
pages = {466--480},
publisher = {Springer US},
title = {{Actor-Network Theory and IS Research: Current Status and Future Prospects}},
url = {http://link.springer.com/10.1007/978-0-387-35309-8{\_}23},
year = {1997}
}
@article{Torres2012,
abstract = {BACKGROUND: The course of autosomal dominant polycystic kidney disease (ADPKD) is often associated with pain, hypertension, and kidney failure. Preclinical studies indicated that vasopressin V2-receptor antagonists inhibit cyst growth and slow the decline of kidney function. METHODS: In this phase 3, multicenter, double-blind, placebo-controlled, 3-year trial, we randomly assigned 1445 patients, 18 to 50 years of age, who had ADPKD with a total kidney volume of 750 ml or more and an estimated creatinine clearance of 60 ml per minute or more, in a 2:1 ratio to receive tolvaptan, a V2-receptor antagonist, at the highest of three twice-daily dose regimens that the patient found tolerable, or placebo. The primary outcome was the annual rate of change in the total kidney volume. Sequential secondary end points included a composite of time to clinical progression (defined as worsening kidney function, kidney pain, hypertension, and albuminuria) and rate of kidney-function decline. RESULTS: Over a 3-year period, the increase in total kidney volume in the tolvaptan group was 2.8{\%} per year (95{\%} confidence interval [CI], 2.5 to 3.1), versus 5.5{\%} per year in the placebo group (95{\%} CI, 5.1 to 6.0; P{\textless}0.001). The composite end point favored tolvaptan over placebo (44 vs. 50 events per 100 follow-up-years, P = 0.01), with lower rates of worsening kidney function (2 vs. 5 events per 100 person-years of followup, P{\textless}0.001) and kidney pain (5 vs. 7 events per 100 person-years of follow-up, P = 0.007). Tolvaptan was associated with a slower decline in kidney function (reciprocal of the serum creatinine level, -2.61 [mg per milliliter]-1 per year vs. -3.81 [mg per milliliter]-1 per year; P{\textless}0.001). There were fewer ADPKD-related adverse events in the tolvaptan group but more events related to aquaresis (excretion of electrolyte-free water) and hepatic adverse events unrelated to ADPKD, contributing to a higher discontinuation rate (23{\%}, vs. 14{\%} in the placebo group). CONCLUSIONS: Tolvaptan, as compared with placebo, slowed the increase in total kidney volume and the decline in kidney function over a 3-year period in patients with ADPKD but was associated with a higher discontinuation rate, owing to adverse events. (Funded by Otsuka Pharmaceuticals and Otsuka Pharmaceutical Development and Commercialization; TEMPO 3:4 ClinicalTrials.gov number, NCT00428948.) Copyright {\textcopyright} 2012 Massachusetts Medical Society.},
author = {Torres, Vicente E. and Chapman, Arlene B. and Devuyst, Olivier and Gansevoort, Ron T. and Grantham, Jared J. and Higashihara, Eiji and Perrone, Ronald D. and Krasa, Holly B. and Ouyang, John and Czerwiec, Frank S.},
doi = {10.1056/NEJMoa1205511},
issn = {15334406},
journal = {N. Engl. J. Med.},
month = {dec},
number = {25},
pages = {2407--2418},
pmid = {23121377},
publisher = {Massachusetts Medical Society},
title = {{Tolvaptan in patients with autosomal dominant polycystic kidney disease}},
url = {http://www.nejm.org/doi/abs/10.1056/NEJMoa1205511},
volume = {367},
year = {2012}
}
@article{Gatov2017,
abstract = {{\textcopyright} 2017 Joule Inc. or its licensors. BACKGROUND: We examined mortality time trends and premature mortality among individuals with and without schizophrenia over a 20-year period. METHODS: In this population-based, repeated cross-sectional study, we identified all individual deaths that occurred in Ontario between 1993 and 2012 in persons aged 15 and over. We plotted overall and cause-specific age- and sex-standardized mortality rates (ASMRs), stratified all-cause ASMR trends by sociodemographic characteristics, and analyzed premature mortality using years of potential life lost. Additionally, we calculated mortality rate ratios (MRRs) using negative binomial regression with adjustment for age, sex, income, rurality and year of death. RESULTS: We identified 31 349 deaths among persons with schizophrenia, and 1 589 902 deaths among those without schizophrenia. Mortality rates among people with schizophrenia were 3 times higher than among those without schizophrenia (adjusted MRR 3.12, 95{\%} confidence interval 3.06-3.17). All-cause ASMRs in both groups declined in parallel over the study period, by about 35{\%}, and were higher for men, for those with low income and for rural dwellers. The absolute ASMR difference also declined throughout the study period (from 16.15 to 10.49 deaths per 1000 persons). Cause-specific ASMRs were greater among those with schizophrenia, with circulatory conditions accounting for most deaths between 1993 and 2012, whereas neoplasms became the leading cause of death for those without schizophrenia afer 2005. Individuals with schizophrenia also died, on average, 8 years younger than those without schizophrenia, losing more potential years of life. INTERPRETATION: Although mortality rates among people with schizophrenia have declined over the past 2 decades, specialized approaches may be required to close the persistent 3-fold relative mortality gap with the general population.},
author = {Gatov, Evgenia and Rosella, Laura and Chiu, Maria and Kurdyak, Paul A.},
doi = {10.1503/cmaj.161351},
issn = {14882329},
journal = {CMAJ},
month = {sep},
number = {37},
pages = {E1177--E1187},
pmid = {28923795},
title = {{Trends in standardized mortality among individuals with schizophrenia, 1993-2012: A population-based, repeated cross-sectional study}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28923795 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5602497 http://www.cmaj.ca/lookup/doi/10.1503/cmaj.161351},
volume = {189},
year = {2017}
}
@article{Linero2018,
abstract = {Decision tree ensembles are an extremely popular tool for obtaining high-quality predictions in nonpara-metric regression problems. Unmodified, however, many commonly used decision tree ensemble methods do not adapt to sparsity in the regime in which the number of predictors is larger than the number of obser-vations. A recent stream of research concerns the construction of decision tree ensembles that are motivated by a generative probabilistic model, the most influential method being the Bayesian additive regression trees (BART) framework. In this article, we take a Bayesian point of view on this problem and show how to construct priors on decision tree ensembles that are capable of adapting to sparsity in the predictors by placing a sparsity-inducing Dirichlet hyperprior on the splitting proportions of the regression tree prior. We characterize the asymptotic distribution of the number of predictors included in the model and show how this prior can be easily incorporated into existing Markov chain Monte Carlo schemes. We demonstrate that our approach yields useful posterior inclusion probabilities for each predictor and illustrate the usefulness of our approach relative to other decision tree ensemble approaches on both simulated and real datasets. Supplementary materials for this article are available online.},
author = {Linero, Antonio R.},
doi = {10.1080/01621459.2016.1264957},
issn = {1537274X},
journal = {J. Am. Stat. Assoc.},
keywords = {Bayesian additive regression trees,Bayesian learning,Decision trees,Nonparametric regression,Random forests,Variable selection},
month = {apr},
number = {522},
pages = {626--636},
title = {{Bayesian Regression Trees for High-Dimensional Prediction and Variable Selection}},
url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2016.1264957},
volume = {113},
year = {2018}
}
@article{Ferreira2015,
abstract = {In this paper, we propose a technique for time series clustering using community detection in complex networks. Firstly, we present a method to transform a set of time series into a network using different distance functions, where each time series is represented by a vertex and the most similar ones are connected. Then, we apply community detection algorithms to identify groups of strongly connected vertices (called a community) and, consequently, identify time series clusters. Still in this paper, we make a comprehensive analysis on the influence of various combinations of time series distance functions, network generation methods and community detection techniques on clustering results. Experimental study shows that the proposed network-based approach achieves better results than various classic or up-to-date clustering techniques under consideration. Statistical tests confirm that the proposed method outperforms some classic clustering algorithms, such as k-medoids, diana, median-linkage and centroid-linkage in various data sets. Interestingly, the proposed method can effectively detect shape patterns presented in time series due to the topological structure of the underlying network constructed in the clustering process. At the same time, other techniques fail to identify such patterns. Moreover, the proposed method is robust enough to group time series presenting similar pattern but with time shifts and/or amplitude variations. In summary, the main point of the proposed method is the transformation of time series from time-space domain to topological domain. Therefore, we hope that our approach contributes not only for time series clustering, but also for general time series analysis tasks.},
archivePrefix = {arXiv},
arxivId = {1508.04757},
author = {Ferreira, Leonardo N. and Zhao, Liang},
doi = {10.1016/j.ins.2015.07.046},
eprint = {1508.04757},
issn = {00200255},
journal = {Inf. Sci. (Ny).},
keywords = {Community detection,Complex networks,Time series clustering,Time series data mining},
month = {aug},
pages = {227--242},
title = {{Time series clustering via community detection in networks}},
url = {http://arxiv.org/abs/1508.04757 http://dx.doi.org/10.1016/j.ins.2015.07.046},
volume = {326},
year = {2016}
}
@misc{Fletcher2009,
abstract = {The standardised mortality ratio for a certain town, using England and Wales as the standard, is reported as 70. Which of the following statements, if any, are true?},
author = {Fletcher, John},
booktitle = {BMJ},
doi = {10.1136/bmj.b2005},
issn = {17561833},
month = {may},
number = {7705},
pages = {b2005--b2005},
publisher = {British Medical Journal Publishing Group},
title = {{Statistical question: Standardised mortality ratios}},
url = {http://www.bmj.com/cgi/doi/10.1136/bmj.b2005},
volume = {338},
year = {2009}
}
@inproceedings{Heffernan2017,
author = {Heffernan, Kevin and Li{\`{o}}, Pietro and Teufel, Simone},
booktitle = {Int. Meet. Comput. Intell. Methods Bioinforma. Biostat.},
doi = {10.1007/978-3-319-67834-4_17},
pages = {209--219},
publisher = {Springer, Cham},
title = {{Multilayer Data and Document Stratification for Comorbidity Analysis}},
url = {http://link.springer.com/10.1007/978-3-319-67834-4{\_}17},
year = {2017}
}
@inproceedings{Zhang2016d,
archivePrefix = {arXiv},
arxivId = {1601.02376},
author = {Zhang, Weinan and Du, Tianming and Wang, Jun},
eprint = {1601.02376},
month = {jan},
title = {{Deep Learning over Multi-field Categorical Data: A Case Study on User Response Prediction}},
url = {https://arxiv.org/abs/1601.02376},
year = {2016}
}
@article{Sahakian2015a,
abstract = {In addition to causing distress and disability to the individual, neuropsychiatric disorders are also extremely expensive to society and governments. These disorders are both common and debilitating and impact on cognition, functionality and wellbeing. Cognitive enhancing drugs, such as cholinesterase inhibitors and methylphenidate, are used to treat cognitive dysfunction in Alzheimer's disease and attention deficit hyperactivity disorder, respectively. Other cognitive enhancers include specific computerized cognitive training and devices. An example of a novel form of cognitive enhancement using the technological advancement of a game on an iPad that also acts to increase motivation is presented. Cognitive enhancing drugs, such as methylphenidate and modafinil, which were developed as treatments, are increasingly being used by healthy people. Modafinil not only affects 'cold' cognition, but also improves 'hot' cognition, such as emotion recognition and task-related motivation. The lifestyle use of 'smart drugs' raises both safety concerns as well as ethical issues, including coercion and increasing disparity in society. As a society, we need to consider which forms of cognitive enhancement (e.g. pharmacological, exercise, lifelong learning) are acceptable and for which groups (e.g. military, doctors) under what conditions (e.g. war, shift work) and by what methods we would wish to improve and flourish.},
author = {Sahakian, Barbara J. and Bruhl, Annette B. and Cook, Jennifer and Killikelly, Clare and Savulich, George and Piercy, Thomas and Hafizi, Sepehr and Perez, Jesus and Fernandez-Egea, Emilio and Suckling, John and Jones, Peter B.},
doi = {10.1098/rstb.2014.0214},
issn = {14712970},
journal = {Philos. Trans. R. Soc. B Biol. Sci.},
keywords = {Cognitive enhancers,Cognitive training,Game,Neuroethics,Schizophrenia,Smart drugs},
month = {sep},
number = {1677},
pages = {20140214},
publisher = {The Royal Society},
title = {{The impact of neuroscience on society: Cognitive enhancement in neuropsychiatric disorders and in healthy people}},
url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2014.0214},
volume = {370},
year = {2015}
}
@misc{software_rodbc,
author = {Ripley, Brian and Lapsley, Michael},
title = {{RODBC: ODBC Database Access}},
url = {https://cran.r-project.org/package=RODBC},
year = {2017}
}
@article{Tsai2016,
abstract = {In practice, the data collected from data mining usually contain some missing values. Imputation is the process of replacing the missing values in incomplete datasets. It is usually based on providing estimations for missing values by reasoning from the observed data. Consequently, the effectiveness of missing value imputation is heavily dependent on the observed data (or complete data) in the incomplete datasets. The objective of this study is to investigate the effect of performing instance selection to filter out some noisy data (or outliers) from a given dataset on the imputation task. Specifically, four different processes for combining instance selection and missing value imputation are proposed and compared in terms of data classification. The experimental results based on 29 datasets containing categorical, numerical, and mixed attribute types of data show that the process of performing instance selection first and imputation second allows the k-NN and SVM classifiers to outperform the other processes over the categorical and numerical datasets. For the mixed type of datasets, k-NN performs the best when instance selection is performed again on the datasets produced by the second process. Finally, some specific decision rules about when to employ which process are also provided for future research.},
author = {Tsai, Chih Fong and Chang, Fu Yu},
doi = {10.1016/j.jss.2016.08.093},
issn = {01641212},
journal = {J. Syst. Softw.},
keywords = {Data mining,Incomplete data,Instance selection,Missing value imputation},
pages = {63--71},
title = {{Combining instance selection for better missing value imputation}},
volume = {122},
year = {2016}
}
@article{Herrett2015,
abstract = {{\textcopyright} The Author 2015. Published by Oxford University Press on behalf of the International Epidemiological Association.The Clinical Practice Research Datalink (CPRD) is an ongoing primary care database of anonymised medical records from general practitioners, with coverage of over 11.3 million patients from 674 practices in the UK. With 4.4 million active (alive, currently registered) patients meeting quality criteria, approximately 6.9{\%} of the UK population are included and patients are broadly representative of the UK general population in terms of age, sex and ethnicity. General practitioners are the gatekeepers of primary care and specialist referrals in the UK. The CPRD primary care database is therefore a rich source of health data for research, including data on demographics, symptoms, tests, diagnoses, therapies, health-related behaviours and referrals to secondary care. For over half of patients, linkage with datasets from secondary care, disease-specific cohorts and mortality records enhance the range of data available for research. The CPRD is very widely used internationally for epidemiological research and has been used to produce over 1000 research studies, published in peer-reviewed journals across a broad range of health outcomes. However, researchers must be aware of the complexity of routinely collected electronic health records, including ways to manage variable completeness, misclassification and development of disease definitions for research.},
author = {Herrett, Emily and Gallagher, Arlene M and Bhaskaran, Krishnan and Forbes, Harriet and Mathur, Rohini and van Staa, Tjeerd and Smeeth, Liam},
doi = {10.1093/ije/dyv098},
issn = {14643685},
journal = {Int. J. Epidemiol.},
keywords = {datasets,electronic medical records,ethnic group,family,misclassification,mortality,patient referral,peer review,physicians},
month = {jun},
number = {3},
pages = {827--836},
publisher = {Narnia},
title = {{Data Resource Profile: Clinical Practice Research Datalink (CPRD)}},
url = {https://academic.oup.com/ije/article-lookup/doi/10.1093/ije/dyv098},
volume = {44},
year = {2015}
}
@article{Page2012,
abstract = {BACKGROUND: Climate change is expected to have significant effects on human health, partly through an increase in extreme events such as heatwaves. People with mental illness may be at particular risk.$\backslash$n$\backslash$nAIMS: To estimate risk conferred by high ambient temperature on patients with psychosis, dementia and substance misuse.$\backslash$n$\backslash$nMETHOD: We applied time-series regression analysis to data from a nationally representative primary care cohort study. Relative risk of death per 1°C increase in temperature was calculated above a threshold.$\backslash$n$\backslash$nRESULTS: Patients with mental illness showed an overall increase in risk of death of 4.9{\%} (95{\%} CI 2.0-7.8) per 1°C increase in temperature above the 93rd percentile of the annual temperature distribution. Younger patients and those with a primary diagnosis of substance misuse demonstrated greatest mortality risk.$\backslash$n$\backslash$nCONCLUSIONS: The increased risk of death during hot weather in patients with psychosis, dementia and substance misuse has implications for public health strategies during heatwaves.},
author = {Page, Lisa A. and Hajat, Shakoor and {Sari Kovats}, R. and Howard, Louise M.},
doi = {10.1192/bjp.bp.111.100404},
isbn = {1472-1465 (Electronic) 0007-1250 (Linking)},
issn = {00071250},
journal = {Br. J. Psychiatry},
month = {jun},
number = {6},
pages = {485--490},
pmid = {22661680},
title = {{Temperature-related deaths in people with psychosis, dementia and substance misuse}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22661680 https://www.cambridge.org/core/product/identifier/S0007125000079551/type/journal{\_}article},
volume = {200},
year = {2012}
}
@article{Banerjee2017j,
abstract = {We outline an automated computational and machine learning framework that predicts disease severity and stratifies patients. We apply our framework to available clinical data. Our algorithm automatically generates insights and predicts disease severity with minimal operator intervention. The computational framework presented here can be used to stratify patients, predict disease severity and propose novel biomarkers for disease. Insights from machine learning algorithms coupled with clinical data may help guide therapy, personalize treatment and help clinicians understand the change in disease over time. Computational techniques like these can be used in translational medicine in close collaboration with clinicians and healthcare providers. Our models are also interpretable, allowing clinicians with minimal machine learning experience to engage in model building. This work is a step towards automated machine learning in the clinic.},
author = {Banerjee, Soumya},
doi = {10.7906/15.3.4},
issn = {1334-4684},
journal = {Interdiscip. Descr. Complex Syst.},
number = {3},
pages = {199--208},
title = {{Automated Interpretable Computational Biology in the Clinic: a Framework To Predict Disease Severity and Stratify Patients From Clinical Data}},
url = {http://indecs.eu/index.php?s=x{\&}y=2017{\&}p=199-208},
volume = {15},
year = {2017}
}
@article{Lemaitre2017,
abstract = {Imbalanced-learn is an open-source python toolbox aiming at providing a wide range of methods to cope with the problem of imbalanced dataset frequently encountered in machine learning and pattern recognition. The implemented state-of-the-art methods can be categorized into 4 groups: (i) under-sampling, (ii) over-sampling, (iii) combination of over- and under-sampling, and (iv) ensemble learning methods. The proposed toolbox only depends on numpy, scipy, and scikit-learn and is distributed under MIT license. Furthermore, it is fully compatible with scikit-learn and is part of the scikit-learn-contrib supported project. Documentation, unit tests as well as integration tests are provided to ease usage and contribution. The toolbox is publicly available in GitHub: https://github.com/scikit-learn-contrib/imbalanced-learn.},
archivePrefix = {arXiv},
arxivId = {1609.06570},
author = {Lemaitre, Guillaume and Nogueira, Fernando and Aridas, Christos K.},
doi = {http://www.jmlr.org/papers/volume18/16-365/16-365.pdf},
eprint = {1609.06570},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
number = {17},
pages = {1--5},
title = {{Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning}},
url = {http://www.jmlr.org/papers/v18/16-365.html http://arxiv.org/abs/1609.06570},
volume = {18},
year = {2016}
}
@inproceedings{pietro_logical_framework,
author = {Jo{\"{e}}lle, Despeyroux and Amy, Felty and Lio, Pietro and Olarte, Carlos},
booktitle = {Int. Symp. Mol. Log. Comput. Synth. Biol.},
title = {{A Logical Framework for Modelling Breast Cancer Progression}},
year = {2018}
}
@article{Bishop1997,
abstract = {This paper describes and discusses Bayesian Neural Network (BNN). The paper showcases a few different applications of them for classification and regression problems. BNNs are comprised of a Probabilistic Model and a Neural Network. The intent of such a design is to combine the strengths of Neural Networks and Stochastic modeling. Neural Networks exhibit continuous function approximator capabilities. Stochastic models allow direct specification of a model with known interaction between parameters to generate data. During the prediction phase, stochastic models generate a complete posterior distribution and produce probabilistic guarantees on the predictions. Thus BNNs are a unique combination of neural network and stochastic models with the stochastic model forming the core of this integration. BNNs can then produce probabilistic guarantees on it's predictions and also generate the distribution of parameters that it has learnt from the observations. That means, in the parameter space, one can deduce the nature and shape of the neural network's learnt parameters. These two characteristics makes them highly attractive to theoreticians as well as practitioners. Recently there has been a lot of activity in this area, with the advent of numerous probabilistic programming libraries such as: PyMC3, Edward, Stan etc. Further this area is rapidly gaining ground as a standard machine learning approach for numerous problems},
archivePrefix = {arXiv},
arxivId = {1801.07710},
author = {Bishop, Christopher M.},
doi = {10.1590/S0104-65001997000200006},
eprint = {1801.07710},
issn = {0104-6500},
journal = {J. Brazilian Comput. Soc.},
month = {jan},
number = {1},
title = {{Bayesian Neural Networks}},
url = {http://arxiv.org/abs/1801.07710 http://www.scielo.br/scielo.php?script=sci{\_}arttext{\&}pid=S0104-65001997000200006{\&}lng=en{\&}nrm=iso{\&}tlng=en},
volume = {4},
year = {1997}
}
@article{Lun2018,
abstract = {Data exploration is critical to the comprehension of large biological data sets generated by high-throughput assays such as sequencing. However, most existing tools for interactive visualisation are limited to specific assays or analyses. Here, we present the iSEE (Interactive SummarizedExperiment Explorer) software package, which provides a general visual interface for exploring data in a SummarizedExperiment object. iSEE is directly compatible with many existing R/Bioconductor packages for analysing high-throughput biological data, and provides useful features such as simultaneous examination of (meta)data and analysis results, dynamic linking between plots and code tracking for reproducibility. We demonstrate the utility and flexibility of iSEE by applying it to explore a range of real transcriptomics and proteomics data sets.},
author = {Lun, Aaron T.L. and Rue-Albrecht, Kevin and Marini, Federico and Soneson, Charlotte},
doi = {10.12688/f1000research.14966.1},
issn = {1759796X},
journal = {F1000Research},
keywords = {Bioconductor,Genomics,Interactive,Proteomics,R,Shiny,Transcriptomics,Visualization},
month = {jun},
pages = {741},
publisher = {F1000 Research Limited},
title = {{iSEE: Interactive SummarizedExperiment Explorer}},
url = {https://f1000research.com/articles/7-741/v1},
volume = {7},
year = {2018}
}
@inproceedings{DeMaria2014,
abstract = {We propose a novel approach for the formal verification of biological systems based on the use of a modal linear logic. We show how such a logic can be used, with worlds as instants of time, as an unified framework to encode both biological systems and temporal properties of their dynamic behaviour. To illustrate our methodology, we consider a model of the P53/Mdm2 DNA-damage repair mechanism. We prove several properties that are important for such a model to satisfy and serve to illustrate the promise of our approach. We formalize the proofs of these properties in the Coq Proof Assistant, with the help of a Lambda Prolog prover for partial automation of the proofs.},
archivePrefix = {arXiv},
arxivId = {1404.5439},
author = {{De Maria}, Elisabetta and Despeyroux, Jo{\"{e}}lle and Felty, Amy P.},
booktitle = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
doi = {10.1007/978-3-319-10398-3_10},
eprint = {1404.5439},
isbn = {9783319103976},
issn = {16113349},
month = {apr},
pages = {136--155},
title = {{A logical framework for systems biology}},
url = {http://arxiv.org/abs/1404.5439},
volume = {8738 LNBI},
year = {2014}
}
@article{Rudin2019,
abstract = {Black box machine learning models are currently being used for high-stakes decision making throughout society, causing problems in healthcare, criminal justice and other domains. Some people hope that creating methods for explaining these black box models will alleviate some of the problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practice and can potentially cause great harm to society. The way forward is to design models that are inherently interpretable. This Perspective clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare and computer vision.},
author = {Rudin, Cynthia},
doi = {10.1038/s42256-019-0048-x},
issn = {2522-5839},
journal = {Nat. Mach. Intell.},
keywords = {Computer science,Criminology,Science,Statistics,technology and society},
month = {may},
number = {5},
pages = {206--215},
publisher = {Nature Publishing Group},
title = {{Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead}},
url = {http://www.nature.com/articles/s42256-019-0048-x},
volume = {1},
year = {2019}
}
@article{Albuquerque2019,
abstract = {The Congenital Zika Syndrome (CZS) epidemic took place in Brazil between 2015 and 2017 and led to the emergence of at least 3194 children born with CZS. We explored access to healthcare services and activities in the Unified Health Service (Sistema {\'{U}}nico de Sa{\'{u}}de: SUS) from the perspective of mothers of children with CZS and professionals in the Public Healthcare Network. We carried out a qualitative, exploratory study, using semi-structured interviews, in two Brazilian states—Pernambuco, which was the epicentre of the epidemic in Brazil, and Rio de Janeiro, where the epidemic was less intense. The mothers and health professionals reported that healthcare provision was insufficient and fragmented and there were problems with follow-up care. There was a lack of co-ordination and an absence of communication between the various specialized services and between different levels of the health system. We also noted a public–private mixture in access to healthcare services, resulting from a segmented system and related to inequality of access. High reported household expenditure is an expression of the phenomenon of underfunding of the public system. The challenges that mothers and health professionals reported exposes contradictions in the health system which, although universal, does not guarantee equitable and comprehensive care. Other gaps were revealed through the outbreak. The epidemic provided visibility regarding difficulties of access for other children with disabilities determined by other causes. It also made explicit the gender inequalities that had an impact on the lives of mothers and other female caregivers, as well as an absence of the provision of care for these groups. In the face of an epidemic, the Brazilian State reproduced old fashioned forms of action—activities related to the transmitting mosquito and to prevention with an emphasis on the individual and no action related to social determinants.},
author = {Albuquerque, Maria S V and Lyra, Tereza M and Melo, Ana P L and Valongueiro, Sandra A and Ara{\'{u}}jo, Thalia V B and Pimentel, Camila and Moreira, Martha C N and Mendes, Corina H F and Nascimento, Marcos and Kuper, Hannah and Penn-Kekana, Loveday},
doi = {10.1093/heapol/czz059},
issn = {0268-1080},
journal = {Health Policy Plan.},
keywords = {brazil,child,congenital zika syndrome,epidemics,health care systems,health personnel,mothers,zika virus},
month = {sep},
number = {7},
pages = {499--507},
publisher = {Narnia},
title = {{Access to healthcare for children with Congenital Zika Syndrome in Brazil: perspectives of mothers and health professionals}},
url = {https://academic.oup.com/heapol/article/34/7/499/5542773},
volume = {34},
year = {2019}
}
@article{Li2018,
abstract = {BACKGROUND: Accurate predictive modeling in clinical research enables effective early intervention that patients are most likely to benefit from. However, due to the complex biological nature of disease progression, capturing the highly non-linear information from low-level input features is quite challenging. This requires predictive models with high-capacity. In practice, clinical datasets are often of limited size, bringing danger of overfitting for high-capacity models. To address these two challenges, we propose a deep multi-task neural network for predictive modeling. METHODS: The proposed network leverages clinical measures as auxiliary targets that are related to the primary target. The predictions for the primary and auxiliary targets are made simultaneously by the neural network. Network structure is specifically designed to capture the clinical relevance by learning a shared feature representation between the primary and auxiliary targets. We apply the proposed model in a hypertension dataset and a breast cancer dataset, where the primary tasks are to predict the left ventricular mass indexed to body surface area and the time of recurrence of breast cancer. Moreover, we analyze the weights of the proposed neural network to rank input features for model interpretability. RESULTS: The experimental results indicate that the proposed model outperforms other different models, achieving the best predictive accuracy (mean squared error 199.76 for hypertension data, 860.62 for Wisconsin prognostic breast cancer data) with the ability to rank features according to their contributions to the targets. The ranking is supported by previous related research. CONCLUSION: We propose a novel effective method for clinical predictive modeling by combing the deep neural network and multi-task learning. By leveraging auxiliary measures clinically related to the primary target, our method improves the predictive accuracy. Based on featue ranking, our model is interpreted and shows consistency with previous studies on cardiovascular diseases and cancers.},
author = {Li, Xiangrui and Zhu, Dongxiao and Levy, Phillip},
doi = {10.1186/s12911-018-0676-9},
issn = {14726947},
journal = {BMC Med. Inform. Decis. Mak.},
keywords = {Auxiliary task,Deep neural network,Multi-task learning,Predictive modeling},
month = {dec},
number = {S4},
pages = {126},
publisher = {BioMed Central},
title = {{Leveraging auxiliary measures: A deep multi-task neural network for predictive modeling in clinical research}},
url = {https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-018-0676-9},
volume = {18},
year = {2018}
}
@article{Vold2015,
abstract = {Andy Clark and David Chalmers (1998) argue that certain mental states and processes can be partially constituted by objects located beyond one's brain and body: this is their extended mind the-sis (EM). But they maintain that consciousness relies on processing that is too high in speed and bandwidth to be realized outside the body (see Chalmers, 2008, and Clark, 2009). I evaluate Clark's and Chalmers' reason for denying that consciousness extends while still supporting unconscious state extension. I argue that their reason is not well grounded and does not hold up against foreseeable advances in technology. I conclude that their current position needs re-evalua-tion. If their original parity argument works as a defence of EM, they have yet to identify a good reason why it does not also work as a defence of extended consciousness. I end by advancing a parity argu-ment for extended consciousness and consider some possible replies.},
author = {Vold, Karina},
issn = {13558250},
journal = {J. Conscious. Stud.},
number = {3-4},
pages = {16--33},
publisher = {Imprint Academic},
title = {{The Parity Argument for Extended Consciousness}},
url = {https://www.ingentaconnect.com/content/imp/jcs/2015/00000022/F0020003/art00002},
volume = {22},
year = {2015}
}
@article{Salvatier2015,
abstract = {Probabilistic programming (PP) allows flexible specification of Bayesian statistical models in code. PyMC3 is a new, open-source PP framework with an intutive and readable, yet powerful, syntax that is close to the natural syntax statisticians use to describe models. It features next-generation Markov chain Monte Carlo (MCMC) sampling algorithms such as the No-U-Turn Sampler (NUTS; Hoffman, 2014), a self-tuning variant of Hamiltonian Monte Carlo (HMC; Duane, 1987). Probabilistic programming in Python confers a number of advantages including multi-platform compatibility, an expressive yet clean and readable syntax, easy integration with other scientific libraries, and extensibility via C, C++, Fortran or Cython. These features make it relatively straightforward to write and use custom statistical distributions, samplers and transformation functions, as required by Bayesian analysis.},
archivePrefix = {arXiv},
arxivId = {1507.08050},
author = {Salvatier, John and Wiecki, Thomas and Fonnesbeck, Christopher},
doi = {10.7717/peerj-cs.55},
eprint = {1507.08050},
isbn = {9783319238258},
issn = {2376-5992},
journal = {PeerJ Comput. Sci.},
keywords = {Bayesian statistic,Markov chain Monte Carlo,Probabilistic Programming,Python,Statistical modeling},
month = {apr},
pages = {e55},
publisher = {PeerJ Inc.},
title = {{Probabilistic Programming in Python using PyMC}},
url = {https://peerj.com/articles/cs-55 http://arxiv.org/abs/1507.08050},
volume = {2},
year = {2015}
}
@article{Vink2019,
abstract = {Purpose: The purpose of this paper is to analyze how service design practices reshape mental models to enable innovation. Mental models are actors' assumptions and beliefs that guide their behavior and interpretation of their environment. Design/methodology/approach: This paper offers a conceptual framework for innovation in service ecosystems through service design that connects the macro view of innovation as changing institutional arrangements with the micro view of innovation as reshaping actors' mental models. Furthermore, through an 18-month ethnographic study of service design practices in the context of healthcare, how service design practices reshape mental models to enable innovation is investigated. Findings: This research highlights that service design reshapes mental models through the practices of sensing surprise, perceiving multiples and embodying alternatives. This paper delineates the enabling conditions for these practices to occur, such as coaching, diverse participation and supportive physical materials. Research limitations/implications: This study brings forward the underappreciated role of actors' mental models in innovation. It highlights that innovation in service ecosystems is not simply about actors making changes to their external context but also actors shifting their own assumptions and beliefs. Practical implications: This paper offers insights for service managers and service designers interested in supporting innovation on how to catalyze shifts in actors' mental models by creating the conditions for specific service design practices. Originality/value: This paper is the first to shed light on the central role of actors' mental models in innovation and identify the service design practices that reshape mental models.},
author = {Vink, Josina and Edvardsson, Bo and Wetter-Edman, Katarina and Tronvoll, B{\aa}rd},
doi = {10.1108/JOSM-08-2017-0186},
issn = {17575818},
journal = {J. Serv. Manag.},
keywords = {Innovation,Institutional arrangements,Institutional work,Mental models,Service design,Service ecosystems},
month = {jan},
number = {1},
pages = {75--104},
publisher = {Emerald Publishing Limited},
title = {{Reshaping mental models – enabling innovation through service design}},
url = {https://www.emeraldinsight.com/doi/10.1108/JOSM-08-2017-0186},
volume = {30},
year = {2019}
}
@article{Udrescu2019,
abstract = {A core challenge for both physics and artificial intellicence (AI) is symbolic regression: finding a symbolic expression that matches data from an unknown function. Although this problem is likely to be NP-hard in principle, functions of practical interest often exhibit symmetries, separability, compositionality and other simplifying properties. In this spirit, we develop a recursive multidimensional symbolic regression algorithm that combines neural network fitting with a suite of physics-inspired techniques. We apply it to 100 equations from the Feynman Lectures on Physics, and it discovers all of them, while previous publicly available software cracks only 71; for a more difficult test set, we improve the state of the art success rate from 15{\%} to 90{\%}.},
archivePrefix = {arXiv},
arxivId = {1905.11481},
author = {Udrescu, Silviu-Marian and Tegmark, Max},
eprint = {1905.11481},
month = {may},
title = {{AI Feynman: a Physics-Inspired Method for Symbolic Regression}},
url = {http://arxiv.org/abs/1905.11481},
year = {2019}
}
@inproceedings{Sokol2018a,
abstract = {The prevalence of automated decision making, influencing important aspects of our lives - e.g., school admission, job market, insurance and banking - has resulted in increasing pressure from society and regulators to make this process more transparent and ensure its explainability, accountability and fairness. We demonstrate a prototype voice-enabled device, called Glass-Box, which users can question to understand automated decisions and identify the underlying model's biases and errors. Our system explains algorithmic predictions with class-contrastive counterfactual statements (e.g., “Had a number of conditions been different:. . . the prediction would change. . .”), which show a difference in a particular scenario that causes an algorithm to “change its mind”. Such explanations do not require any prior technical knowledge to understand, hence are suitable for a lay audience, who interact with the system in a natural way - through an interactive dialogue. We demonstrate the capabilities of the device by allowing users to impersonate a loan applicant who can question the system to understand the automated decision that he received.},
author = {Sokol, Kacper and Flach, Peter},
booktitle = {IJCAI Int. Jt. Conf. Artif. Intell.},
doi = {10.24963/ijcai.2018/865},
isbn = {9780999241127},
issn = {10450823},
pages = {5868--5870},
publisher = {International Joint Conferences on Artificial Intelligence},
title = {{Glass-box: Explaining AI decisions with counterfactual statements through conversation with a voice-enabled virtual assistant}},
volume = {2018-July},
year = {2018}
}
@article{Iuliano2016,
abstract = {International initiatives such as the Cancer Genome Atlas (TCGA) and the International Cancer Genome Consortium (ICGC) are collecting multiple datasets at different genome-scales with the aim of identifying novel cancer biomarkers and predicting survival patients. To analyze such data, several statistical methods have been applied, among them Cox regression models. Although these models provide a good statistical framework to analyze omic data, there is still a lack of studies that illustrate advantages and drawbacks in integrating biological information and selecting groups of biomarkers. In fact, classical Cox regression algorithms focus on the selection of a single biomarker, without taking into account the strong correlation between genes. Even though network-based Cox regression algorithms overcome such drawbacks, such network-based approaches are less widely used within the life science community. In this article, we aim to provide a clear methodological framework on the use of such approaches in order to turn cancer research results into clinical applications. Therefore, we first discuss the rationale and the practical usage of three recently proposed network-based Cox regression algorithms (i.e., Net-Cox, AdaLnet and fastcox). Then, we show how to combine existing biological knowledge and available data with such algorithms to identify networks of cancer biomarkers and to estimate survival patients. Finally, we describe in detail a new permutation-based approach to better validate the significance of the selection in terms of cancer gene signatures and pathway/networks identification. We illustrate the proposed methodology by means of both simulations and real case studies. Overall, the aim of our work is two-fold. Firstly, to show how network-based Cox regression models can be used to integrate biological knowledge (e.g. multi-omics data) for the analysis of survival data. Secondly, to provide a clear methodological and computational approach for investigating cancers regulatory networks.},
author = {Iuliano, Antonella and Occhipinti, Annalisa and Angelini, Claudia and {De Feis}, Italia and Li{\'{o}}, Pietro},
doi = {10.3389/fphys.2016.00208},
issn = {1664042X},
journal = {Front. Physiol.},
keywords = {Cancer,Cox model,Gene expression,High-dimensionality,Network,Regularization,Survival},
month = {jun},
number = {JUN},
pages = {208},
publisher = {Frontiers},
title = {{Cancer markers selection using network-based cox regression: A methodological and computational practice}},
url = {http://journal.frontiersin.org/Article/10.3389/fphys.2016.00208/abstract},
volume = {7},
year = {2016}
}
@article{Ustun2013,
abstract = {We introduce Supersparse Linear Integer Models (SLIM) as a tool to create scoring systems for binary classification. We derive theoretical bounds on the true risk of SLIM scoring systems, and present experimental results to show that SLIM scoring systems are accurate, sparse, and interpretable classification models.},
archivePrefix = {arXiv},
arxivId = {arXiv:1306.6677v1},
author = {Ustun, Berk and Trac, Stefano},
eprint = {arXiv:1306.6677v1},
month = {jun},
title = {{Supersparse Linear Integer Models for Interpretable Classification}},
url = {http://arxiv.org/abs/1306.5860},
year = {2013}
}
@article{Sutskever2013,
abstract = {Recently, pre-trained deep neural networks (DNNs) have outperformed traditional acoustic models based on Gaussian mixture models (GMMs) on a variety of large vocabulary speech recognition benchmarks. Deep neural nets have also achieved excellent results on various computer vision tasks using a random “dropout” procedure that drastically improves generalization error by randomly omitting a fraction of the hidden units in all layers. Since dropout helps avoid overﬁtting, it has also been successful on a small-scale phone recognition task using larger neural nets. However, training deep neural net acoustic models for large vocabulary speech recognition takes a very long time and dropout is likely to only increase training time. Neural networks with rectiﬁed linear unit (ReLU) non-linearities have been highly successful for computer vision tasks and proved faster to train than standard sigmoid units, sometimes also improving discriminative performance. In this work, we show on a 50-hour English Broadcast News task that modiﬁed deep neural networks using ReLUs trained with dropout during frame level training provide an 4.2{\%} relative improvement over a DNN trained with sigmoid units, and a 14.4{\%} relative improvement over a strong GMM/HMM system. We were able to obtain our results with minimal human hyper-parameter tuning using publicly available Bayesian optimization code},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3605v3},
author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
doi = {10.1109/ICASSP.2013.6639346},
eprint = {arXiv:1301.3605v3},
isbn = {9781479903566},
issn = {15206149},
journal = {ICASSP, IEEE Int. Conf. Acoust. Speech Signal Process. - Proc.},
keywords = {Bayesian optimization,LVCSR,acoustic modeling,broadcast news,deep learning,dropout,neural networks,rectified linear units},
number = {2010},
pages = {8609--8613},
pmid = {303902},
publisher = {JMLR.org},
title = {{On the importance of initialization and momentum in deep learning}},
url = {https://dl.acm.org/citation.cfm?id=3043064},
year = {2013}
}
@article{Goldstein1999,
abstract = {Emergence, as in the title of this new journal, refers to the arising of novel and coherent structures, patterns, and properties during the process of self-organization in complex systems. Emergent phenomena are conceptualized as occurring on the macro level, in contrast to the micro-level components and processes out of which they arise. In a wide variety of scientific and mathematical fields, grouped together loosely under the title "complexity theory," an intense search is now under way for characteristics and laws associated with emergent phenomena observed across different types of complex systems. As a prelude to the study of emergence in organizations, in this article I want to discuss some of the main issues surrounding the explanatory use of the construct of emergence in general, as well as place it in a historical context in order to gain a better grasp on what is unique about its contemporary manifestations.(...)},
author = {Goldstein, Jeffrey},
doi = {10.1207/s15327000em0101_4},
issn = {1521-3250},
journal = {Emergence},
month = {mar},
number = {1},
pages = {49--72},
publisher = {Informa UK Limited},
title = {{Emergence as a Construct: History and Issues}},
volume = {1},
year = {1999}
}
@inproceedings{Miskov-Zivanov2016,
abstract = {We use computational modeling and formal analysis techniques to study temporal behavior of a discrete logical model of naive T cell differentiation. The model is analyzed formally and automatically by performing temporal logic queries via statistical model checking. While the model can be verified and then further explored using Monte Carlo simulation, model checking allows for much more efficient analysis by testing a large set of system properties, with much smaller runtime than the one required by simulations. The results obtained using model checking provide details about relative time of events in the system, which would otherwise be very cumbersome and time comsuming to obtain through simulations only. We efficiently test a large number of properties, and confirm or reject hypotheses that were drawn from previous analysis of experimental and simulation data.},
author = {Miskov-Zivanov, Natasa and Zuliani, Paolo and Wang, Qinsi and Clarke, Edmund M. and Faeder, James R.},
booktitle = {2016 IEEE Int. High Lev. Des. Valid. Test Work. HLDVT 2016},
doi = {10.1109/HLDVT.2016.7748271},
isbn = {9781509042708},
keywords = {Boolean networks,Immune system,Statistical model checking,Stochastic simulation,T cell,formal methods},
month = {oct},
pages = {162--169},
publisher = {IEEE},
title = {{High-level modeling and verification of cellular signaling}},
url = {http://ieeexplore.ieee.org/document/7748271/},
year = {2016}
}
@article{Lucisano2015,
abstract = {enal sonography is an essential diagnostic tool in nephrology. It represents a first-choice diagnostic procedure for assess-ment of several kidney and urinary tract diseases because of its considerable effectiveness in imaging studies of different structures that constitute the kidney parenchyma. Because of its noninvasive-ness and low cost, renal sonography is widely used as a diagnostic tool in daily nephrologic workups, even in bedside settings. 1 However, it is probably underused in the evaluation of renal structural changes for assessment of chronic renal failure. 2},
author = {Lucisano, Gaetano and Comi, Nicolino and Pelagi, Elena and Cianfrone, Paola and Fuiano, Laura and Fuiano, Giorgio},
doi = {10.7863/ultra.34.2.299},
issn = {15509613},
journal = {J. Ultrasound Med.},
keywords = {Body height,Chronic kidney disease,Genitourinary ultrasound,Glomerular filtration rate,Kidney length,Renal parenchymal thickness,Renal sonography},
month = {feb},
number = {2},
pages = {299--306},
pmid = {25614403},
title = {{Can renal sonography be a reliable diagnostic tool in the assessment of chronic kidney disease?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25614403 http://doi.wiley.com/10.7863/ultra.34.2.299},
volume = {34},
year = {2015}
}
@misc{cba_package_R,
author = {Buchta, Christian and Hahsler, Michael},
title = {{cba: Clustering for Business Analytics}},
url = {https://cran.r-project.org/package=cba},
year = {2017}
}
@inproceedings{Corneanu2019,
abstract = {The flexibility and high-accuracy of Deep Neural Networks (DNNs) has transformed computer vision. But, the fact that we do not know when a specific DNN will work and when it will fail has resulted in a lack of trust. A clear example is self-driving cars; people are uncomfortable sitting in a car driven by algorithms that may fail under some unknown, unpredictable conditions. Interpretability and ex-plainability approaches attempt to address this by uncovering what a DNN models, i.e., what each node (cell) in the network represents and what images are most likely to activate it. This can be used to generate, for example, ad-versarial attacks. But these approaches do not generally allow us to determine where a DNN will succeed or fail and why. i.e., does this learned representation generalize to unseen samples? Here, we derive a novel approach to define what it means to learn in deep networks, and how to use this knowledge to detect adversarial attacks. We show how this defines the ability of a network to generalize to unseen testing samples and, most importantly, why this is the case.},
author = {Corneanu, Ciprian A. and Escalera, Sergio and Martinez, Aleix M.},
booktitle = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
pages = {4757--4766},
title = {{What does it mean to learn in deep networks? And, how does one detect adversarial attacks?}},
url = {http://openaccess.thecvf.com/content{\_}CVPR{\_}2019/html/Corneanu{\_}What{\_}Does{\_}It{\_}Mean{\_}to{\_}Learn{\_}in{\_}Deep{\_}Networks{\_}And{\_}CVPR{\_}2019{\_}paper.html},
year = {2019}
}
@article{Viechtbauer,
author = {Viechtbauer, Wolfgang},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{Meta-Analysis Package for R. Package ‘metafor'. R package version 1.6-1.}},
url = {https://cran.r-project.org/web/packages/metafor/index.html http://cran.r-project.org/web/packages/metafor/index.html},
year = {2011}
}
@article{Mohamadlou2018,
abstract = {Background A major problem in treating acute kidney injury (AKI) is that clinical criteria for recognition are markers of established kidney damage or impaired function; treatment before such damage manifests is desirable. Clinicians could intervene during what may be a crucial stage for preventing permanent kidney injury if patients with incipient AKI and those at high risk of developing AKI could be identified. Objective In this study, we evaluate a machine learning algorithm for early detection and prediction of AKI. Design We used a machine learning technique, boosted ensembles of decision trees, to train an AKI prediction tool on retrospective data taken from more than 300 000 inpatient encounters. Setting Data were collected from inpatient wards at Stanford Medical Center and intensive care unit patients at Beth Israel Deaconess Medical Center. Patients Patients older than the age of 18 whose hospital stays lasted between 5 and 1000 hours and who had at least one documented measurement of heart rate, respiratory rate, temperature, serum creatinine (SCr), and Glasgow Coma Scale (GCS). Measurements We tested the algorithm's ability to detect AKI at onset and to predict AKI 12, 24, 48, and 72 hours before onset. Methods We tested AKI detection and prediction using the National Health Service (NHS) England AKI Algorithm as a gold standard. We additionally tested the algorithm's ability to detect AKI as defined by the Kidney Disease: Improving Global Outcomes (KDIGO) guidelines. We compared the algorithm's 3-fold cross-validation performance to the Sequential Organ Failure Assessment (SOFA) score for AKI identification in terms of area under the receiver operating characteristic (AUROC). Results The algorithm demonstrated high AUROC for detecting and predicting NHS-defined AKI at all tested time points. The algorithm achieves AUROC of 0.872 (95{\%} confidence interval [CI], 0.867-0.878) for AKI detection at time of onset. For prediction 12 hours before onset, the algorithm achieves an AUROC of 0.800 (95{\%} CI, 0.792-0.809). For 24-hour predictions, the algorithm achieves AUROC of 0.795 (95{\%} CI, 0.785-0.804). For 48-hour and 72-hour predictions, the algorithm achieves AUROC values of 0.761 (95{\%} CI, 0.753-0.768) and 0.728 (95{\%} CI, 0.719-0.737), respectively. Limitations Because of the retrospective nature of this study, we cannot draw any conclusions about the impact the algorithm's predictions will have on patient outcomes in a clinical setting. Conclusions The results of these experiments suggest that a machine learning-based AKI prediction tool may offer important prognostic capabilities for determining which patients are likely to suffer AKI, potentially allowing clinicians to intervene before kidney damage manifests. Contexte Une des principales difficult{\'{e}}s li{\'{e}}es au traitement de l'insuffisance r{\'{e}}nale aigu{\"{e}} (IRA) est le fait que les crit{\`{e}}res cliniques diagnostiques sont des marqueurs d'une l{\'{e}}sion ou d'une dysfonction r{\'{e}}nale d{\'{e}}j{\`{a}} {\'{e}}tablie. Il est souhaitable d'intervenir avant une telle issue. En d{\'{e}}pistant les patients {\`{a}} risque d'IRA ou atteints d'IRA d{\'{e}}butante, les cliniciens seraient en mesure d'intervenir pr{\'{e}}cocement et ainsi pr{\'{e}}venir les l{\'{e}}sions r{\'{e}}nales permanentes. Objectif de l'{\'{e}}tude L'{\'{e}}tude visait {\`{a}} {\'{e}}valuer un algorithme d'apprentissage automatique destin{\'{e}} {\`{a}} la pr{\'{e}}diction des cas d'IRA et {\`{a}} sa d{\'{e}}tection pr{\'{e}}coce. Type d'{\'{e}}tude Nous avons employ{\'{e}} une technique d'apprentissage automatique, soit des ensembles d'arbres d{\'{e}}cisionnels amplifi{\'{e}}s, pour entrainer un outil de pr{\'{e}}diction de l'IRA {\`{a}} partir de donn{\'{e}}es r{\'{e}}trospectives provenant de plus de 300 000 consultations aupr{\`{e}}s de patients hospitalis{\'{e}}s. Cadre de l'{\'{e}}tude Les donn{\'{e}}es ont {\'{e}}t{\'{e}} collig{\'{e}}es {\`{a}} partir des dossiers des unit{\'{e}}s d'hospitalisation du centre m{\'{e}}dical de l'universit{\'{e}} Stanford et de l'unit{\'{e}} des soins intensifs du centre m{\'{e}}dical Beth Israel Deaconess. Participants Ont {\'{e}}t{\'{e}} inclus dans l'{\'{e}}tude tous les patients adultes dont l'hospitalisation avait dur{\'{e}} de 5 {\`{a}} 1 000 heures et pour lesquels on disposait d'au moins une mesure parmi les suivantes : pouls, rythme respiratoire, temp{\'{e}}rature corporelle, taux de cr{\'{e}}atinine s{\'{e}}rique (SCr) et score de Glasgow. Mesures Nous avons test{\'{e}} l'efficacit{\'{e}} de l'algorithme {\`{a}} d{\'{e}}tecter l'IRA d{\`{e}}s son apparition, et {\`{a}} la pr{\'{e}}dire 12, 24, 48 et 72 heures avant qu'elle ne se manifeste. M{\'{e}}thodologie L'algorithme du NHS England a servi de r{\'{e}}f{\'{e}}rence pour tester l'efficacit{\'{e}} de notre algorithme de pr{\'{e}}diction et de d{\'{e}}tection de l'IRA. Nous avons {\'{e}}galement test{\'{e}} l'efficacit{\'{e}} de notre algorithme {\`{a}} d{\'{e}}tecter l'IRA telle que d{\'{e}}finie par les Recommandations de Bonnes Pratiques Cliniques du KDIGO (Kidney Disease: Improving Global Outcomes). Nous avons utilis{\'{e}} la surface sous la courbe ROC (Receiver Operating Characteristic) pour comparer le score SOFA {\`{a}} l'efficacit{\'{e}} de validation crois{\'{e}}e tripartite de notre algorithme. R{\'{e}}sultats L'algorithme a d{\'{e}}montr{\'{e}} une SSROC (surface sous la courbe ROC) {\'{e}}lev{\'{e}}e pour la d{\'{e}}tection et la pr{\'{e}}diction de l'IRA (telle que d{\'{e}}finie par le NHS) pour tous les moments test{\'{e}}s. En d{\'{e}}tection de la maladie {\`{a}} son apparition, l'algorithme a obtenu une SSROC de 0,872 (IC 95 {\%} : 0,867-0,878). En pr{\'{e}}diction, l'algorithme a obtenu une SSROC de 0,800 (IC 95 {\%} : 0,792-0,809) {\`{a}} 12 heures, de 0,795 {\`{a}} 24 heures (IC 95 {\%} : 0,785-0,804), de 0,761 (IC 95 {\%} : 0,753-0,768) {\`{a}} 48 heures et de 0,728 (IC 95 {\%} : 0,719-0,737) {\`{a}} 72 heures avant l'apparition des premiers sympt{\^{o}}mes. Limites de l'{\'{e}}tude La nature r{\'{e}}trospective de l'{\'{e}}tude ne nous permet pas de tirer de conclusions sur les cons{\'{e}}quences qu'auront les pr{\'{e}}dictions de l'algorithme sur les r{\'{e}}sultats cliniques des patients. Conclusion Les r{\'{e}}sultats de nos essais laissent supposer qu'un outil de pr{\'{e}}diction de l'IRA fond{\'{e}} sur l'apprentissage automatique pourrait offrir d'importantes fonctions pronostiques pour d{\'{e}}tecter les patients susceptibles de d{\'{e}}velopper une IRA en vue d'une intervention pr{\'{e}}coce.},
author = {Mohamadlou, Hamid and Lynn-Palevsky, Anna and Barton, Christopher and Chettipally, Uli and Shieh, Lisa and Calvert, Jacob and Saber, Nicholas R and Das, Ritankar},
doi = {10.1177/2054358118776326},
issn = {2054-3581},
journal = {Can. J. Kidney Heal. Dis.},
keywords = {acute kidney injury,machine learning},
pages = {205435811877632},
pmid = {30094049},
publisher = {SAGE Publications},
title = {{Prediction of Acute Kidney Injury With a Machine Learning Algorithm Using Electronic Health Record Data}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/30094049 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC6080076 http://journals.sagepub.com/doi/10.1177/2054358118776326},
volume = {5},
year = {2018}
}
@article{Frenck1998,
abstract = {A gradual loss of telomeric repeat sequences with aging previously has been noted in normal adult tissues, and this process has been implicated in cell senescence. No data exist that address the rate of telomere shortening in normal human cells within families or early in life. To address these questions, we measured telomere lengths in peripheral blood leukocytes (PBLs) from 75 members of 12 families and in a group of unrelated healthy children who were 5-48 months old. Here we report the surprising observation that rates of telomere attrition vary markedly at different ages. Telomeric repeats are lost rapidly (at a rate of {\textgreater}1 kilobase per year) from the PBLs of young children, followed by an apparent plateau between age 4 and young adulthood, and by gradual attrition later in life. These data suggest that the loss of telomeric repeats in hematopoietic cells is a dynamic process that is differentially regulated in young children and adults. Our results have implications for current models of how telomeric sequences are lost in normal somatic cells and suggest that PBLs are an excellent tissue to investigate how this process is controlled.},
author = {Frenck, R W and Blackburn, E H and Shannon, K M},
doi = {10.1073/pnas.95.10.5607},
issn = {0027-8424},
journal = {Proc. Natl. Acad. Sci. U. S. A.},
month = {may},
number = {10},
pages = {5607--10},
pmid = {9576930},
publisher = {National Academy of Sciences},
title = {{The rate of telomere sequence loss in human leukocytes varies with age.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/9576930 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC20425 http://www.ncbi.nlm.nih.gov/pubmed/9576930{\%}0Ahttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC20425},
volume = {95},
year = {1998}
}
@article{Love2014,
abstract = {In comparative high-throughput sequencing assays, a fundamental task is the analysis of count data, such as read counts per gene in RNA-seq, for evidence of systematic changes across experimental conditions. Small replicate numbers, discreteness, large dynamic range and the presence of outliers require a suitable statistical approach. We present DESeq2, a method for differential analysis of count data, using shrinkage estimation for dispersions and fold changes to improve stability and interpretability of estimates. This enables a more quantitative analysis focused on the strength rather than the mere presence of differential expression. The DESeq2 package is available at $\backslash$n http://www.bioconductor.org/packages/release/bioc/html/DESeq2.html$\backslash$n $\backslash$n .},
author = {Love, Michael I and Huber, Wolfgang and Anders, Simon},
doi = {10.1186/s13059-014-0550-8},
isbn = {1465-6906},
issn = {1474-760X},
journal = {Genome Biol.},
number = {12},
pages = {550},
pmid = {25516281},
publisher = {BioMed Central},
title = {{Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25516281 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4302049 http://genomebiology.biomedcentral.com/articles/10.1186/s13059-014-0550-8},
volume = {15},
year = {2014}
}
@article{Friedman1982,
abstract = {A general class of models for analysis of censored survival data with covariates is considered. If n individuals are observed over a time period divided into I(n) intervals, it is assumed that hj(t), the hazard rate function of the time to failure of the individual j, is constant and equal to hij{\textgreater}0 on the ith interval, and that the vector l = {\{}log hij:j=1, ..., n; i=1, ..., I(n){\}} lies in a linear subspace. The maximum likelihood estimate l or l provides a simultaneous estimate of the underlying hazard rate function, and of the effects of the covariates. Maximum likelihood equations and conditions for existence of l are given. The asymptotic properties of linear functions of l are studied in the general case where the true hazard rate function h0(t) is not a step function and I(n) increases without bound as the maximum interval length decreases. In comparison with recent work on regression analysis of survival data, the asymptotic results are obtained under more relaxed conditions on the regression variables.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Friedman, Michael},
doi = {10.1214/aos/1176345693},
eprint = {arXiv:1011.1669v3},
isbn = {00905364},
issn = {0090-5364},
journal = {Ann. Stat.},
keywords = {Asymptotic theory,censored data,log-linear model,maximum likelihood estimation,piecewise exponential model,survival data},
month = {mar},
number = {1},
pages = {101--113},
pmid = {20948974},
publisher = {Institute of Mathematical Statistics},
title = {{Piecewise Exponential Models for Survival Data with Covariates}},
url = {http://projecteuclid.org/euclid.aos/1176345693},
volume = {10},
year = {1982}
}
@article{Austin2012,
abstract = {When outcomes are binary, the c-statistic (equivalent to the area under the Receiver Operating Characteristic curve) is a standard measure of the predictive accuracy of a logistic regression model. An analytical expression was derived under the assumption that a continuous explanatory variable follows a normal distribution in those with and without the condition. We then conducted an extensive set of Monte Carlo simulations to examine whether the expressions derived under the assumption of binormality allowed for accurate prediction of the empirical c-statistic when the explanatory variable followed a normal distribution in the combined sample of those with and without the condition. We also examine the accuracy of the predicted c-statistic when the explanatory variable followed a gamma, log-normal or uniform distribution in combined sample of those with and without the condition. Under the assumption of binormality with equality of variances, the c-statistic follows a standard normal cumulative distribution function with dependence on the product of the standard deviation of the normal components (reflecting more heterogeneity) and the log-odds ratio (reflecting larger effects). Under the assumption of binormality with unequal variances, the c-statistic follows a standard normal cumulative distribution function with dependence on the standardized difference of the explanatory variable in those with and without the condition. In our Monte Carlo simulations, we found that these expressions allowed for reasonably accurate prediction of the empirical c-statistic when the distribution of the explanatory variable was normal, gamma, log-normal, and uniform in the entire sample of those with and without the condition. The discriminative ability of a continuous explanatory variable cannot be judged by its odds ratio alone, but always needs to be considered in relation to the heterogeneity of the population.},
author = {Austin, Peter C and Steyerberg, Ewout W},
doi = {10.1186/1471-2288-12-82},
issn = {1471-2288},
journal = {BMC Med. Res. Methodol.},
keywords = {Health Sciences,Medicine,Statistical Theory and Methods,Statistics for Life Sciences,Theory of Medicine/Bioethics},
month = {dec},
number = {1},
pages = {82},
publisher = {BioMed Central},
title = {{Interpreting the concordance statistic of a logistic regression model: relation to the variance and odds ratio of a continuous explanatory variable}},
url = {http://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-12-82},
volume = {12},
year = {2012}
}
@article{Reeves2008,
abstract = {The previous articles in this series discussed several methodological approaches commonly used by qualitative researchers in the health professions. This article focuses on another important qualitative meth-odology: ethnography. It provides background for those who will encounter this methodology in their reading rather than instructions for carrying out such research. What is ethnography? Ethnography is the study of social interactions, behaviours, and perceptions that occur within groups, teams, organisations, and communities. Its roots can be traced back to anthropological studies of small, rural (and often remote) societies that were undertaken in the early 1900s, when researchers such as Bronislaw Malinowski and Alfred Radcliffe-Brown participated in these societies over long periods and documented their social arrangements and belief systems. This approach was later adopted by members of the Chicago School of Sociology (for example, Everett Hughes, Robert Park, Louis Wirth) and applied to a variety of urban settings in their studies of social life. The central aim of ethnography is to provide rich, holistic insights into people's views and actions, as well as the nature (that is, sights, sounds) of the location they inhabit, through the collection of detailed observations and interviews. As Hammersley states, " The task [of ethnographers] is to document the culture, the perspectives and practices, of the people in these settings. The aim is to 'get inside' the way each group of people sees the world. " 1 Box 1 outlines the key features of ethnographic research. Examples of ethnographic research within the health services literature include Strauss's study of achieving and maintaining order between managers, clinicians, and patients within psychiatric hospital settings; Taxis and Barber's exploration of intravenous medication errors in acute care hospitals; Costello's examination of death and dying in elderly care wards; and {\O}sterlund's work on doctors' and nurses' use of traditional and digital information systems in their clinical communications. 3-6 Becker and colleagues' Boys in White, an ethnographic study of medical education in the late 1950s, remains a classic in this field.},
author = {Reeves, Scott and Kuper, Ayelet and Hodges, Brian David},
doi = {10.1136/bmj.a1020},
issn = {09598146},
journal = {BMJ},
month = {aug},
number = {7668},
pages = {512--514},
pmid = {18687725},
publisher = {British Medical Journal Publishing Group},
title = {{Qualitative research: Qualitative research methodologies: Ethnography}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18687725},
volume = {337},
year = {2008}
}
@article{Bates2015,
abstract = {Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.},
author = {Bates, Douglas and M{\"{a}}chler, Martin and Bolker, Ben and Walker, Steve},
doi = {10.18637/jss.v067.i01},
issn = {1548-7660},
journal = {J. Stat. Softw.},
keywords = {Cholesky decomposition,linear mixed models,penalized least squares,sparse matrix methods},
month = {oct},
number = {1},
pages = {1--48},
title = {{Fitting Linear Mixed-Effects Models Using lme4}},
url = {http://www.jstatsoft.org/v67/i01/},
volume = {67},
year = {2015}
}
@article{Webb2019a,
abstract = {Many complex natural and cultural phenomena are well modelled by systems of simple interactions between particles. A number of architectures have been developed to articulate this kind of structure, both implicitly and explicitly. We consider an unsupervised explicit model, the NRI model, and make a series of representational adaptations and physically motivated changes. Most notably we factorise the inferred latent interaction graph into a multiplex graph, allowing each layer to encode for a different interaction-type. This fNRI model is smaller in size and significantly outperforms the original in both edge and trajectory prediction, establishing a new state-of-the-art. We also present a simplified variant of our model, which demonstrates the NRI's formulation as a variational auto-encoder is not necessary for good performance, and make an adaptation to the NRI's training routine, significantly improving its ability to model complex physical dynamical systems.},
archivePrefix = {arXiv},
arxivId = {1905.08721},
author = {Webb, Ezra and Day, Ben and Andres-Terre, Helena and Li{\'{o}}, Pietro},
eprint = {1905.08721},
month = {may},
title = {{Factorised Neural Relational Inference for Multi-Interaction Systems}},
url = {http://arxiv.org/abs/1905.08721},
year = {2019}
}
@article{Blackburn2015,
abstract = {Telomeres are the protective end-complexes at the termini of eukaryotic chromosomes. Telomere attrition can lead to potentially maladaptive cellular changes, block cell division, and interfere with tissue replenishment. Recent advances in the understanding of human disease processes have clarified the roles of telomere biology, especially in diseases of human aging and in some aging-related processes. Greater overall telomere attrition predicts mortality and aging-related diseases in inherited telomere syndrome patients, and also in general human cohorts. However, genetically caused variations in telomere maintenance either raise or lower risks and progression of cancers, in a highly cancer type-specific fashion. Telomere maintenance is determined by genetic factors and is also cumulatively shaped by nongenetic influences throughout human life; both can interact. These and other recent findings highlight both causal and potentiating roles for telomere attrition in human diseases.},
author = {Blackburn, Elizabeth H and Epel, Elissa S and Lin, Jue},
doi = {10.1126/science.aab3389},
issn = {10959203},
journal = {Science (80-. ).},
month = {dec},
number = {6265},
pages = {1193--1198},
pmid = {26785477},
publisher = {American Association for the Advancement of Science},
title = {{Human telomere biology: A contributory and interactive factor in aging, disease risks, and protection}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26785477},
volume = {350},
year = {2015}
}
@article{Lum2013,
abstract = {This paper applies topological methods to study complex high dimensional data sets by extracting shapes (patterns) and obtaining insights about them. Our method combines the best features of existing standard methodologies such as principal component and cluster analyses to provide a geometric representation of complex data sets. Through this hybrid method, we often find subgroups in data sets that traditional methodologies fail to find. Our method also permits the analysis of individual data sets as well as the analysis of relationships between related data sets. We illustrate the use of our method by applying it to three very different kinds of data, namely gene expression from breast tumors, voting data from the United States House of Representatives and player performance data from the NBA, in each case finding stratifications of the data which are more refined than those produced by standard methods.},
author = {Lum, P. Y. and Singh, G. and Lehman, A. and Ishkanov, T. and Vejdemo-Johansson, M. and Alagappan, M. and Carlsson, J. and Carlsson, G.},
doi = {10.1038/srep01236},
issn = {20452322},
journal = {Sci. Rep.},
keywords = {Applied mathematics,Computational science,Scientific data,Software},
month = {dec},
number = {1},
pages = {1236},
publisher = {Nature Publishing Group},
title = {{Extracting insights from the shape of complex data using topology}},
url = {http://www.nature.com/articles/srep01236},
volume = {3},
year = {2013}
}
@article{Gomez-Cabrero2016a,
abstract = {{\textcopyright} 2016 The Author(s). Background: Deep mining of healthcare data has provided maps of comorbidity relationships between diseases. In parallel, integrative multi-omics investigations have generated high-resolution molecular maps of putative relevance for understanding disease initiation and progression. Yet, it is unclear how to advance an observation of comorbidity relations (one disease to others) to a molecular understanding of the driver processes and associated biomarkers. Results: Since Chronic Obstructive Pulmonary disease (COPD) has emerged as a central hub in temporal comorbidity networks, we developed a systematic integrative data-driven framework to identify shared disease-associated genes and pathways, as a proxy for the underlying generative mechanisms inducing comorbidity. We integrated records from approximately 13 M patients from the Medicare database with disease-gene maps that we derived from several resources including a semantic-derived knowledge-base. Using rank-based statistics we not only recovered known comorbidities but also discovered a novel association between COPD and digestive diseases. Furthermore, our analysis provides the first set of COPD co-morbidity candidate biomarkers, including IL15, TNF and JUP, and characterizes their association to aging and life-style conditions, such as smoking and physical activity. Conclusions: The developed framework provides novel insights in COPD and especially COPD co-morbidity associated mechanisms. The methodology could be used to discover and decipher the molecular underpinning of other comorbidity relationships and furthermore, allow the identification of candidate co-morbidity biomarkers.},
author = {Gomez-Cabrero, David and Menche, J{\"{o}}rg and Vargas, Claudia and Cano, Isaac and Maier, Dieter and Barab{\'{a}}si, Albert L{\'{a}}szl{\'{o}} and Tegn{\'{e}}r, Jesper and Roca, Josep},
doi = {10.1186/s12859-016-1291-3},
issn = {14712105},
journal = {BMC Bioinformatics},
keywords = {Algorithms,Bioinformatics,Computational Biology/Bioinformatics,Computer Appl. in Life Sciences,Microarrays},
month = {nov},
number = {S15},
pages = {441},
publisher = {BioMed Central},
title = {{From comorbidities of chronic obstructive pulmonary disease to identification of shared molecular mechanisms by data integration}},
url = {http://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1291-3},
volume = {17},
year = {2016}
}
@inproceedings{Tishby2015,
abstract = {Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network's simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.},
archivePrefix = {arXiv},
arxivId = {1503.02406},
author = {Tishby, Naftali and Zaslavsky, Noga},
booktitle = {2015 IEEE Inf. Theory Work. ITW 2015},
doi = {10.1109/ITW.2015.7133169},
eprint = {1503.02406},
isbn = {9781479955268},
month = {jun},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Deep learning and the information bottleneck principle}},
year = {2015}
}
@article{Dash2016,
abstract = {This paper proposes an improved version of CLONALG, Clone Selection Algorithm based on Artificial Immune System(AIS), that matches with the conventional classifiers in terms of accuracy tested on the same data sets. Clonal Selection Algorithm is an artificial immune system model. Instead of randomly selecting antibodies, it is proposed to take k memory pools consisting of all the learning cases. Also, an array averaged over the pools is created and is considered for cloning. Instead of using the best clone and calculating the similarity measure and comparing with the original cell, here, k best clones were selected, the average similarity measure was evaluated and noise was filtered. This process enhances the accuracy from 76.9 percentage to 94.2 percentage, ahead of the conventional classification methods.},
author = {Dash, Sanghamitra and Mishra, Rabindra Kishore and Panigrahy, Arijit and Das, Rama Krushna},
doi = {10.14738/tmlai.46.2562},
issn = {2054-7309},
journal = {Trans. Mach. Learn. Artif. Intell.},
keywords = {Accuracy,Antibody,Artificial Immune System,CLONALG,CSCA,classification,classifier,cloning,intrusion detection,machine learning},
month = {jan},
number = {6},
pages = {88},
title = {{E-CLONALG: A classifier based on Clonal Selection Algorithm}},
url = {http://scholarpublishing.org/index.php/TMLAI/article/view/2562 http://dx.doi.org/10.14738/tmlai.46.2562},
volume = {4},
year = {2016}
}
@article{Simidjievski2019,
abstract = {International initiatives such as the Molecular Taxonomy of Breast Cancer International Consortium (METABRIC), Cancer Genome Atlas (TCGA), and the International Cancer Genome Consortium (ICGC) are collecting multiple data sets at different genome-scales with the aim to identify novel cancer bio-markers and predict patient survival. To analyse such data, several machine learning, bioinformatics and statistical methods have been applied, among them neural networks such as autoencoders. Although these models provide a good statistical learning framework to analyse multi-omic and/or clinical data, there is a distinct lack of work on how to integrate diverse patient data and identify the optimal design best suited to the available data. In this paper, we investigate several autoencoder architectures that integrate a variety of cancer patient data types (e.g., multi-omics and clinical data). We perform extensive analyses of these approaches and provide a clear methodological and computational framework for designing systems that enable clinicians to investigate cancer traits and translate the results into clinical applications. We demonstrate how these networks can be designed, built and, in particular, applied to tasks of integrative analyses of heterogeneous breast cancer data. The results show that these approaches yield relevant data representations that, in turn, lead to accurate and stable diagnosis.},
author = {Simidjievski, Nikola and Bodnar, Cristian and Tariq, Ifrah and Scherer, Paul and Andres-Terre, Helena and Shams, Zohreh and Jamnik, Mateja and Li{\`{o}}, Pietro},
doi = {10.1101/719542},
journal = {bioRxiv},
month = {jul},
pages = {719542},
publisher = {Cold Spring Harbor Laboratory},
title = {{Variational Autoencoders for Cancer Data Integration: Design Principles and Computational Practice}},
url = {https://www.biorxiv.org/content/10.1101/719542v2},
year = {2019}
}
@article{Zhang2018,
abstract = {High-risk neuroblastoma is a very aggressive disease, with excessive tumor growth and poor outcomes. A proper stratification of the high-risk patients by prognostic outcome is important for treatment. However, there is still a lack of survival stratification for the high-risk neuroblastoma. To fill the gap, we adopt a deep learning algorithm, Autoencoder, to integrate multi-omics data, and combine it with K-means clustering to identify two subtypes with significant survival differences. By comparing the Autoencoder with PCA, iCluster, and DGscore about the classification based on multi-omics data integration, Autoencoder-based classification outperforms the alternative approaches. Furthermore, we also validated the classification in two independent datasets by training machine-learning classification models, and confirmed its robustness. Functional analysis revealed that MYCN amplification was more frequently occurred in the ultra-high-risk subtype, in accordance with the overexpression of MYC/MYCN targets in this subtype. In summary, prognostic subtypes identified by deep learning-based multi-omics integration could not only improve our understanding of molecular mechanism, but also help the clinicians make decisions.},
author = {Zhang, Li and Lv, Chenkai and Jin, Yaqiong and Cheng, Ganqi and Fu, Yibao and Yuan, Dongsheng and Tao, Yiran and Guo, Yongli and Ni, Xin and Shi, Tieliu},
doi = {10.3389/fgene.2018.00477},
issn = {1664-8021},
journal = {Front. Genet.},
keywords = {High-risk neuroblastoma,MYCN amplification,Multi-omics data Integration,deep learning,machine learning},
month = {oct},
pages = {477},
publisher = {Frontiers},
title = {{Deep Learning-Based Multi-Omics Data Integration Reveals Two Prognostic Subtypes in High-Risk Neuroblastoma}},
url = {https://www.frontiersin.org/article/10.3389/fgene.2018.00477/full},
volume = {9},
year = {2018}
}
@article{Nicholson2015,
abstract = {The vector autoregression (VAR) has long proven to be an effective method for modeling the joint dynamics of macroe- conomic time series as well as forecasting. A major shortcomings of the VAR that has hindered its applicability is its heavy parameterization: the parameter space grows quadratically with the number of series included, quickly exhausting the available degrees of freedom. Consequently, forecasting using VARs is intractable for low-frequency, high-dimensional macroeconomic data. However, empirical evidence suggests that VARs that incorporate more component series tend to result in more accurate forecasts. Conventional methods that allow for the estimation of large VARs either tend to require ad hoc subjective specifications or are computationally infeasible. Moreover, as global economies become more intricately intertwined, there has been substantial interest in incorporating the impact of stochastic, unmodeled exogenous variables. Vector autoregression with exogenous variables (VARX) extends the VAR to allow for the inclusion of unmodeled variables, but it similarly faces dimensionality challenges.   We introduce the VARX-L framework, a structured family of VARX models, and provide methodology which allows for both efficient estimation and accurate forecasting in high-dimensional analysis. VARX-L adapts several prominent scalar regression regularization techniques to a vector time series context to greatly reduce the parameter space of VAR and VARX models. We formulate convex optimization procedures that are amenable to efficient solutions for the time-ordered, high-dimensional problems we aim to solve. We also highlight a compelling extension that allows for shrinking toward reference models. We demonstrate the efficacy of VARX-L in both low- and high-dimensional macroeconomic applications and simulated data examples.},
archivePrefix = {arXiv},
arxivId = {1508.07497},
author = {Nicholson, William and Matteson, David and Bien, Jacob},
eprint = {1508.07497},
month = {aug},
title = {{VARX-L: Structured Regularization for Large Vector Autoregressions with Exogenous Variables}},
url = {http://arxiv.org/abs/1508.07497},
year = {2015}
}
@article{Malla2016,
abstract = {{\textcopyright} 2015, Springer-Verlag Berlin Heidelberg. Purpose: The objective of this review is to report on recent developments in youth mental health incorporating all levels of severity of mental disorders encouraged by progress in the field of early intervention in psychotic disorders, research in deficiencies in the current system and social advocacy. Methods: The authors have briefly reviewed the relevant current state of knowledge, challenges and the service and research response across four countries (Australia, Ireland, the UK and Canada) currently active in the youth mental health field. Results: Here we present information on response to principal challenges associated with improving youth mental services in each country. Australia has developed a model comprised of a distinct front-line youth mental health service (Headspace) to be implemented across the country and initially stimulated by success in early intervention in psychosis; in Ireland, Headstrong has been driven primarily through advocacy and philanthropy resulting in front-line services (Jigsaw) which are being implemented across different jurisdictions; in the UK, a limited regional response has addressed mostly problems with transition from child–adolescent to adult mental health services; and in Canada, a national multi-site research initiative involving transformation of youth mental health services has been launched with public and philanthropic funding, with the expectation that results of this study will inform implementation of a transformed model of service across the country including indigenous peoples. Conclusions: There is evidence that several countries are now engaged in transformation of youth mental health services and in evaluation of these initiatives.},
author = {Malla, Ashok and Iyer, Srividya and McGorry, Patrick and Cannon, Mary and Coughlan, Helen and Singh, Swaran and Jones, Peter and Joober, Ridha},
doi = {10.1007/s00127-015-1165-4},
issn = {09337954},
journal = {Soc. Psychiatry Psychiatr. Epidemiol.},
keywords = {Early intervention,Service delivery,Youth mental health},
month = {mar},
number = {3},
pages = {319--326},
publisher = {Springer Berlin Heidelberg},
title = {{From early intervention in psychosis to youth mental health reform: a review of the evolution and transformation of mental health services for young people}},
url = {http://link.springer.com/10.1007/s00127-015-1165-4},
volume = {51},
year = {2016}
}
@article{Lee2018c,
abstract = {One broad goal of biomedical informatics is to generate fully-synthetic, faithfully representative electronic health records (EHRs) to facilitate data sharing between healthcare providers and researchers and promote methodological research. A variety of methods existing for generating synthetic EHRs, but they are not capable of generating unstructured text, like emergency department (ED) chief complaints, history of present illness, or progress notes. Here, we use the encoder-decoder model, a deep learning algorithm that features in many contemporary machine translation systems, to generate synthetic chief complaints from discrete variables in EHRs, like age group, gender, and discharge diagnosis. After being trained end-to-end on authentic records, the model can generate realistic chief complaint text that appears to preserve the epidemiological information encoded in the original record-sentence pairs. As a side effect of the model's optimization goal, these synthetic chief complaints are also free of relatively uncommon abbreviation and misspellings, and they include none of the personally identifiable information (PII) that was in the training data, suggesting that this model may be used to support the de-identification of text in EHRs. When combined with algorithms like generative adversarial networks (GANs), our model could be used to generate fully-synthetic EHRs, allowing healthcare providers to share faithful representations of multimodal medical data without compromising patient privacy. This is an important advance that we hope will facilitate the development of machine-learning methods for clinical decision support, disease surveillance, and other data-hungry applications in biomedical informatics.},
author = {Lee, Scott H},
issn = {2398-6352},
journal = {NPJ Digit. Med.},
pages = {63},
pmid = {30687797},
title = {{Natural language generation for electronic health records.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/30687797{\%}0Ahttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC6345174},
volume = {1},
year = {2018}
}
@misc{opendata_mental_health,
title = {{Open data mental health}},
url = {https://sites.google.com/view/avec2019/home/important-dates},
urldate = {2019-04-26}
}
@article{Caetano2018,
author = {Caetano, S.J. and Sonpavde, G. and Pond, G.R.},
doi = {10.1016/J.EJCA.2017.10.027},
issn = {0959-8049},
journal = {Eur. J. Cancer},
month = {feb},
pages = {130--132},
publisher = {Pergamon},
title = {{C-statistic: A brief explanation of its construction, interpretation and limitations}},
url = {https://www.sciencedirect.com/science/article/pii/S0959804917313679?via{\%}3Dihub},
volume = {90},
year = {2018}
}
@article{McCarthy2017,
abstract = {Motivation: Single-cell RNA sequencing (scRNA-seq) is increasingly used to study gene expression at the level of individual cells. However, preparing raw sequence data for further analysis is not a straightforward process. Biases, artifacts and other sources of unwanted variation are present in the data, requiring substantial time and effort to be spent on pre-processing, quality control (QC) and normalization. Results: We have developed the R/Bioconductor package scater to facilitate rigorous pre-processing, quality control, normalization and visualization of scRNA-seq data. The package provides a convenient, flexible workflow to process raw sequencing reads into a high-quality expression dataset ready for downstream analysis. scater provides a rich suite of plotting tools for single-cell data and a flexible data structure that is compatible with existing tools and can be used as infrastructure for future software development.},
author = {McCarthy, Davis J. and Campbell, Kieran R. and Lun, Aaron T.L. and Wills, Quin F.},
doi = {10.1093/bioinformatics/btw777},
issn = {14602059},
journal = {Bioinformatics},
month = {jan},
number = {8},
pages = {1179--1186},
title = {{Scater: Pre-processing, quality control, normalization and visualization of single-cell RNA-seq data in R}},
url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btw777},
volume = {33},
year = {2017}
}
@article{DeAngelis2014,
abstract = {Planning, implementation and evaluation of public health policies to control the human immunodeficiency virus (HIV) epidemic require regular monitoring of disease burden. This includes the proportion living with HIV, whether diagnosed or not, and the rate of new infections in the general population and in specific risk groups and regions. Estimation of these quantities is not straightforward: data informing them directly are not typically available, but a wealth of indirect information from surveillance systems and ad hoc studies can inform functions of these quantities. In this paper we show how the estimation problem can be successfully solved through a Bayesian evidence synthesis approach, relaxing the focus on "best available" data to which classical methods are typically restricted. This more comprehensive and flexible use of evidence has led to the adoption of our proposed approach as the official method to estimate HIV prevalence in the United Kingdom since 2005.},
archivePrefix = {arXiv},
arxivId = {1405.4679},
author = {{De Angelis}, Daniela and Presanis, Anne M. and Conti, Stefano and Ades, A. E.},
doi = {10.1214/13-STS428},
eprint = {1405.4679},
issn = {0883-4237},
journal = {Stat. Sci.},
keywords = {Bayesian inference,HIV,disease burden,evidence synthesis,graphical model},
month = {feb},
number = {1},
pages = {9--17},
publisher = {Institute of Mathematical Statistics},
title = {{Estimation of HIV Burden through Bayesian Evidence Synthesis}},
url = {http://projecteuclid.org/euclid.ss/1399645723},
volume = {29},
year = {2014}
}
@article{Monfardini2008,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Monfardini, G. and {Ah Chung Tsoi} and Hagenbuchner, M. and Scarselli, F. and Gori, M.},
doi = {10.1109/tnn.2008.2005605},
eprint = {arXiv:1011.1669v3},
isbn = {1045-9227 VO - 20},
issn = {1045-9227},
journal = {IEEE Trans. Neural Networks},
month = {jan},
number = {1},
pages = {61--80},
pmid = {19068426},
title = {{The Graph Neural Network Model}},
url = {http://ieeexplore.ieee.org/document/4700287/},
volume = {20},
year = {2008}
}
@article{Coppen1983,
abstract = {In a prospective double-blind trial we examined the affective morbility and side-effects of 72 patients who were randomly allocated either to continue with their usual dose of lithium or to receive either a 25{\%} or 50{\%} reduction in lithium dosage. Patients who underwent a dosage reduction with consequently lower plasma lithium levels (0.45-0.79 mmol/1) had significantly decreased affective morbidity. Thyroid stimulating hormone levels were also significantly decreased in this group. Total subjective side-effects score and tremor were also reduced. No change in affective morbidity was observed during the trial in patients whose dosage was not altered. These changes were observed in both unipolar and bipolar patients. It was concluded that a once a day dosage with a sustained release lithium preparation that maintained a 12-h plasma level of about 0.6 mmol/1 is both more effective and produces less side effects than does conventional dosages. {\textcopyright} 1983.},
author = {Coppen, A. and Abou-Saleh, M. and Milln, P. and Bailey, J. and Wood, K.},
doi = {10.1016/0165-0327(83)90026-5},
issn = {01650327},
journal = {J. Affect. Disord.},
month = {nov},
number = {4},
pages = {353--362},
pmid = {6229567},
title = {{Decreasing lithium dosage reduces morbidity and side-effects during prophylaxis}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/6229567 https://linkinghub.elsevier.com/retrieve/pii/0165032783900265},
volume = {5},
year = {1983}
}
@article{Dinga2019,
abstract = {Pattern recognition predictive models have become an important tool for analysis of neuroimaging data and answering important questions from clinical and cognitive neuroscience. Regardless of the application, the most commonly used method to quantify model performance is to calculate prediction accuracy, i.e. the proportion of correctly classified samples. While simple and intuitive, other performance measures are often more appropriate with respect to many common goals of neuroimaging pattern recognition studies. In this paper, we will review alternative performance measures and focus on their interpretation and practical aspects of model evaluation. Specifically, we will focus on 4 families of performance measures: 1) categorical performance measures such as accuracy, 2) rank based performance measures such as the area under the curve, 3) probabilistic performance measures based on quadratic error such as Brier score, and 4) probabilistic performance measures based on information criteria such as logarithmic score. We will examine their statistical properties in various settings using simulated data and real neuroimaging data derived from public datasets. Results showed that accuracy had the worst performance with respect to statistical power, detecting model improvement, selecting informative features and reliability of results. Therefore in most cases, it should not be used to make statistical inference about model performance. Accuracy should also be avoided for evaluating utility of clinical models, because it does not take into account clinically relevant information, such as relative cost of false-positive and false-negative misclassification or calibration of probabilistic predictions. We recommend alternative evaluation criteria with respect to the goals of a specific machine learning model.},
author = {Dinga, Richard and Penninx, Brenda W.J.H. and Veltman, Dick J. and Schmaal, Lianne and Marquand, Andre F.},
doi = {10.1101/743138},
journal = {bioRxiv},
month = {aug},
pages = {743138},
publisher = {Cold Spring Harbor Laboratory},
title = {{Beyond accuracy: Measures for assessing machine learning models, pitfalls and guidelines}},
url = {https://www.biorxiv.org/content/10.1101/743138v1?rss=1},
year = {2019}
}
@article{Graff2014,
abstract = {We present the first public release of our generic neural network training algorithm, called SKYNET. This efficient and robust machine learning tool is able to train large and deep feed-forward neural networks, including autoencoders, for use in a wide range of supervised and unsupervised learning applications, such as regression, classification, density estimation, clustering and dimensionality reduction. SKYNET uses a `pre-training' method to obtain a set of network parameters that has empirically been shown to be close to a good solution, followed by further optimization using a regularized variant of Newton's method, where the level of regularization is determined and adjusted automatically; the latter uses second-order derivative information to improve convergence, but without the need to evaluate or store the full Hessian matrix, by using a fast approximate method to calculate Hessian-vector products. This combination of methods allows for the training of complicated networks that are difficult to optimize using standard backpropagation techniques. SKYNET employs convergence criteria that naturally prevent overfitting, and also includes a fast algorithm for estimating the accuracy of network outputs. The utility and flexibility of SKYNET are demonstrated by application to a number of toy problems, and to astronomical problems focusing on the recovery of structure from blurred and noisy images, the identification of gamma-ray bursters, and the compression and denoising of galaxy images. The SKYNET software, which is implemented in standard ANSI C and fully parallelized using MPI, is available at http://www.mrao.cam.ac.uk/software/skynet/.},
archivePrefix = {arXiv},
arxivId = {1309.0790},
author = {Graff, Philip and Feroz, Farhan and Hobson, Michael P. and Lasenby, Anthony},
doi = {10.1093/mnras/stu642},
eprint = {1309.0790},
issn = {13652966},
journal = {Mon. Not. R. Astron. Soc.},
keywords = {Methods: data analysis,Methods: statistical},
month = {jun},
number = {2},
pages = {1741--1759},
title = {{SKYNET: An efficient and robust neural network training tool for machine learning in astronomy}},
url = {http://arxiv.org/abs/1309.0790 http://dx.doi.org/10.1093/mnras/stu642 http://academic.oup.com/mnras/article/441/2/1741/1071156/SkyNet-an-efficient-and-robust-neural-network},
volume = {441},
year = {2014}
}
@article{Calhoun2018,
abstract = {Multivariate survival trees require few statistical assumptions, are easy to interpret, and provide meaningful diagnosis and prediction rules. Trees can handle a large number of predictors with mixed types and do not require predictor variable transformation or selection. These are useful features in many application fields and are often required in the current era of big data. The aim of this article is to introduce the R package MST. This package constructs multivariate survival trees using marginal model and frailty model based approaches. It allows the user to control and see how the trees are constructed. The package can also simulate high-dimensional, multivariate survival data from marginal and frailty models.},
author = {Calhoun, Peter and Su, Xiaogang and Nunn, Martha and Fan, Juanjuan},
doi = {10.18637/jss.v083.i12},
issn = {1548-7660},
journal = {J. Stat. Softw.},
keywords = {CART,R,frailty model,marginal model,multivariate survival trees},
month = {feb},
number = {12},
pages = {1--21},
title = {{Constructing Multivariate Survival Trees: The MST Package for R}},
url = {http://www.jstatsoft.org/v83/i12/},
volume = {83},
year = {2018}
}
@book{Goodfellow_deeplearning_book,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
publisher = {MIT Press},
title = {{Deep Learning}},
url = {https://www.deeplearningbook.org/},
year = {2016}
}
@article{Spasov2018a,
abstract = {Some forms of mild cognitive impairment (MCI) can be the clinical precursor of severe dementia like Alzheimers disease (AD), while other types of MCI tend to remain stable over-time and do not progress to AD pathology. To choose an effective and personalized treatment for AD, we need to identify which MCI patients are at risk of developing AD and which are not. Here, we present a novel deep learning architecture, based on dual learning and an ad hoc layer for 3D separable convolutions, which aims at identifying those people with MCI who have a high likelihood of developing AD. Our deep learning procedures combine structural magnetic resonance imaging (MRI), demographic, neuropsychological, and APOe4 genotyping data as input measures. The most novel characteristics of our machine learning model compared to previous ones are as follows: 1) multi-tasking, in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion, and AD vs healthy classification which facilitates the relevant feature extraction for prognostication; 2) the neural network classifier employs relatively few parameters compared to other deep learning architectures (we use {\~{}}500,000 network parameters, orders of magnitude lower than other network designs) without compromising network complexity and hence significantly limits data-overfitting; 3) both structural MRI images and warp field characteristics, which quantify the amount of volumetric change compared to the common template, were used as separate input streams to extract as much information as possible from the MRI data. All the analyses were performed on a subset of the Alzheimers Disease Neuroimaging Initiative (ADNI) database, for a total of n=785 participants (192 AD, 409 MCI, and184 healthy controls (HC)). We found that the most predictive combination of inputs included the structural MRI images and the demographic, neuropsychological, and APOe4 data, while the warp field metric added little predictive value. We achieved an area under the ROC curve (AUC) of 0.925 with a 10-fold cross-validated accuracy of 86{\%}, a sensitivity of 87.5{\%} and specificity of 85{\%} in classifying MCI patients who developed AD in three years' time from those individuals showing stable MCI over the same time-period. To the best of our knowledge, this is the highest performance reported on a test set achieved in the literature using similar data. The same network provided an AUC of 1 and 100{\%} accuracy, sensitivity and specificity when classifying NC from AD. We also demonstrated that our classification framework was robust to different co-registration templates and possibly irrelevant features / image sections. Our approach is flexible and can in principle integrate other imaging modalities, such as PET, and a more diverse group of clinical data. The convolutional framework is potentially applicable to any 3D image dataset and gives the flexibility to design a computer-aided diagnosis system targeting the prediction of any medical condition utilizing multi-modal imaging and tabular clinical data.},
author = {Spasov, Simeon and Passamonti, Luca and Duggento, Andrea and Lio, Pietro and Toschi, Nicola},
doi = {10.1101/383687},
journal = {bioRxiv},
month = {aug},
pages = {383687},
publisher = {Cold Spring Harbor Laboratory},
title = {{A parameter-efficient deep learning approach to predict conversion from mild cognitive impairment to Alzheimer's disease}},
url = {https://www.biorxiv.org/content/early/2018/08/02/383687 https://www.biorxiv.org/content/early/2018/11/15/383687?{\%}3Fcollection=},
year = {2018}
}
@misc{vdjdb_website,
title = {{VDJdb browser}},
url = {https://vdjdb.cdr3.net/},
urldate = {2019-12-18}
}
@misc{keras_software,
author = {Chollet, F},
title = {keras},
url = {https://github.com/keras-team/keras},
year = {2015}
}
@article{Blundell2015,
abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1505.05424},
author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
eprint = {1505.05424},
month = {may},
title = {{Weight Uncertainty in Neural Networks}},
url = {http://arxiv.org/abs/1505.05424},
year = {2015}
}
@article{Polson2013,
abstract = {We propose a new data-augmentation strategy for fully Bayesian inference in models with binomial likelihoods. The approach appeals to a new class of Polya-Gamma distributions, which are constructed in detail. A variety of examples are presented to show the versatility of the method, including logistic regression, negative binomial regression, nonlinear mixed-effects models, and spatial models for count data. In each case, our data-augmentation strategy leads to simple, effective methods for posterior inference that: (1) circumvent the need for analytic approximations, numerical integration, or Metropolis-Hastings; and (2) outperform other known data-augmentation strategies, both in ease of use and in computational efficiency. All methods, including an efficient sampler for the Polya-Gamma distribution, are implemented in the R package BayesLogit. In the technical supplement appended to the end of the paper, we provide further details regarding the generation of Polya-Gamma random variables; the empirical benchmarks reported in the main manuscript; and the extension of the basic data-augmentation framework to contingency tables and multinomial outcomes.},
archivePrefix = {arXiv},
arxivId = {1205.0310},
author = {Polson, Nicholas G. and Scott, James G. and Windle, Jesse},
doi = {10.1080/01621459.2013.829001},
eprint = {1205.0310},
isbn = {0025-5408},
issn = {1537274X},
journal = {J. Am. Stat. Assoc.},
keywords = {Bayesian methods,Data augmentation,Logistic regression,Negative binomial regression,P{\'{o}}lya-Gamma distribution},
month = {dec},
number = {504},
pages = {1339--1349},
publisher = {Taylor {\&} Francis Group},
title = {{Bayesian inference for logistic models using P{\'{o}}lya-Gamma latent variables}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.2013.829001},
volume = {108},
year = {2013}
}
@article{Dehning2017,
abstract = {{\textcopyright} 2017 Grunze et al. Background: The optimal duration of antidepressant treatment in bipolar depression appears to be controversial due to a lack of quality evidence, and guideline recommendations are either vague or contradictive. This is especially true for second line treatments such as bupropion that had not been subject to rigourous long term studies in Bipolar Disorder. Case presentation: We report the case of a 75 year old woman who presented with treatment refractory bipolar depression. Because of insufficient response to previous mood stabilizer treatment and refractory depressive symptoms, bupropion was added to venlafaxine and lamotrigine. From there onwards, the patient improved continuously without experiencing deterioration of depression or a switch into hypomania. Our patient being on antidepressants for allmost four years experienced an obvious benefit from longterm antidepressant administration. Conclusion: Noradrenergic/dopaminergic mechanisms of action may play a more prominent role in bipolar depression, and may still be underused as a therapeutic strategy in the acute phase as well as in long-term maintenance in at least a subgroup of bipolar patients. There is still a lack of evidence from RCTs, but this case report further supports antidepressant long-term continuation and the usefulness of a noradrenergic/dopaminergic antidepressant in the acute and maintenance treatment of bipolar disorder.},
author = {Dehning, Julia and Grunze, Heinz and Hausmann, Armand},
doi = {10.2174/1745017901713010043},
issn = {1745-0179},
journal = {Clin. Pract. Epidemiol. Ment. Heal.},
keywords = {Bipolar disorder,Bupropion,Depression,Guidelines,Maintenance treatment,TEAS},
number = {1},
pages = {43--48},
pmid = {28659991},
publisher = {Bentham Science Publishers},
title = {{Bupropion Maintenance Treatment in Refractory Bipolar Depression: A Case Report}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28659991 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5470062},
volume = {13},
year = {2017}
}
@article{Simidjievski2019b,
author = {Simidjievski, Nikola and Bodnar, Cristian and Tariq, Ifrah and Scherer, Paul and {Andres Terre}, Helena and Shams, Zohreh and Jamnik, Mateja and Li{\`{o}}, Pietro},
doi = {10.3389/fgene.2019.01205},
issn = {1664-8021},
journal = {Front. Genet.},
month = {dec},
title = {{Variational Autoencoders for Cancer Data Integration: Design Principles and Computational Practice}},
url = {https://www.frontiersin.org/article/10.3389/fgene.2019.01205/full},
volume = {10},
year = {2019}
}
@article{Post2018,
abstract = {Lithium use for the treatment of mood disorders remains quite low, particularly in the United States compared with some European countries. Mogens Schou pioneered the study of lithium for prophylaxis of the recurrent mood disorder and encouraged its greater use. In an effort to further address the appropriate role of this drug, the multiple assets of lithium beyond its well-known antimanic effect are reviewed, and a brief summary of its side effects is outlined. It appears that lithium has positive effects in depression and suicide prevention, cognition, and reducing the incidence of dementia. It increases the length of telomeres and has positive effects in prevention of some medical illnesses. Lithium side-effect burden, especially its association with end-stage renal disease, may be less than many have surmised. New data indicate the importance of long-term prophylaxis after a first manic episode to lessen episode recurrence, allow cognition to recover to normal, and prevent various aspects of illness progression. After a first manic episode, 1 year of randomized treatment with lithium was superior to that of quetiapine, suggesting the importance of having lithium in the treatment regimen. Given the highly recurrent and progressive course of bipolar disorder sometimes even in the face of conventional treatment, the role and enhanced use of lithium deserves reconsideration.},
author = {Post, Robert M},
doi = {10.1038/npp.2017.238},
issn = {1740634X},
journal = {Neuropsychopharmacology},
keywords = {Bipolar disorder,Pharmacology},
month = {apr},
number = {5},
pages = {1174--1179},
publisher = {Nature Publishing Group},
title = {{The New News about Lithium: An Underutilized Treatment in the United States}},
url = {http://www.nature.com/articles/npp2017238},
volume = {43},
year = {2018}
}
@article{Kowsar2017,
abstract = {{\textcopyright} 2017 The Author(s). After intercourse/insemination, large numbers of sperm are deposited in the female reproductive tract (FRT), triggering a massive recruitment of neutrophils (PMNs) into the FRT, possibly to eliminate excessive sperm via phagocytosis. Some bovine oviductal fluid components (BOFCs) have been shown to regulate in vitro sperm phagocytosis (spermophagy) by PMNs. The modeling approach-based logistic regression (LR) and autoregressive logistic regression (ALR) can be used to predict the behavior of complex biological systems. We, first, compared the LR and ALR models using in vitro data to find which of them provides a better prediction of in vitro spermophagy in bovine. Then, the best model was used to identify and classify the reciprocal effects of BOFCs in regulating spermophagy. The ALR model was calibrated using an iterative procedure with a dynamical search direction. The superoxide production data were used to illustrate the accuracy in validating logit model-based ALR and LR. The ALR model was more accurate than the LR model. Based on in vitro data, the ALR predicted that the regulation of spermophagy by PMNs in bovine oviduct is more sensitive to alpha-1 acid glycoprotein (AGP), PGE2, bovine serum albumin (BSA), and to the combination of AGP or BSA with other BOFCs.},
author = {Kowsar, Rasoul and Keshtegar, Behrooz and Marey, Mohamed. A. and Miyamoto, Akio},
doi = {10.1038/s41598-017-04841-z},
issn = {20452322},
journal = {Sci. Rep.},
keywords = {Endocrine system and metabolic diseases,Statistical methods},
month = {dec},
number = {1},
pages = {4482},
publisher = {Nature Publishing Group},
title = {{An autoregressive logistic model to predict the reciprocal effects of oviductal fluid components on in vitro spermophagy by neutrophils in cattle}},
url = {http://www.nature.com/articles/s41598-017-04841-z},
volume = {7},
year = {2017}
}
@techreport{Minsky1974,
abstract = {Here is the essence of the theory: When one encounters a new situation (or makes a substantial change in one's view of the present problem) one selects from memory a structure called a Frame. This is a remembered framework to be adapted to fit reality by changing details as necessary.

A frame is a data-structure for representing a stereotyped situation, like being in a certain kind of living room, or going to a child's birthday party. Attached to each frame are several kinds of information. Some of this information is about how to use the frame. Some is about what one can expect to happen next.},
author = {Minsky, Marvin},
title = {{A Framework for Representing Knowledge}},
url = {http://web.media.mit.edu/{~}minsky/papers/Frames/frames.html},
year = {1974}
}
@article{Ching2018,
abstract = {Abstract Deep learning excels in vision and speech applications where it pushed the state- of-the-art to a new level. However its impact on other fields remains to be shown. The Merck Kaggle challenge on chemical compound activity was won by Hin- ton's group with deep networks. This indicates the high potential of deep learning in drug design and attracted the attention of big pharma. However, the unreal- istically small scale of the Kaggle dataset does not allow to assess the value of deep learning in drug target prediction if applied to in-house data of pharmaceu- tical companies. Even a publicly available drug activity data base like ChEMBL is magnitudes larger than the Kaggle dataset. ChEMBL has 13M compound de- scriptors, 1.3Mcompounds, and 5 k drug targets, compared to the Kaggle dataset with 11 k descriptors, 164 k compounds, and 15 drug targets. On the ChEMBL database, we compared the performance of deep learning to seven target prediction methods, including two commercial predictors, three pre- dictors deployed by pharma, and machine learning methods that we could scale to this dataset. Deep learning outperformed all other methods with respect to the area under ROC curve and was significantly better than all commercial products. Deep learning surpassed the threshold to make virtual compound screening possible and has the potential to become a standard tool in industrial drug design.},
archivePrefix = {arXiv},
arxivId = {142760},
author = {Ching, Travers and Himmelstein, Daniel S and Beaulieu-Jones, Brett K and Kalinin, Alexandr A and Do, Brian T and Way, Gregory P and Ferrero, Enrico and Agapow, Paul Michael and Zietz, Michael and Hoffman, Michael M and Xie, Wei and Rosen, Gail L and Lengerich, Benjamin J and Israeli, Johnny and Lanchantin, Jack and Woloszynek, Stephen and Carpenter, Anne E and Shrikumar, Avanti and Xu, Jinbo and Cofer, Evan M and Lavender, Christopher A and Turaga, Srinivas C and Alexandari, Amr M and Lu, Zhiyong and Harris, David J and Decaprio, Dave and Qi, Yanjun and Kundaje, Anshul and Peng, Yifan and Wiley, Laura K and Segler, Marwin H.S. and Boca, Simina M and Swamidass, S Joshua and Huang, Austin and Gitter, Anthony and Greene, Casey S},
doi = {10.1098/rsif.2017.0387},
eprint = {142760},
isbn = {0000000305396},
issn = {17425662},
journal = {J. R. Soc. Interface},
keywords = {deep learning,genomics,machine learning,precision medicine},
month = {apr},
number = {141},
pages = {20170387},
pmid = {1000224777},
publisher = {The Royal Society},
title = {{Opportunities and obstacles for deep learning in biology and medicine}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/29618526 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5938574},
volume = {15},
year = {2018}
}
@article{Brannen2000,
abstract = {This study expands theoretical research on negotiated culture by testing basic assumptions in the context of a German-Japanese joint venture. Data collected by semi-structured interviews are analyzed using textual analysis software to uncover key issues that became catalysts for negotiation. Results include a model of cultural negotiation linking organizational events with issue domains as points of departure for negotiations. Results show that aggregate models of cultural difference are useful only to the extent that they serve as latent conceptual anchors guiding individuals' cultural responses to events. The study shows that structural/contextual influences together with individuals' culturally determined sense-making with regard to specific organizational events are more useful determinants of negotiated outcomes. Authors conclude that, while it is unlikely we can predict organizational culture formation in complex cultural organizations, we can understand the process of cultural negotiation and as a result be better prepared to monitor and manage in culturally diverse settings.},
author = {Brannen, Mary Yoko and Salk, Jane E.},
doi = {10.1177/0018726700534001},
issn = {00187267},
journal = {Hum. Relations},
keywords = {Complex cultural systems,Joint ventures,Negotiated order,Organizational change,Organizational culture},
month = {apr},
number = {4},
pages = {451--487},
publisher = {Sage PublicationsSage CA: Thousand Oaks, CA},
title = {{Partnering across borders: Negotiating organizational culture in a German-Japanese joint venture}},
url = {http://journals.sagepub.com/doi/10.1177/0018726700534001},
volume = {53},
year = {2000}
}
@article{Lasko2013,
abstract = {Inferring precise phenotypic patterns from population-scale clinical data is a core computational task in the development of precision, personalized medicine. The traditional approach uses supervised learning, in which an expert designates which patterns to look for (by specifying the learning task and the class labels), and where to look for them (by specifying the input variables). While appropriate for individual tasks, this approach scales poorly and misses the patterns that we don't think to look for. Unsupervised feature learning overcomes these limitations by identifying patterns (or features) that collectively form a compact and expressive representation of the source data, with no need for expert input or labeled examples. Its rising popularity is driven by new deep learning methods, which have produced high-profile successes on difficult standardized problems of object recognition in images. Here we introduce its use for phenotype discovery in clinical data. This use is challenging because the largest source of clinical data – Electronic Medical Records – typically contains noisy, sparse, and irregularly timed observations, rendering them poor substrates for deep learning methods. Our approach couples dirty clinical data to deep learning architecture via longitudinal probability densities inferred using Gaussian process regression. From episodic, longitudinal sequences of serum uric acid measurements in 4368 individuals we produced continuous phenotypic features that suggest multiple population subtypes, and that accurately distinguished (0.97 AUC) the uric-acid signatures of gout vs. acute leukemia despite not being optimized for the task. The unsupervised features were as accurate as gold-standard features engineered by an expert with complete knowledge of the domain, the classification task, and the class labels. Our findings demonstrate the potential for achieving computational phenotype discovery at population scale. We expect such data-driven phenotypes to expose unknown disease variants and subtypes and to provide rich targets for genetic association studies. Citation: Lasko TA, Denny JC, Levy MA (2013) Computational Phenotype Discovery Using Unsupervised Feature Learning over Noisy, Sparse, and Irregular Clinical Data. PLoS ONE 8(6): e66341.},
author = {Lasko, Thomas A. and Denny, Joshua C. and Levy, Mia A.},
doi = {10.1371/journal.pone.0066341},
editor = {Devaney, Joseph},
issn = {19326203},
journal = {PLoS One},
month = {jun},
number = {6},
pages = {e66341},
publisher = {Public Library of Science},
title = {{Computational Phenotype Discovery Using Unsupervised Feature Learning over Noisy, Sparse, and Irregular Clinical Data}},
url = {https://dx.plos.org/10.1371/journal.pone.0066341},
volume = {8},
year = {2013}
}
@misc{limma_use_tutorial,
title = {{Using limma for microarray analysis}},
url = {http://genomicsclass.github.io/book/pages/using{\_}limma.html},
urldate = {2017-12-04}
}
@article{Douer2018,
abstract = {The Responsibility Quantification (ResQu) Model of Human Interaction with Automation http://arxiv.org/abs/1810.12644},
archivePrefix = {arXiv},
arxivId = {1810.12644},
author = {Douer, Nir and Meyer, Joachim and Member, Senior},
eprint = {1810.12644},
month = {oct},
pages = {1--13},
title = {{The Responsibility Quantification (ResQu) Model of Human Interaction with Automation}},
url = {http://arxiv.org/abs/1810.12644 https://twitter.com/arxiv{\_}cshc/status/1057433020230221824},
year = {2018}
}
@inproceedings{Izzo2017,
abstract = {We introduce the use of high order automatic differentiation, implemented via the algebra of truncated Taylor polynomials, in genetic programming. Using the Cartesian Genetic Programming encoding we obtain a high-order Taylor representation of the program output that is then used to back-propagate errors during learning. The resulting machine learning framework is called differentiable Cartesian Genetic Programming (dCGP). In the context of symbolic regression, dCGP offers a new approach to the long unsolved problem of constant representation in GP expressions. On several problems of increasing complexity we find that dCGP is able to find the exact form of the symbolic expression as well as the constants values. We also demonstrate the use of dCGP to solve a large class of differential equations and to find prime integrals of dynamical systems, presenting, in both cases, results that confirm the efficacy of our approach.},
archivePrefix = {arXiv},
arxivId = {1611.04766},
author = {Izzo, Dario and Biscani, Francesco and Mereta, Alessio},
booktitle = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
doi = {10.1007/978-3-319-55696-3_3},
eprint = {1611.04766},
isbn = {9783319556956},
issn = {16113349},
keywords = {Back-propagation,Genetic programming,Machine learning,Symbolic regression,Truncated Taylor polynomials},
pages = {35--51},
publisher = {Springer, Cham},
title = {{Differentiable genetic programming}},
url = {http://link.springer.com/10.1007/978-3-319-55696-3{\_}3},
volume = {10196 LNCS},
year = {2017}
}
@article{Alber2019,
abstract = {Fueled by breakthrough technology developments, the biological, biomedical, and behavioral sciences are now collecting more data than ever before. There is a critical need for time- and cost-efficient strategies to analyze and interpret these data to advance human health. The recent rise of machine learning as a powerful technique to integrate multimodality, multifidelity data, and reveal correlations between intertwined phenomena presents a special opportunity in this regard. However, classical machine learning techniques often ignore the fundamental laws of physics and result in ill-posed problems or non-physical solutions. Multiscale modeling is a successful strategy to integrate multiscale, multiphysics data and uncover mechanisms that explain the emergence of function. However, multiscale modeling alone often fails to efficiently combine large data sets from different sources and different levels of resolution. We show how machine learning and multiscale modeling can complement each other to create robust predictive models that integrate the underlying physics to manage ill-posed problems and explore massive design spaces. We critically review the current literature, highlight applications and opportunities, address open questions, and discuss potential challenges and limitations in four overarching topical areas: ordinary differential equations, partial differential equations, data-driven approaches, and theory-driven approaches. Towards these goals, we leverage expertise in applied mathematics, computer science, computational biology, biophysics, biomechanics, engineering mechanics, experimentation, and medicine. Our multidisciplinary perspective suggests that integrating machine learning and multiscale modeling can provide new insights into disease mechanisms, help identify new targets and treatment strategies, and inform decision making for the benefit of human health.},
archivePrefix = {arXiv},
arxivId = {1910.01258},
author = {Alber, Mark and {Buganza Tepole}, Adrian and Cannon, William R. and De, Suvranu and Dura-Bernal, Salvador and Garikipati, Krishna and Karniadakis, George and Lytton, William W. and Perdikaris, Paris and Petzold, Linda and Kuhl, Ellen},
doi = {10.1038/s41746-019-0193-y},
eprint = {1910.01258},
journal = {npj Digit. Med.},
month = {dec},
number = {1},
publisher = {Springer Science and Business Media LLC},
title = {{Integrating machine learning and multiscale modeling—perspectives, challenges, and opportunities in the biological, biomedical, and behavioral sciences}},
volume = {2},
year = {2019}
}
@article{Power2003,
abstract = {Background: Young people with early psychosis are at particularly high risk of suicide. However, there is evidence that early intervention can reduce this risk. Despite these advances, first episode psychosis patients attending these new services still remain at risk. To address this concern, a program called LifeSPAN was established within the Early Psychosis Prevention and Intervention Centre (EPPIC). The program developed and evaluated a number of suicide prevention strategies within EPPIC and included a cognitively oriented therapy (LifeSPAN therapy) for acutely suicidal patients with psychosis. We describe the development of these interventions in this paper. Method: Clinical audit and surveys provided an indication of the prevalence of suicidality among first episode psychosis patients attending EPPIC. Second, staff focus groups and surveys identified gaps in service provision for suicidal young people attending the service. Third, a suicide risk monitoring system was introduced to identify those at highest risk. Finally, patients so identified were referred to and offered LifeSPAN therapy whose effectiveness was evaluated in a randomised controlled trial. Results: Fifty-six suicidal patients with first episode psychosis were randomly assigned to standard clinical care or standard care plus LifeSPAN therapy. Forty-two patients completed the intervention. Clinical ratings and measures of suicidality and risk were assessed before, immediately after the intervention, and 6 months later. Benefits were noted in the treatment group on indirect measures of suicidality, e.g., hopelessness. The treatment group showed a greater average improvement (though not significant) on a measure of suicide ideation. Conclusions: Early intervention in psychosis for young people reduces the risk of suicide. Augmenting early intervention with a suicide preventative therapy may further reduce this risk.},
author = {Power, Patrick J.R. and Bell, R. J. and Mills, R. and Herman-Doig, T. and Davern, M. and Henry, L. and Yuen, H. P. and Khademy-Deljo, A. and McGorry, P. D.},
doi = {10.1046/j.1440-1614.2003.01209.x},
issn = {00048674},
journal = {Aust. N. Z. J. Psychiatry},
keywords = {Cognitive therapy,Early intervention,First episode psychosis,Suicide,Suicide prevention},
month = {aug},
number = {4},
pages = {414--420},
pmid = {12873325},
title = {{Suicide prevention in first episode psychosis: The development of a randomised controlled trial of cognitive therapy for acutely suicidal patients with early psychosis}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12873325 http://journals.sagepub.com/doi/10.1046/j.1440-1614.2003.01209.x},
volume = {37},
year = {2003}
}
@article{Probert2019,
abstract = {The number of all possible epidemics of a given infectious disease that could occur on a given landscape is large for systems of real-world complexity. Furthermore, there is no guarantee that the control actions that are optimal, on average, over all possible epidemics are also best for each possible epidemic. Reinforcement learning (RL) and Monte Carlo control have been used to develop machine-readable context-dependent solutions for complex problems with many possible realizations ranging from video-games to the game of Go. RL could be a valuable tool to generate context-dependent policies for outbreak response, though translating the resulting policies into simple rules that can be read and interpreted by human decision-makers remains a challenge. Here we illustrate the application of RL to the development of context-dependent outbreak response policies to minimize outbreaks of foot-and-mouth disease. We show that control based on the resulting context-dependent policies, which adapt interventions to t...},
author = {Probert, W. J. M. and Lakkur, S. and Fonnesbeck, C. J. and Shea, K. and Runge, M. C. and Tildesley, M. J. and Ferrari, M. J.},
doi = {10.1098/rstb.2018.0277},
issn = {0962-8436},
journal = {Philos. Trans. R. Soc. B Biol. Sci.},
keywords = {FMD,machine learning,optimal control,outbreak response,reinforcement learning,vaccination},
month = {jul},
number = {1776},
pages = {20180277},
publisher = {The Royal Society},
title = {{Context matters: using reinforcement learning to develop human-readable, state-dependent outbreak response policies}},
url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2018.0277},
volume = {374},
year = {2019}
}
@article{Ribeiro2016a,
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
archivePrefix = {arXiv},
arxivId = {1602.04938},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
eprint = {1602.04938},
month = {feb},
title = {{Why Should I Trust You? Explaining the Predictions of Any Classifier}},
url = {http://arxiv.org/abs/1602.04938},
year = {2016}
}
@article{Bengio2012,
abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
archivePrefix = {arXiv},
arxivId = {1206.5533},
author = {Bengio, Yoshua},
eprint = {1206.5533},
month = {jun},
title = {{Practical recommendations for gradient-based training of deep architectures}},
url = {http://arxiv.org/abs/1206.5533},
year = {2012}
}
@article{Ustun2014,
abstract = {We present an integer programming framework to build accurate and interpretable discrete linear classification models. Unlike existing approaches, our framework is designed to provide practitioners with the control and flexibility they need to tailor accurate and interpretable models for a domain of choice. To this end, our framework can produce models that are fully optimized for accuracy, by minimizing the 0--1 classification loss, and that address multiple aspects of interpretability, by incorporating a range of discrete constraints and penalty functions. We use our framework to produce models that are difficult to create with existing methods, such as scoring systems and M-of-N rule tables. In addition, we propose specially designed optimization methods to improve the scalability of our framework through decomposition and data reduction. We show that discrete linear classifiers can attain the training accuracy of any other linear classifier, and provide an Occam's Razor type argument as to why the use of small discrete coefficients can provide better generalization. We demonstrate the performance and flexibility of our framework through numerical experiments and a case study in which we construct a highly tailored clinical tool for sleep apnea diagnosis.},
archivePrefix = {arXiv},
arxivId = {1405.4047},
author = {Ustun, Berk and Rudin, Cynthia},
eprint = {1405.4047},
journal = {arXiv},
month = {may},
pages = {1--57},
title = {{Methods and Models for Interpretable Linear Classification}},
url = {http://arxiv.org/abs/1405.4047},
year = {2014}
}
@article{Najgebauer2018,
abstract = {The selection of appropriate cancer models is a key prerequisite for maximising translational potential and clinical relevance of in vitro studies. An important criterion for this selection is the molecular resemblance of available models to the primary disease they represent. While studies are being increasingly conducted to comprehensively compare genomic profiles of cell lines and matched primary tumours, there is no data-driven, robust and user-friendly tool assisting scientists in such selection, by adequately estimating the molecular heterogeneity of a primary disease that is captured by existing models. We developed CELLector: a computational tool implemented in an open source R Shiny application and R package that allows researchers to select the most relevant cancer cell lines in a genomic-guided fashion. CELLector combines methods from graph theory and market basket analysis; it leverages tumour genomics data to explore, rank, and select optimal cell line models in a user-friendly way, enabling scientists to make appropriate and informed choices about model inclusion/exclusion in retrospective analyses and future studies. Additionally, it allows the selection of models within user-defined contexts, for example, by focusing on genomic alterations occurring in biological pathways of interest or considering only predetermined sub-cohorts of cancer patients. Finally, CELLector identifies combinations of molecular alterations underlying disease subtypes currently lacking representative cell lines, providing guidance for the future development of new cancer models. To demonstrate usefulness and applicability of our tool, we present example case studies, where it is used to select representative cell lines for user-defined populations of colorectal cancer patients of current clinical interest.},
author = {Najgebauer, Hanna and Yang, Mi and Francies, Hayley and Stronach, Euan A and Garnett, Mathew J and Saez-Rodriguez, Julio and Iorio, Francesco},
doi = {10.1101/275032},
journal = {bioRxiv},
keywords = {Key worlds cell lines,cancer heterogeneity,cancer models,experimental design,genomics,in vitro study,informed decision,molecular subtyping,new algorithm,oncogenic alterations,patient cohort,representative in vitro models},
month = {mar},
pages = {275032},
publisher = {Cold Spring Harbor Laboratory},
title = {{CELLector: Genomics Guided Selection of Cancer in vitro Models}},
url = {https://www.biorxiv.org/content/early/2018/03/03/275032},
year = {2018}
}
@article{Borner2018,
abstract = {Human survival depends on our ability to predict future outcomes so that we can make informed decisions. Human cognition and perception are optimized for local, short-term decision-making, such as deciding when to fight or flight, whom to mate, or what to eat. For more elaborate decisions (e.g., when to harvest, when to go to war or not, and whom to marry), people used to consult oracles—prophetic predictions of the future inspired by the gods. Over time, oracles were replaced by models of the structure and dynamics of natural, technological, and social systems. In the 21st century, computational models and visualizations of model results inform much of our decision-making: near real-time weather forecasts help us decide when to take an umbrella, plant, or harvest; where to ground airplanes; or when to evacuate inhabitants in the path of a hurricane, tornado, or flood (1). Long-term weather and climate forecasts predict a future with increasing torrential rains, stronger winds, and more frequent drought, landslides, and forest fires as well as rising sea levels, enabling decision makers to prepare for these changes by building dikes, moving cities and roads, and building larger water reservoirs and better storm sewers (2).

Computational models are particularly useful if they are combined with high-quality data and if they are widely used and understood. As early as 1960, Buckminster Fuller proposed the “World Game” to address the world's problems through a holistic and anticipatory systems approach (3, 4). The game used Fuller's Dymaxion Map to visualize resources, trends, and scenarios. It was meant to be accessible to everyone (not just experts); therefore, decisions could be made collectively, and results could be used by anyone. In the 1970s, “The limits to growth: A report to the club of Rome” (5) used simulations to forecast future states of the {\ldots} 

[↵][1]1To whom correspondence should be addressed. Email: katy{\{}at{\}}indiana.edu.

 [1]: {\#}xref-corresp-1-1},
author = {B{\"{o}}rner, Katy and Rouse, William B and Trunfio, Paul and Stanley, H Eugene},
doi = {10.1073/pnas.1818750115},
issn = {1091-6490},
journal = {Proc. Natl. Acad. Sci. U. S. A.},
month = {dec},
number = {50},
pages = {12573--12581},
pmid = {30530683},
publisher = {National Academy of Sciences},
title = {{Forecasting innovations in science, technology, and education.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/30530683 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC6294920},
volume = {115},
year = {2018}
}
@incollection{Lewis1993,
author = {Lewis, John D. and MacDonald, Bruce A.},
doi = {10.1007/978-1-4471-3207-3_14},
pages = {166--178},
publisher = {Springer, London},
title = {{Machine learning under felicity conditions: exploiting pedagogical behavior}},
url = {http://link.springer.com/10.1007/978-1-4471-3207-3{\_}14},
year = {1993}
}
@misc{Winston_603,
author = {Winston, Patrick Henry},
title = {{Syllabus | The Human Intelligence Enterprise | Electrical Engineering and Computer Science | MIT OpenCourseWare}},
url = {https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-803-the-human-intelligence-enterprise-spring-2006/syllabus/},
urldate = {2019-12-29}
}
@article{Coe2002,
author = {Coe, Robert;},
keywords = {Conference Papers,Data Interpretation,Effect Size,Information Utilisation,Statistical Significance},
month = {sep},
publisher = {Education-line},
title = {{It's the effect size, stupid: what effect size is and why it is important}},
url = {https://www.leeds.ac.uk/educol/documents/00002182.htm},
year = {2002}
}
@article{Snoek2012,
abstract = {Machine learning algorithms frequently require careful tuning of model hyperparameters, regularization terms, and optimization parameters. Unfortunately, this tuning is often a "black art" that requires expert experience, unwritten rules of thumb, or sometimes brute-force search. Much more appealing is the idea of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand. In this work, we consider the automatic tuning problem within the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). The tractable posterior distribution induced by the GP leads to efficient use of the information gathered by previous experiments, enabling optimal choices about what parameters to try next. Here we show how the effects of the Gaussian process prior and the associated inference procedure can have a large impact on the success or failure of Bayesian optimization. We show that thoughtful choices can lead to results that exceed expert-level performance in tuning machine learning algorithms. We also describe new algorithms that take into account the variable cost (duration) of learning experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization on a diverse set of contemporary algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
eprint = {1206.2944},
month = {jun},
title = {{Practical Bayesian Optimization of Machine Learning Algorithms}},
url = {http://arxiv.org/abs/1206.2944},
year = {2012}
}
@book{Hyndman2019,
author = {Hyndman, RJ and Athanasopoulos, George},
isbn = {0987507117},
title = {{Forecasting: principles and practice}},
url = {https://otexts.com/fpp2/ https://books.google.com/books?hl=en{\&}lr={\&}id={\_}bBhDwAAQBAJ{\&}oi=fnd{\&}pg=PA7{\&}ots=ThiYzfTNLJ{\&}sig=diNUKoHXboYgtC6BsZHjjrLJ2wk},
year = {2018}
}
@article{Vaccaro2019,
author = {Vaccaro, Michelle and Waldo, Jim},
doi = {10.1145/3359338},
issn = {00010782},
journal = {Commun. ACM},
month = {oct},
number = {11},
pages = {104--110},
title = {{The effects of mixing machine learning and human judgment}},
url = {http://dl.acm.org/citation.cfm?doid=3368886.3359338},
volume = {july-augus},
year = {2019}
}
@misc{Macey2001,
author = {Macey, R. I and Oster., G. F.},
publisher = {University of California at Berkeley, Berkeley, Calif.},
title = {{2001. Berkeley Madonna, version 8.0.}},
url = {www.berkeleymadonna.com/},
year = {2001}
}
@misc{Winston_howtospeak,
author = {Winston, Patrick},
title = {{How to Speak | MIT OpenCourseWare}},
url = {https://ocw.mit.edu/resources/res-tll-005-how-to-speak-january-iap-2018/{\#}},
urldate = {2019-12-23}
}
@book{Box1992,
abstract = {Wiley classics library ed. Originally published: Reading, Mass. : Addison-Wesley Pub. Co., {\textcopyright}1973. "A Wiley-Interscience publication."},
author = {Box, George E. P. and Tiao, George C.},
isbn = {9780471574286},
pages = {588},
publisher = {Wiley},
title = {{Bayesian inference in statistical analysis}},
url = {https://www.wiley.com/en-gb/Bayesian+Inference+in+Statistical+Analysis-p-9780471574286},
year = {1992}
}
@article{Bonnardel2019,
abstract = {Macrophages are strongly adapted to their tissue of residence. Yet, little is known about the cell-cell interactions that imprint the tissue-specific identities of macrophages in their respective niches. Using conditional depletion of liver Kupffer cells, we traced the developmental stages of monocytes differentiating into Kupffer cells and mapped the cellular interactions imprinting the Kupffer cell identity. Kupffer cell loss induced tumor necrosis factor (TNF)- and interleukin-1 (IL-1) receptor-dependent activation of stellate cells and endothelial cells, resulting in the transient production of chemokines and adhesion molecules orchestrating monocyte engraftment. Engrafted circulating monocytes transmigrated into the perisinusoidal space and acquired the liver-associated transcription factors inhibitor of DNA 3 (ID3) and liver X receptor-$\alpha$ (LXR-$\alpha$). Coordinated interactions with hepatocytes induced ID3 expression, whereas endothelial cells and stellate cells induced LXR-$\alpha$ via a synergistic NOTCH-BMP pathway. This study shows that the Kupffer cell niche is composed of stellate cells, hepatocytes, and endothelial cells that together imprint the liver-specific macrophage identity.},
author = {Bonnardel, Johnny and T'Jonck, Wouter and Gaublomme, Djoere and Browaeys, Robin and Scott, Charlotte L. and Martens, Liesbet and Vanneste, Bavo and {De Prijck}, Sofie and Nedospasov, Sergei A. and Kremer, Anna and {Van Hamme}, Evelien and Borghgraef, Peter and Toussaint, Wendy and {De Bleser}, Pieter and Mannaerts, Inge and Beschin, Alain and van Grunsven, Leo A. and Lambrecht, Bart N. and Taghon, Tom and Lippens, Saskia and Elewaut, Dirk and Saeys, Yvan and Guilliams, Martin},
doi = {10.1016/j.immuni.2019.08.017},
issn = {10974180},
journal = {Immunity},
keywords = {Bmp9,Id3,Kupffer cell,LXRa,Notch,Nr1h3,endothelial cell,fibroblast,liver,macrophage,monocyte,niche,stellate cell},
month = {oct},
number = {4},
pages = {638--654.e9},
publisher = {Cell Press},
title = {{Stellate Cells, Hepatocytes, and Endothelial Cells Imprint the Kupffer Cell Identity on Monocytes Colonizing the Liver Macrophage Niche}},
volume = {51},
year = {2019}
}
@article{Bender2018,
abstract = {This article introduces the pammtools package, which facilitates data transformation, estimation and interpretation of Piece-wise exponential Additive Mixed Models. A special focus is on time-varying effects and cumulative effects of time-dependent covariates, where multiple past observations of a covariate can cumulatively affect the hazard, possibly weighted by a non-linear function. The package provides functions for convenient simulation and visualization of such effects as well as a robust and versatile function to transform time-to-event data from standard formats to a format suitable for their estimation. The models can be represented as Generalized Additive Mixed Models and estimated using the R package mgcv. Many examples on real and simulated data as well as the respective R code are provided throughout the article.},
archivePrefix = {arXiv},
arxivId = {1806.01042},
author = {Bender, Andreas and Scheipl, Fabian},
eprint = {1806.01042},
month = {jun},
title = {{pammtools: Piece-wise exponential Additive Mixed Modeling tools}},
url = {http://arxiv.org/abs/1806.01042},
year = {2018}
}
@article{Juurlink2004,
abstract = {OBJECTIVES: To study the association between hospital admission for lithium toxicity and the use of diuretics, angiotensin-converting enzyme (ACE) inhibitors, and nonsteroidal antiinflammatory drugs (NSAIDs) in the elderly. DESIGN: Population-based nested case-control study. SETTING: Ontario, Canada. PARTICIPANTS: Ontario residents aged 66 and older treated with lithium. MEASUREMENTS: Estimated relative risk of hospital admission for lithium toxicity. RESULTS: From January 1992 to December 2001, 10,615 elderly patients continuously receiving lithium were identified, of whom 413 (3.9{\%}) were admitted to the hospital at least once for lithium toxicity. After adjustment for potential confounders, a dramatically increased risk of lithium toxicity was seen within a month of initiating treatment with a loop diuretic (relative risk (RR) = 5.5, 95{\%} confidence interval (CI) = 1.9-16.1) or an ACE inhibitor (RR = 7.6, 95{\%} CI = 2.6-22.0). Conversely, neither thiazide diuretics nor NSAIDs were independently associated with a significantly increased risk of hospitalization for lithium toxicity. CONCLUSION: The use of loop diuretics or ACE inhibitors significantly increases the risk of hospitalization for lithium toxicity, particularly in na{\"{i}}ve recipients.},
author = {Juurlink, David N. and Mamdani, Muhammad M. and Kopp, Alexander and Rochon, Paula A. and Shulman, Kenneth I. and Redelmeier, Donald A.},
doi = {10.1111/j.1532-5415.2004.52221.x},
issn = {00028614},
journal = {J. Am. Geriatr. Soc.},
keywords = {ACE inhibitors,Aged,Diuretics,Drug interactions,Lithium,Nested case-control studies,Nonsteroidal anti-inflammatory agents,Pharmacoepidemiology,Toxicity},
month = {may},
number = {5},
pages = {794--798},
publisher = {John Wiley {\&} Sons, Ltd (10.1111)},
title = {{Drug-Induced Lithium Toxicity in the Elderly: A Population-Based Study}},
url = {http://doi.wiley.com/10.1111/j.1532-5415.2004.52221.x},
volume = {52},
year = {2004}
}
@article{DeKleer1984,
abstract = {A qualitative physics predicts and explains the behavior of mechanisms in qualitative terms. The goals for the qualitative physics are (1) to be far simpler than the classical physics and yet retain all the important distinctions (e.g., state, oscillation, gain, momentum) without invoking the mathematics of continuously varying quantities and differential equations, (2) to produce causal accounts of physical mechanisms that are easy to understand, and (3) to provide the foundations for commonsense models for the next generation of expert systems. This paper presents a fairly encompassing account of qualitative physics. First, we discuss the general subject of naive physics and some of its methodological considerations. Second, we present a framework for modeling the generic behavior of individual components of a device based on the notions of qualitative differential equations (confluences) and qualitative state. This requires developing a qualitative version of the calculus. The modeling primitives induce two kinds of behavior, intrastate and interstate, which are governed by different laws. Third, we present algorithms for determining the behavior of a composite device from the generic behavior of its components. Fourth, we examine a theory of explanation for these predictions based on logical proof. Fifth, we introduce causality as an ontological commitment for explaining how devices behave.},
author = {{De Kleer}, Johan and Brown, John Seely},
doi = {10.1016/0004-3702(84)90037-7},
issn = {0004-3702},
journal = {Artif. Intell.},
month = {dec},
number = {1-3},
pages = {7--83},
publisher = {Elsevier},
title = {{A qualitative physics based on confluences}},
url = {https://www.sciencedirect.com/science/article/abs/pii/0004370284900377},
volume = {24},
year = {1984}
}
@inproceedings{Kanter2015,
abstract = {In this paper, we develop the Data Science Machine, which is able to derive predictive models from raw data automatically. To achieve this automation, we first propose and develop the Deep Feature Synthesis algorithm for automatically generating features for relational datasets. The algorithm follows relationships in the data to a base field, and then sequentially applies mathematical functions along that path to create the final feature. Second, we implement a generalizable machine learning pipeline and tune it using a novel Gaussian Copula process based approach. We entered the Data Science Machine in 3 data science competitions that featured 906 other data science teams. Our approach beats 615 teams in these data science competitions. In 2 of the 3 competitions we beat a majority of competitors, and in the third, we achieved 94{\%} of the best competitor's score. In the best case, with an ongoing competition, we beat 85.6{\%} of the teams and achieved 95.7{\%} of the top submissions score.},
author = {Kanter, James Max and Veeramachaneni, Kalyan},
booktitle = {Proc. 2015 IEEE Int. Conf. Data Sci. Adv. Anal. DSAA 2015},
doi = {10.1109/DSAA.2015.7344858},
isbn = {9781467382731},
month = {oct},
pages = {1--10},
publisher = {IEEE},
title = {{Deep feature synthesis: Towards automating data science endeavors}},
url = {http://ieeexplore.ieee.org/document/7344858/},
year = {2015}
}
@article{Pratanwanich2014,
abstract = {Drug treatments often perturb the activities of certain pathways, sets of functionally related genes. Examining pathways/gene sets that are responsive to drug treatments instead of a simple list of regulated genes can advance our understanding about such cellular processes after perturbations. In general, pathways do not work in isolation and their connections can cause secondary effects. To address this, we present a new method to better identify pathway responsiveness to drug treatments and simultaneously to determine between-pathway interactions. Firstly, we developed a Bayesian matrix factorisation of gene expression data together with known gene–pathway memberships to identify pathways perturbed by drugs. Secondly, in order to determine the interactions between pathways, we implemented a Gaussian Markov Random Field (GMRF) under the matrix factorization framework. Assuming a Gaussian distribution of pathway responsiveness, we calculated the correlations between pathways. We applied the combination of the Bayesian factor model and the GMRF to analyse gene expression data of 1169 drugs with 236 known pathways, 66 of which were disease-related pathways. Our model yielded a significantly higher average precision than the existing methods for identifying pathway responsiveness to drugs that affected multiple pathways. This implies the advantage of the between-pathway interactions and confirms our assumption that pathways are not independent, an aspect that has been commonly overlooked in the existing methods. Additionally, we demonstrate four case studies illustrating that the between-pathway network enhances the performance of pathway identification and provides insights into disease comorbidity, drug repositioning, and tissue-specific comparative analysis of drug treatments.},
author = {Pratanwanich, Naruemon and Li{\'{o}}, Pietro},
doi = {10.1039/c4mb00014e},
issn = {17422051},
journal = {Mol. Biosyst.},
month = {jun},
number = {6},
pages = {1538--1548},
pmid = {24695945},
title = {{Pathway-based Bayesian inference of drug-disease interactions}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24695945},
volume = {10},
year = {2014}
}
@article{Gelman2006,
abstract = {Various noninformative prior distributions have been suggested for scale parameters in hierarchical models. We construct a new folded-noncentral-t family of conditionally conjugate priors for hierarchical standard deviation parameters, and then consider noninformative and weakly informative priors in this family. We use an example to illustrate serious problems with the inverse-gamma family of "noninformative" prior distributions. We suggest instead to use a uniform prior on the hierarchical standard deviation, using the half-t family when the number of groups is small and in other settings where a weakly informative prior is desired. We also illustrate the use of the half-t family for hierarchical modeling of multiple variance parameters such as arise in the analysis of variance.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Gelman, Andrew},
doi = {10.1214/06-BA117A},
eprint = {arXiv:1011.1669v3},
isbn = {9781139460934},
issn = {19360975},
journal = {Bayesian Anal.},
keywords = {Bayesian inference,Conditional conjugacy,Folded-noncentral-t distribution,Half-t distribution,Hierarchical model,Multilevel model,Noninformative prior distribution,Weakly informative prior distribution},
month = {sep},
number = {3},
pages = {515--534},
pmid = {5026},
publisher = {International Society for Bayesian Analysis},
title = {{Prior distributions for variance parameters in hierarchical models}},
url = {http://projecteuclid.org/euclid.ba/1340371048},
volume = {1},
year = {2006}
}
@article{Vold2019,
author = {Vold, Karina and Schlimm, Dirk},
doi = {10.1007/s11229-019-02097-w},
issn = {15730964},
journal = {Synthese},
keywords = {Cognitive integration,Enculturation,Mathematical cognition,Mental content,Representational vehicles,Vehicle externalism},
month = {jan},
pages = {1--21},
publisher = {Springer Netherlands},
title = {{Extended mathematical cognition: external representations with non-derived content}},
url = {http://link.springer.com/10.1007/s11229-019-02097-w},
year = {2019}
}
@article{Spelke2007,
abstract = {Human cognition is founded, in part, on four systems for representing objects, actions, number, and space. It may be based, as well, on a fifth system for representing social partners. Each system has deep roots in human phylogeny and ontogeny, and it guides and shapes the mental lives of adults. Converging research on human infants, non-human primates, children and adults in diverse cultures can aid both understanding of these systems and attempts to overcome their limits.},
author = {Spelke, Elizabeth S. and Kinzler, Katherine D.},
doi = {10.1111/j.1467-7687.2007.00569.x},
issn = {1363755X},
journal = {Dev. Sci.},
month = {jan},
number = {1},
pages = {89--96},
pmid = {17181705},
title = {{Core knowledge}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17181705 http://doi.wiley.com/10.1111/j.1467-7687.2007.00569.x},
volume = {10},
year = {2007}
}
@article{Hajek2011,
abstract = {Presents a case report of a patient who had her first episode of depression at age 18 when leaving home to attend university. Following the birth of her third child, she experienced another depression and her psychiatrist prescribed lithium (Li), and her condition improved with in 2 weeks. She continued Li for the next 21 years, with her levels always within the therapeutic range. At age 56, one of AB's routine laboratory tests revealed increased serum creatinine. Her family physician asked her to gradually discontinue Li. The decision to discontinue Li needs to carefully balance the benefit of Li treatment against the severity of side effects and the probability that they are caused by Li. Assessing the Li response in a particular patient requires gauging the symptomatic and prophylactic benefits achieved while on Li against the possibility that Li did not contribute to these effects. Especially in an excellent Li responder, it becomes critical to determine the cost of continuing the Li prophylaxis. The mildly elevated serum creatinine may or may not have been related to the Li treatment. It is unclear whether a switch to a medication with proven mood stabilizing properties, rather than to topiramate, would have yielded a different outcome for the patient. Li should never be discontinued, however, the decision requires a thorough clinical assessment. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
author = {Hajek, Tomas},
doi = {10.1503/jpn.110117},
issn = {11804882},
journal = {J. Psychiatry Neurosci.},
month = {nov},
number = {6},
pages = {E39--E40},
pmid = {22011562},
title = {{Discontinuation of lithium because of side effects}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22011562 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3201996 http://jpn.ca/vol36-issue6/36-6-e39/},
volume = {36},
year = {2011}
}
@article{Miller2019,
abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good' explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
archivePrefix = {arXiv},
arxivId = {1706.07269},
author = {Miller, Tim},
doi = {10.1016/j.artint.2018.07.007},
eprint = {1706.07269},
isbn = {0897910087},
issn = {00043702},
journal = {Artif. Intell.},
keywords = {Explainability,Explainable AI,Explanation,Interpretability,Transparency},
month = {jun},
pages = {1--38},
pmid = {9223372036854775808},
title = {{Explanation in artificial intelligence: Insights from the social sciences}},
url = {http://arxiv.org/abs/1706.07269},
volume = {267},
year = {2019}
}
@incollection{Muller2012,
author = {M{\"{u}}ller, Klaus-Robert},
doi = {10.1007/978-3-642-35289-8_1},
pages = {1--5},
title = {{Neural Networks: Tricks of the Trade}},
url = {http://link.springer.com/10.1007/978-3-642-35289-8{\_}1},
year = {2012}
}
@misc{kubernetes_website,
title = {{Borg, Omega, and Kubernetes - ACM Queue}},
url = {https://queue.acm.org/detail.cfm?id=2898444},
urldate = {2019-01-31}
}
@inproceedings{Potapenko2017,
abstract = {We consider probabilistic topic models and more recent word embedding techniques from a perspective of learning hidden semantic representations. Inspired by a striking similarity of the two approaches, we merge them and learn probabilistic embeddings with online EM-algorithm on word co-occurrence data. The resulting embeddings perform on par with Skip-Gram Negative Sampling (SGNS) on word similarity tasks and benefit in the interpretability of the components. Next, we learn probabilistic document embeddings that outperform paragraph2vec on a document similarity task and require less memory and time for training. Finally, we employ multimodal Additive Regularization of Topic Models (ARTM) to obtain a high sparsity and learn embeddings for other modalities, such as timestamps and categories. We observe further improvement of word similarity performance and meaningful inter-modality similarities.},
archivePrefix = {arXiv},
arxivId = {1711.04154},
author = {Potapenko, Anna and Popov, Artem and Vorontsov, Konstantin},
booktitle = {Commun. Comput. Inf. Sci.},
doi = {10.1007/978-3-319-71746-3_15},
eprint = {1711.04154},
isbn = {9783319717456},
issn = {18650929},
month = {nov},
pages = {167--180},
title = {{Interpretable probabilistic embeddings: Bridging the gap between topic models and neural networks}},
url = {http://arxiv.org/abs/1711.04154},
volume = {789},
year = {2018}
}
@article{Hara2016,
abstract = {Tree ensembles, such as random forest and boosted trees, are renowned for their high prediction performance, whereas their interpretability is critically limited. In this paper, we propose a post processing method that improves the model interpretability of tree ensembles. After learning a complex tree ensembles in a standard way, we approximate it by a simpler model that is interpretable for human. To obtain the simpler model, we derive the EM algorithm minimizing the KL divergence from the complex ensemble. A synthetic experiment showed that a complicated tree ensemble was approximated reasonably as interpretable.},
archivePrefix = {arXiv},
arxivId = {1606.05390},
author = {Hara, Satoshi and Hayashi, Kohei},
eprint = {1606.05390},
journal = {2016 ICML Work. Hum. Interpret. Mach. Learn.},
month = {jun},
number = {Whi},
title = {{Making Tree Ensembles Interpretable}},
url = {http://arxiv.org/abs/1606.05390},
year = {2016}
}
@article{Li2017,
abstract = {Deep neural networks are widely used for classification. These deep models often suffer from a lack of interpretability -- they are particularly difficult to understand because of their non-linear nature. As a result, neural networks are often treated as "black box" models, and in the past, have been trained purely to optimize the accuracy of predictions. In this work, we create a novel network architecture for deep learning that naturally explains its own reasoning for each prediction. This architecture contains an autoencoder and a special prototype layer, where each unit of that layer stores a weight vector that resembles an encoded training input. The encoder of the autoencoder allows us to do comparisons within the latent space, while the decoder allows us to visualize the learned prototypes. The training objective has four terms: an accuracy term, a term that encourages every prototype to be similar to at least one encoded input, a term that encourages every encoded input to be close to at least one prototype, and a term that encourages faithful reconstruction by the autoencoder. The distances computed in the prototype layer are used as part of the classification process. Since the prototypes are learned during training, the learned network naturally comes with explanations for each prediction, and the explanations are loyal to what the network actually computes.},
archivePrefix = {arXiv},
arxivId = {1710.04806},
author = {Li, Oscar and Liu, Hao and Chen, Chaofan and Rudin, Cynthia},
doi = {927410856 [pii]\r10.1080/09297041003783310},
eprint = {1710.04806},
isbn = {1744-4136 (Electronic)$\backslash$r0929-7049 (Linking)},
month = {oct},
pmid = {20924853},
title = {{Deep Learning for Case-Based Reasoning through Prototypes: A Neural Network that Explains Its Predictions}},
url = {http://arxiv.org/abs/1710.04806},
year = {2017}
}
@article{Winston2012,
abstract = {Gesehen am 17.05.2017.},
author = {Winston, Patrick Henry},
journal = {Adv. Cogn. Syst.},
pages = {23--36},
publisher = {Cognitive Systems Foundation},
title = {{The right way}},
url = {http://dspace.mit.edu/handle/1721.1/72174},
volume = {1},
year = {2012}
}
@article{Kurvers2015,
abstract = {Copyright 2015 American Medical Association. All rights reserved. IMPORTANCE Incidence rates of skin cancer are increasing globally, and the correct classification of skin lesions (SLs) into benign and malignant tissue remains a continuous challenge. A collective intelligence approach to skin cancer detection may improve accuracy. OBJECTIVE To evaluate the performance of 2 well-known collective intelligence rules (majority rule and quorum rule) that combine the independent conclusions of multiple decision makers into a single decision. DESIGN, SETTING, AND PARTICIPANTS Evaluationswere obtained from 2 large and independent data sets. The first data set consisted of 40 experienced dermoscopists, each of whom independently evaluated 108 images of SLs during the Consensus Net Meeting of 2000. The second data set consisted of 82 medical professionals with varying degrees of dermatology experience, each of whom evaluated a minimum of 110 SLs. All SLs were evaluated via the Internet. Image selection of SLs was based on high image quality and the presence of histopathologic information. Data were collected from July through October 2000 for study 1 and from February 2003 through January 2004 for study 2 and evaluated from January 5 through August 7, 2015. MAIN OUTCOMES AND MEASURES For both collective intelligence rules,we determined the true-positive rate (ie, the hit rate or specificity) and the false-positive rate (ie, the false-alarm rate or 1 - sensitivity) and compared these rates with the performance of single decision makers. Furthermore, we evaluated the effect of group size on true- and false-positive rates. RESULTS One hundred twenty-two medical professionals performed 16 029 evaluations. Use of either collective intelligence rule consistently outperformed single decision makers. The groups achieved an increased true-positive rate and a decreased false-positive rate. For example, individual decision makers in study 1, using the pattern analysis as diagnostic algorithm, achieved a true-positive rate of 0.83 and a false-positive rate of 0.17. Groups of 3 individuals achieved a true-positive rate of 0.91 and a false-positive rate of 0.14. These improvements increased with increasing group size. CONCLUSIONS AND RELEVANCE Collective intelligence might be a viable approach to increase diagnostic accuracy in skin cancer and reduce skin cancer-related mortality.},
author = {Kurvers, Ralf H.J.M. and Krause, Jens and Argenziano, Giuseppe and Zalaudek, Iris and Wolf, Max},
doi = {10.1001/jamadermatol.2015.3149},
issn = {21686068},
journal = {JAMA Dermatology},
keywords = {algorithm,benign,benign skin neoplasms,cancer diagnosis,classification,decision support systems,dermal neoplasm,dermoscopy,diagnostic errors,diagnostic imaging,diagnostic techniques and procedures,false-positive results,histopathology tests,internet,interobserver variation,measurement error,roc curve,shared decision making,skin cancer,skin lesion,skin neoplasms,true-positive result},
month = {dec},
number = {12},
pages = {1346--1353},
publisher = {American Medical Association},
title = {{Detection accuracy of collective intelligence assessments for skin cancer diagnosis}},
url = {http://archderm.jamanetwork.com/article.aspx?doi=10.1001/jamadermatol.2015.3149},
volume = {151},
year = {2015}
}
@article{Lao2020,
author = {Lao, Bryan and Tamei, Tomoya and Ikeda, Kazushi},
doi = {10.3389/fcomp.2020.00003},
issn = {2624-9898},
journal = {Front. Comput. Sci.},
month = {feb},
title = {{Data-Efficient Framework for Personalized Physiotherapy Feedback}},
url = {https://www.frontiersin.org/article/10.3389/fcomp.2020.00003/full},
volume = {2},
year = {2020}
}
@article{Howes2014,
abstract = {Schizophrenia remains a major burden on patients and society. The dopamine hypothesis attempts to explain the pathogenic mechanisms of the disorder, and the neurodevelopmental hypothesis the origins. In the past 10 years an alternative, the cognitive model, has gained popularity. However, the first two theories have not been satisfactorily integrated, and the most influential iteration of the cognitive model makes no mention of dopamine, neurodevelopment, or indeed the brain. In this Review we show that developmental alterations secondary to variant genes, early hazards to the brain, and childhood adversity sensitise the dopamine system, and result in excessive presynaptic dopamine synthesis and release. Social adversity biases the cognitive schema that the individual uses to interpret experiences towards paranoid interpretations. Subsequent stress results in dysregulated dopamine release, causing the misattribution of salience to stimuli, which are then misinterpreted by the biased cognitive processes. The resulting paranoia and hallucinations in turn cause further stress, and eventually repeated dopamine dysregulation hardwires the psychotic beliefs. Finally, we consider the implications of this model for understanding and treatment of schizophrenia.},
author = {Howes, Oliver D and Murray, Robin M},
doi = {10.1016/S0140-6736(13)62036-X},
isbn = {0140-6736},
issn = {01406736},
journal = {Lancet},
month = {may},
number = {9929},
pages = {1677--1687},
pmid = {24315522},
publisher = {Europe PMC Funders},
title = {{Schizophrenia: an integrated sociodevelopmental-cognitive model}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24315522 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4127444 https://linkinghub.elsevier.com/retrieve/pii/S014067361362036X},
volume = {383},
year = {2014}
}
@misc{LeCun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
booktitle = {Nature},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
isbn = {9780521835688},
issn = {14764687},
keywords = {Computer science,Mathematics and computing},
month = {may},
number = {7553},
pages = {436--444},
pmid = {10463930},
publisher = {Nature Publishing Group},
title = {{Deep learning}},
url = {http://www.nature.com/articles/nature14539},
volume = {521},
year = {2015}
}
@article{Tran2018a,
abstract = {We describe a simple, low-level approach for embedding probabilistic programming in a deep learning ecosystem. In particular, we distill probabilistic programming down to a single abstraction---the random variable. Our lightweight implementation in TensorFlow enables numerous applications: a model-parallel variational auto-encoder (VAE) with 2nd-generation tensor processing units (TPUv2s); a data-parallel autoregressive model (Image Transformer) with TPUv2s; and multi-GPU No-U-Turn Sampler (NUTS). For both a state-of-the-art VAE on 64x64 ImageNet and Image Transformer on 256x256 CelebA-HQ, our approach achieves an optimal linear speedup from 1 to 256 TPUv2 chips. With NUTS, we see a 100x speedup on GPUs over Stan and 37x over PyMC3.},
archivePrefix = {arXiv},
arxivId = {1811.02091},
author = {Tran, Dustin and Hoffman, Matthew and Moore, Dave and Suter, Christopher and Vasudevan, Srinivas and Radul, Alexey and Johnson, Matthew and Saurous, Rif A.},
doi = {arXiv:1811.02091v1},
eprint = {1811.02091},
isbn = {9488119250386},
month = {nov},
title = {{Simple, Distributed, and Accelerated Probabilistic Programming}},
url = {http://arxiv.org/abs/1811.02091},
year = {2018}
}
@article{Ioannou2016,
abstract = {This paper investigates the connections between two state of the art classifiers: decision forests (DFs, including decision jungles) and convolutional neural networks (CNNs). Decision forests are computationally efficient thanks to their conditional computation property (computation is confined to only a small region of the tree, the nodes along a single branch). CNNs achieve state of the art accuracy, thanks to their representation learning capabilities. We present a systematic analysis of how to fuse conditional computation with representation learning and achieve a continuum of hybrid models with different ratios of accuracy vs. efficiency. We call this new family of hybrid models conditional networks. Conditional networks can be thought of as: i) decision trees augmented with data transformation operators, or ii) CNNs, with block-diagonal sparse weight matrices, and explicit data routing functions. Experimental validation is performed on the common task of image classification on both the CIFAR and Imagenet datasets. Compared to state of the art CNNs, our hybrid models yield the same accuracy with a fraction of the compute cost and much smaller number of parameters.},
archivePrefix = {arXiv},
arxivId = {1603.01250},
author = {Ioannou, Yani and Robertson, Duncan and Zikic, Darko and Kontschieder, Peter and Shotton, Jamie and Brown, Matthew and Criminisi, Antonio},
eprint = {1603.01250},
month = {mar},
title = {{Decision Forests, Convolutional Networks and the Models in-Between}},
url = {http://arxiv.org/abs/1603.01250},
year = {2016}
}
@article{McClelland1995,
abstract = {Damage to the hippocampal system disrupts recent memory but leaves remote memory intact. The account presented here suggests that memories are first stored via synaptic changes in the hippocampal system, that these changes support reinstatement of recent memories in the neocortex, that neocortical synapses change a little on each reinstatement, and that remote memory is based on accumulated neocortical changes. Models that learn via changes to connections help explain this organization. These models discover the structure in ensembles of items if learning of each item is gradual and interleaved with learning about other items. This suggests that the neocortex learns slowly to discover the structure in ensembles of experiences. The hippocampal system permits rapid learning of new items without disrupting this structure, and reinstatement of new memories interleaves them with others to integrate them into structured neocortical memory systems.},
author = {McClelland, James L. and McNaughton, Bruce L. and O'Reilly, Randall C.},
doi = {10.1037/0033-295X.102.3.419},
issn = {0033-295X},
journal = {Psychol. Rev.},
month = {jul},
number = {3},
pages = {419--457},
pmid = {7624455},
title = {{Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/7624455 http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.102.3.419},
volume = {102},
year = {1995}
}
@article{Zaremba2014,
abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
archivePrefix = {arXiv},
arxivId = {1409.2329},
author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
eprint = {1409.2329},
month = {sep},
title = {{Recurrent Neural Network Regularization}},
url = {http://arxiv.org/abs/1409.2329},
year = {2014}
}
@inproceedings{Frankle2018,
abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90{\%}, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the lottery ticket hypothesis: dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that-when trained in isolation-reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20{\%} of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
archivePrefix = {arXiv},
arxivId = {1803.03635},
author = {Frankle, Jonathan and Carbin, Michael},
booktitle = {7th Int. Conf. Learn. Represent. ICLR 2019},
eprint = {1803.03635},
month = {mar},
title = {{The lottery ticket hypothesis: Finding sparse, trainable neural networks}},
url = {http://arxiv.org/abs/1803.03635},
year = {2019}
}
@article{Liu2019,
abstract = {It is now known that astrocytes modulate the activity at the tripartite synapses where indirect signaling via the retrograde messengers, endocannabinoids, leads to a localized self-repairing capability. In this paper, a self-repairing spiking astrocyte neural network (SANN) is proposed to demonstrate a distributed self-repairing capability at the network level. The SANN uses a novel learning rule that combines the spike- timing-dependent plasticity (STDP) and Bienenstock, Cooper, and Munro (BCM) learning rules (hereafter referred to as the BSTDP rule). In this learning rule, the synaptic weight potentiation is not only driven by the temporal difference between the presynaptic and postsynaptic neuron firing times but also by the postsynaptic neuron activity. We will show in this paper that the BSTDP modulates the height of the plasticity window to establish an input–output mapping (in the learning phase) and also maintains this mapping (via self-repair) if synaptic pathways become dysfunctional. It is the functional dependence of postsynaptic neuron firing activity on the height of the plasticity window that underpins how the proposed SANN self- repairs on the fly. The SANN also uses the coupling between the tripartite synapses and $\gamma$-GABAergic interneurons. This interaction gives rise to a presynaptic neuron frequency filtering capability that serves to route information, represented as spike trains, to different neurons in the subsequent layers of the SANN. The proposed SANN follows a feedforward architecture with multiple interneuron pathways and astrocytes modulate synaptic activity at the hidden and output neuronal layers. The self- repairing capability will be demonstrated in a robotic obstacle avoidance application, and the simulation results will show that the SANN can maintain learned maneuvers at synaptic fault densities of up to 80{\%} regardless of the fault locations.},
author = {Liu, Junxiu and McDaid, Liam J. and Harkin, Jim and Karim, Shvan and Johnson, Anju P. and Millard, Alan G. and Hilder, James and Halliday, David M. and Tyrrell, Andy M. and Timmis, Jon},
doi = {10.1109/TNNLS.2018.2854291},
issn = {21622388},
journal = {IEEE Trans. Neural Networks Learn. Syst.},
keywords = {Astrocyte,fault tolerance,obstacle avoidance,self-repair,spiking neural networks},
month = {mar},
number = {3},
pages = {865--875},
pmid = {30072349},
title = {{Exploring Self-Repair in a Coupled Spiking Astrocyte Neural Network}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/30072349 https://ieeexplore.ieee.org/document/8423789/},
volume = {30},
year = {2019}
}
@article{Lee2018a,
abstract = {Syndromic surveillance detects and monitors individual and population health indicators through sources such as emergency department records. Automated classification of these records can improve outbreak detection speed and diagnosis accuracy. Current syndromic systems rely on hand-coded keyword-based methods to parse written fields and may benefit from the use of modern supervised-learning classifier models. In this paper we implement two recurrent neural network models based on long short-term memory (LSTM) and gated recurrent unit (GRU) cells and compare them to two traditional bag-of-words classifiers: multinomial naive Bayes (MNB) and a support vector machine (SVM). The MNB classifier is one of only two machine learning algorithms currently being used for syndromic surveillance. All four models are trained to predict diagnostic code groups as defined by Clinical Classification Software, first to predict from discharge diagnosis, then from chief complaint fields. The classifiers are trained on 3.6 million de-identified emergency department records from a single United States jurisdiction. We compare performance of these models primarily using the F1 score. Using discharge diagnoses, the LSTM classifier performs best, though all models exhibit an F1 score above 96.00. The GRU performs best on chief complaints (F1=47.38), and MNB with bigrams performs worst (F1=39.40). Certain syndrome types are easier to detect than others. For examples, chief complaints using the GRU model predicts alcohol-related disorders well (F1=78.91) but predicts influenza poorly (F1=14.80). In all instances, the RNN models outperformed the bag-of-word classifiers, suggesting deep learning models could substantially improve the automatic classification of unstructured text for syndromic surveillance.},
archivePrefix = {arXiv},
arxivId = {1805.07574},
author = {Lee, Scott H and Levin, Drew and Finley, Pat and Heilig, Charles M},
eprint = {1805.07574},
month = {may},
title = {{Chief complaint classification with recurrent neural networks}},
url = {http://arxiv.org/abs/1805.07574},
year = {2018}
}
@article{Satyanarayan2018,
abstract = {Interpretability techniques are normally studied in isolation. We explore the powerful interfaces that arise when you combine them -- and the rich structure of this combinatorial space.},
author = {Satyanarayan, Arvind and Olah, Chris and Mordvintsev, Alexander and Ye, Katherine and Johnson, Ian and Carter, Shan and Schubert, Ludwig},
doi = {10.23915/distill.00010},
issn = {2476-0757},
journal = {Distill},
month = {mar},
number = {3},
pages = {e10},
title = {{The Building Blocks of Interpretability}},
url = {https://distill.pub/2018/building-blocks},
volume = {3},
year = {2018}
}
@inproceedings{Zhu2019,
abstract = {Learning continuous representations of nodes is attracting growing interest in both academia and industry recently, due to their simplicity and effectiveness in a variety of applications. Most of existing node embedding algorithms and systems are capable of processing networks with hundreds of thousands or a few millions of nodes. However, how to scale them to networks that have tens of millions or even hundreds of millions of nodes remains a challenging problem. In this paper, we propose GraphVite, a high-performance CPU-GPU hybrid system for training node embeddings, by co-optimizing the algorithm and the system. On the CPU end, augmented edge samples are parallelly generated by random walks in an online fashion on the network, and serve as the training data. On the GPU end, a novel parallel negative sampling is proposed to leverage multiple GPUs to train node embeddings simultaneously, without much data transfer and synchronization. Moreover, an efficient collaboration strategy is proposed to further reduce the synchronization cost between CPUs and GPUs. Experiments on multiple real-world networks show that GraphVite is super efficient. It takes only about one minute for a network with 1 million nodes and 5 million edges on a single machine with 4 GPUs, and takes around 20 hours for a network with 66 million nodes and 1.8 billion edges. Compared to the current fastest system, GraphVite is about 50 times faster without any sacrifice on performance.},
archivePrefix = {arXiv},
arxivId = {1903.00757},
author = {Zhu, Zhaocheng and Xu, Shizhen and Tang, Jian and Qu, Meng},
doi = {10.1145/3308558.3313508},
eprint = {1903.00757},
month = {mar},
pages = {2494--2504},
title = {{GraphVite: A High-Performance CPU-GPU Hybrid System for Node Embedding}},
url = {http://arxiv.org/abs/1903.00757 http://dx.doi.org/10.1145/3308558.3313508},
year = {2019}
}
@article{Choi2016b,
abstract = {Learning efficient representations for concepts has been proven to be an important basis for many applications such as machine translation or document classification. Proper representations of medical concepts such as diagnosis, medication, procedure codes and visits will have broad applications in healthcare analytics. However, in Electronic Health Records (EHR) the visit sequences of patients include multiple concepts (diagnosis, procedure, and medication codes) per visit. This structure provides two types of relational information, namely sequential order of visits and co-occurrence of the codes within each visit. In this work, we propose Med2Vec, which not only learns distributed representations for both medical codes and visits from a large EHR dataset with over 3 million visits, but also allows us to interpret the learned representations confirmed positively by clinical experts. In the experiments, Med2Vec displays significant improvement in key medical applications compared to popular baselines such as Skip-gram, GloVe and stacked autoencoder, while providing clinically meaningful interpretation.},
archivePrefix = {arXiv},
arxivId = {1602.05568},
author = {Choi, Edward and Bahadori, Mohammad Taha and Searles, Elizabeth and Coffey, Catherine and Sun, Jimeng},
doi = {10.1248/cpb.58.1555},
eprint = {1602.05568},
isbn = {9781450342322},
issn = {0009-2363},
month = {feb},
pmid = {21139254},
title = {{Multi-layer Representation Learning for Medical Concepts}},
url = {http://arxiv.org/abs/1602.05568},
year = {2016}
}
@article{Yang2016b,
abstract = {We present an algorithm for building probabilistic rule lists that is two orders of magnitude faster than previous work. Rule list algorithms are competitors for decision tree algorithms. They are associative classifiers, in that they are built from pre-mined association rules. They have a logical structure that is a sequence of IF-THEN rules, identical to a decision list or one-sided decision tree. Instead of using greedy splitting and pruning like decision tree algorithms, we fully optimize over rule lists, striking a practical balance between accuracy, interpretability, and computational speed. The algorithm presented here uses a mixture of theoretical bounds (tight enough to have practical implications as a screening or bounding procedure), computational reuse, and highly tuned language libraries to achieve computational efficiency. Currently, for many practical problems, this method achieves better accuracy and sparsity than decision trees; further, in many cases, the computational time is practical and often less than that of decision trees. The result is a probabilistic classifier (which estimates P(y = 1|x) for each x) that optimizes the posterior of a Bayesian hierarchical model over rule lists.},
archivePrefix = {arXiv},
arxivId = {1602.08610},
author = {Yang, Hongyu and Rudin, Cynthia and Seltzer, Margo},
eprint = {1602.08610},
month = {feb},
title = {{Scalable Bayesian Rule Lists}},
url = {http://arxiv.org/abs/1602.08610},
year = {2016}
}
@article{Ruder2016,
abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
archivePrefix = {arXiv},
arxivId = {1609.04747},
author = {Ruder, Sebastian},
eprint = {1609.04747},
month = {sep},
title = {{An overview of gradient descent optimization algorithms}},
url = {http://arxiv.org/abs/1609.04747},
year = {2016}
}
@article{Mehta2019,
abstract = {Machine Learning (ML) is one of the most exciting and dynamic areas of modern research and application. The purpose of this review is to provide an introduction to the core concepts and tools of machine learning in a manner easily understood and intuitive to physicists. The review begins by covering fundamental concepts in ML and modern statistics such as the bias-variance tradeoff, overfitting, regularization, generalization, and gradient descent before moving on to more advanced topics in both supervised and unsupervised learning. Topics covered in the review include ensemble models, deep learning and neural networks, clustering and data visualization, energy-based models (including MaxEnt models and Restricted Boltzmann Machines), and variational methods. Throughout, we emphasize the many natural connections between ML and statistical physics. A notable aspect of the review is the use of Python Jupyter notebooks to introduce modern ML/statistical packages to readers using physics-inspired datasets (the Ising Model and Monte-Carlo simulations of supersymmetric decays of proton-proton collisions). We conclude with an extended outlook discussing possible uses of machine learning for furthering our understanding of the physical world as well as open problems in ML where physicists may be able to contribute. (Notebooks are available at https://physics.bu.edu/{\~{}}pankajm/MLnotebooks.html )},
archivePrefix = {arXiv},
arxivId = {1803.08823},
author = {Mehta, Pankaj and Bukov, Marin and Wang, Ching-Hao and Day, Alexandre G. R. and Richardson, Clint and Fisher, Charles K. and Schwab, David J.},
doi = {10.1016/B978-0-08-044132-0.50011-3},
eprint = {1803.08823},
isbn = {0080441327},
issn = {0021-9606},
journal = {Phys. Rep.},
month = {mar},
publisher = {North-Holland},
title = {{A high-bias, low-variance introduction to Machine Learning for physicists}},
url = {https://www.sciencedirect.com/science/article/pii/S0370157319300766 http://arxiv.org/abs/1803.08823},
year = {2018}
}
@article{Abou-Saleh1989,
abstract = {The relationships between lithium dosage, affective morbidity, side-effects, thyroid and renal function and biological markers for depression were examined in the context of a prospective double-blind lithium reduction study in patients receiving prophylactic lithium. Unipolar and bipolar patients on such treatment were randomly allocated to two groups over a period of one year, either continuing with their usual dosage of lithium or reducing their lithium dosage by up to 50{\%}. Biological markers investigated included dexamethasone suppression test (DST) and 5-hydroxytryptamine (5-HT) transport into platelets (Vmax). Results showed no association between affective morbidity and lithium dosage/level. There was, however, an association between lower dosage/level of lithium and lower side-effects, including tremor and weight gain, lower TSH levels and lower 24 h urinary volume in these patients. Elderly patients, however, experienced significantly greater morbidity upon reduction of their lithium dosage. There was an association between increased Vmax of 5-HT transport and reduction in morbidity. DST non-suppression was associated with lower mean weight for the whole year of the study. {\textcopyright} 1989.},
author = {Abou-Saleh, M T and Coppen, A},
doi = {10.1016/0022-3956(89)90006-X},
issn = {00223956},
journal = {J. Psychiatr. Res.},
number = {2},
pages = {157--162},
pmid = {2511299},
title = {{The efficacy of low-dose lithium: Clinical, psychological and biological correlates}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/2511299},
volume = {23},
year = {1989}
}
@article{Bobrow1984,
abstract = {This volume brings together current work on qualitative reasoning. Previous publication has been primarily in scattered conference proceedings. The appearance of this volume reflects the maturity of qualitative reasoning as a research area, and the growing interest in problems of reasoning about physical systems. The papers present knowledge bases for a number of very different domains, from heat flow, to transistors, to digital computation. Anyone concerned with automated reasoning about the real (physical) world should read and understand this material. {\textcopyright} 1984.},
author = {Bobrow, Daniel G.},
doi = {10.1016/b978-0-444-87670-6.50004-2},
isbn = {0444599215},
issn = {00043702},
journal = {Artif. Intell.},
number = {1-3},
pages = {1--5},
publisher = {North-Holland},
title = {{Qualitative reasoning about physical systems: An introduction}},
volume = {24},
year = {1984}
}
@misc{Rej2015,
abstract = {Lithium is an important medication in the treatment of mood disorders. However, clinicians are hesitant to use lithium in older adults for fear of its medical effects, particularly kidney disease. This review describes the current understanding of the epidemiology and mechanisms underlying chronic kidney disease (CKD) in older lithium users, with recommendations for using lithium safely in late life. Prevalence estimates of CKD in older lithium users range from 42-50 {\%}, which does not differ greatly from the 37.8 {\%} rates seen in community-dwelling non-lithium using, non-psychiatric populations. Clinical and pre-clinical data suggest a variety of synergistic mechanisms contributing to CKD in older lithium users, including aging, cardiovascular factors, oxidative stress, inflammation, nephrogenic diabetes insipidus, acute kidney injury, and medication interactions. With regards to CKD, lithium can be used safely in many older adults with mood disorders. Compared to patients with pre-existing CKD, those with an estimated glomerular filtration rate {\textgreater}60 mL/min/1.73 m(2) are probably not as susceptible to lithium-associated renal decline. Using lithium concentrations {\textless}0.8 mmol/L; monitoring lithium concentrations and renal function every 3-6 months; being vigilant about concurrent medication use (e.g., diuretics, anti-inflammatories); as well as preventing/treating acute kidney injury, nephrogenic diabetes insipidus, diabetes mellitus, hypertension, smoking, and coronary artery disease can all help prevent CKD and further renal decline in older lithium users.},
author = {Rej, Soham and Elie, Dominique and Mucsi, Istvan and Looper, Karl J. and Segal, Marilyn},
booktitle = {Drugs and Aging},
doi = {10.1007/s40266-014-0234-9},
isbn = {1170-229X},
issn = {11791969},
language = {eng},
month = {jan},
number = {1},
pages = {31--42},
pmid = {25519823},
title = {{Chronic Kidney Disease in Lithium-Treated Older Adults: A Review of Epidemiology, Mechanisms, and Implications for the Treatment of Late-Life Mood Disorders}},
volume = {32},
year = {2014}
}
@article{Kolenikov2009,
abstract = {The last several years have seen a growth in the number of publications in economics that use principal component analysis (PCA) in the area of welfare studies. This paper explores the ways discrete data can be incorporated into PCA. The effects of discreteness of the observed variables on the PCA are reviewed. The statistical properties of the popular Filmer and Pritchett (2001) procedure are analyzed. The concepts of polychoric and polyserial correlations are introduced with appropriate references to the existing literature demonstrating their statistical properties. A large simulation study is carried out to compare various implementations of discrete data PCA. The simulation results show that the currently used method of running PCA on a set of dummy variables as proposed by Filmer and Pritchett (2001) can be improved upon by using procedures appropriate for discrete data, such as retaining the ordinal variables without breaking them into a set of dummy variables or using polychoric correlations. An empirical example using Bangladesh 2000 Demographic and Health Survey data helps in explaining the differences between procedures.},
author = {Kolenikov, Stanislav and Angeles, Gustavo},
doi = {10.1111/j.1475-4991.2008.00309.x},
issn = {00346586},
journal = {Rev. Income Wealth},
month = {mar},
number = {1},
pages = {128--165},
publisher = {John Wiley {\&} Sons, Ltd (10.1111)},
title = {{Socioeconomic status measurement with discrete proxy variables: Is principal component analysis a reliable answer?}},
url = {http://doi.wiley.com/10.1111/j.1475-4991.2008.00309.x},
volume = {55},
year = {2009}
}
@article{Robins2000,
abstract = {In observational studies with exposures or treatments that vary over time, standard approaches for adjustment of confounding are biased when there exist time-dependent confounders that are also affected by previous treatment. This paper introduces marginal structural models, a new class of causal models that allow for improved adjustment of confounding in those situations. The parameters of a marginal structural model can be consistently estimated using a new class of estimators, the inverse-probability-of-treatment weighted estimators.},
author = {Robins, J M and Hern{\'{a}}n, M A and Brumback, B},
issn = {1044-3983},
journal = {Epidemiology},
month = {sep},
number = {5},
pages = {550--60},
pmid = {10955408},
title = {{Marginal structural models and causal inference in epidemiology.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10955408},
volume = {11},
year = {2000}
}
@article{Jiang2018,
abstract = {Knowing when a classifier's prediction can be trusted is useful in many applications and critical for safely using AI. While the bulk of the effort in machine learning research has been towards improving classifier performance, understanding when a classifier's predictions should and should not be trusted has received far less attention. The standard approach is to use the classifier's discriminant or confidence score; however, we show there exists an alternative that is more effective in many situations. We propose a new score, called the trust score, which measures the agreement between the classifier and a modified nearest-neighbor classifier on the testing example. We show empirically that high (low) trust scores produce surprisingly high precision at identifying correctly (incorrectly) classified examples, consistently outperforming the classifier's confidence score as well as many other baselines. Further, under some mild distributional assumptions, we show that if the trust score for an example is high (low), the classifier will likely agree (disagree) with the Bayes-optimal classifier. Our guarantees consist of non-asymptotic rates of statistical consistency under various nonparametric settings and build on recent developments in topological data analysis.},
archivePrefix = {arXiv},
arxivId = {1805.11783},
author = {Jiang, Heinrich and Kim, Been and Guan, Melody Y. and Gupta, Maya},
eprint = {1805.11783},
month = {may},
title = {{To Trust Or Not To Trust A Classifier}},
url = {http://arxiv.org/abs/1805.11783},
year = {2018}
}
@article{Tomasev2019,
abstract = {The early prediction of deterioration could have an important role in supporting healthcare professionals, as an estimated 11{\%} of deaths in hospital follow a failure to promptly recognize and treat deteriorating patients1. To achieve this goal requires predictions of patient risk that are continuously updated and accurate, and delivered at an individual level with sufficient context and enough time to act. Here we develop a deep learning approach for the continuous risk prediction of future deterioration in patients, building on recent work that models adverse events from electronic health records2–17 and using acute kidney injury—a common and potentially life-threatening condition18—as an exemplar. Our model was developed on a large, longitudinal dataset of electronic health records that cover diverse clinical environments, comprising 703,782 adult patients across 172 inpatient and 1,062 outpatient sites. Our model predicts 55.8{\%} of all inpatient episodes of acute kidney injury, and 90.2{\%} of all acute kidney injuries that required subsequent administration of dialysis, with a lead time of up to 48 h and a ratio of 2 false alerts for every true alert. In addition to predicting future acute kidney injury, our model provides confidence assessments and a list of the clinical features that are most salient to each prediction, alongside predicted future trajectories for clinically relevant blood tests9. Although the recognition and prompt treatment of acute kidney injury is known to be challenging, our approach may offer opportunities for identifying patients at risk within a time window that enables early treatment.},
author = {Toma{\v{s}}ev, Nenad and Glorot, Xavier and Rae, Jack W. and Zielinski, Michal and Askham, Harry and Saraiva, Andre and Mottram, Anne and Meyer, Clemens and Ravuri, Suman and Protsyuk, Ivan and Connell, Alistair and Hughes, C{\'{i}}an O. and Karthikesalingam, Alan and Cornebise, Julien and Montgomery, Hugh and Rees, Geraint and Laing, Chris and Baker, Clifton R. and Peterson, Kelly and Reeves, Ruth and Hassabis, Demis and King, Dominic and Suleyman, Mustafa and Back, Trevor and Nielson, Christopher and Ledsam, Joseph R. and Mohamed, Shakir},
doi = {10.1038/s41586-019-1390-1},
issn = {0028-0836},
journal = {Nature},
keywords = {Predictive markers,Preventive medicine,Translational research},
month = {aug},
number = {7767},
pages = {116--119},
publisher = {Nature Publishing Group},
title = {{A clinically applicable approach to continuous prediction of future acute kidney injury}},
url = {http://www.nature.com/articles/s41586-019-1390-1},
volume = {572},
year = {2019}
}
@misc{Winston_teaching_mit_ocw,
abstract = {Patrick Winston. 6.034 Artificial Intelligence. Fall 2010. Massachusetts Institute of Technology: MIT OpenCourseWare, https://ocw.mit.edu. License: Creative Commons BY-NC-SA.},
author = {Winston, Patrick},
title = {{Artificial Intelligence | Electrical Engineering and Computer Science | MIT OpenCourseWare}},
url = {https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/{\#}},
urldate = {2019-10-19}
}
@article{Such2017,
abstract = {Deep artificial neural networks (DNNs) are typically trained via gradient-based learning algorithms, namely backpropagation. Evolution strategies (ES) can rival backprop-based algorithms such as Q-learning and policy gradients on challenging deep reinforcement learning (RL) problems. However, ES can be considered a gradient-based algorithm because it performs stochastic gradient descent via an operation similar to a finite-difference approximation of the gradient. That raises the question of whether non-gradient-based evolutionary algorithms can work at DNN scales. Here we demonstrate they can: we evolve the weights of a DNN with a simple, gradient-free, population-based genetic algorithm (GA) and it performs well on hard deep RL problems, including Atari and humanoid locomotion. The Deep GA successfully evolves networks with over four million free parameters, the largest neural networks ever evolved with a traditional evolutionary algorithm. These results (1) expand our sense of the scale at which GAs can operate, (2) suggest intriguingly that in some cases following the gradient is not the best choice for optimizing performance, and (3) make immediately available the multitude of neuroevolution techniques that improve performance. We demonstrate the latter by showing that combining DNNs with novelty search, which encourages exploration on tasks with deceptive or sparse reward functions, can solve a high-dimensional problem on which reward-maximizing algorithms (e.g.$\backslash$ DQN, A3C, ES, and the GA) fail. Additionally, the Deep GA is faster than ES, A3C, and DQN (it can train Atari in {\$}{\{}\backslashraise.17ex\backslashhbox{\{}{\$}$\backslash$scriptstyle$\backslash$sim{\$}{\}}{\}}{\$}4 hours on one desktop or {\$}{\{}\backslashraise.17ex\backslashhbox{\{}{\$}$\backslash$scriptstyle$\backslash$sim{\$}{\}}{\}}{\$}1 hour distributed on 720 cores), and enables a state-of-the-art, up to 10,000-fold compact encoding technique.},
archivePrefix = {arXiv},
arxivId = {1712.06567},
author = {Such, Felipe Petroski and Madhavan, Vashisht and Conti, Edoardo and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
doi = {1712.06567},
eprint = {1712.06567},
isbn = {9781439854242},
issn = {07387946},
month = {dec},
pmid = {10264727},
title = {{Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning}},
url = {http://arxiv.org/abs/1712.06567},
year = {2017}
}
@misc{Boyce2014,
abstract = {The entire drug safety enterprise has a need to search, retrieve, evaluate, and synthesize scientific evidence more efficiently. This discovery and synthesis process would be greatly accelerated through access to a common framework that brings all relevant information sources together within a standardized structure. This presents an opportunity to establish an open-source community effort to develop a global knowledge base, one that brings together and standardizes all available information for all drugs and all health outcomes of interest (HOIs) from all electronic sources pertinent to drug safety. To make this vision a reality, we have established a workgroup within the Observational Health Data Sciences and Informatics (OHDSI, http://​ohdsi.​org) collaborative. The workgroup's mission is to develop an open-source standardized knowledge base for the effects of medical products and an efficient procedure for maintaining and expanding it. The knowledge base will make it simpler for practitioners to access, retrieve, and synthesize evidence so that they can reach a rigorous and accurate assessment of causal relationships between a given drug and HOI. Development of the knowledge base will proceed with the measureable goal of supporting an efficient and thorough evidence-based assessment of the effects of 1,000 active ingredients across 100 HOIs. This non-trivial task will result in a high-quality and generally applicable drug safety knowledge base. It will also yield a reference standard of drug–HOI pairs that will enable more advanced methodological research that empirically evaluates the performance of drug safety analysis methods.},
author = {Boyce, Richard D. and Ryan, Patrick B. and Nor{\'{e}}n, G. Niklas and Schuemie, Martijn J. and Reich, Christian and Duke, Jon and Tatonetti, Nicholas P. and Trifir{\`{o}}, Gianluca and Harpaz, Rave and Overhage, J. Marc and Hartzema, Abraham G. and Khayter, Mark and Voss, Erica A. and Lambert, Christophe G. and Huser, Vojtech and Dumontier, Michel},
booktitle = {Drug Saf.},
doi = {10.1007/s40264-014-0189-0},
isbn = {0114-5916},
issn = {11791942},
month = {aug},
number = {8},
pages = {557--567},
pmid = {24985530},
title = {{Bridging islands of information to establish an integrated knowledge base of drugs and health outcomes of interest}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24985530 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4134480 http://link.springer.com/10.1007/s40264-014-0189-0},
volume = {37},
year = {2014}
}
@article{Hinton2014,
abstract = {It is possible to learn multiple layers of non-linear features by backpropagating error derivatives through a feedforward neural network. This is a very effective learning procedure when there is a huge amount of labeled training data, but for many learning tasks very few labeled examples are available. In an effort to overcome the need for labeled data, several different generative models were developed that learned interesting features by modeling the higher order statistical structure of a set of input vectors. One of these generative models, the restricted Boltzmann machine (RBM), has no connections between its hidden units and this makes perceptual inference and learning much simpler. More significantly, after a layer of hidden features has been learned, the activities of these features can be used as training data for another RBM. By applying this idea recursively, it is possible to learn a deep hierarchy of progressively more complicated features without requiring any labeled data. This deep hierarchy can then be treated as a feedforward neural network which can be discriminatively fine-tuned using backpropagation. Using a stack of RBMs to initialize the weights of a feedforward neural network allows backpropagation to work effectively in much deeper networks and it leads to much better generalization. A stack of RBMs can also be used to initialize a deep Boltzmann machine that has many hidden layers. Combining this initialization method with a new method for fine-tuning the weights finally leads to the first efficient way of training Boltzmann machines with many hidden layers and millions of weights.},
author = {Hinton, Geoffrey},
doi = {10.1111/cogs.12049},
issn = {03640213},
journal = {Cogn. Sci.},
keywords = {Backpropagation,Boltzmann machines,Contrastive divergence,Deep learning,Distributed representations,Learning features,Learning graphical models,Variational learning},
month = {aug},
number = {6},
pages = {1078--1101},
pmid = {23800216},
title = {{Where do features come from?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23800216 http://doi.wiley.com/10.1111/cogs.12049},
volume = {38},
year = {2014}
}
@article{Cao2017a,
abstract = {The article examines the relationship between data science problems and the need for systematic thinking, methodologies, and approaches in order to develop machine intelligence. It discusses how data science helps scientists understand and synthesize complexities and intelligence in problem solving. Particular attention is given to data-driven machines to represent and advance human-like intuition and creative thinking through human-data interaction.},
author = {Cao, Longbing},
doi = {10.1145/3015456},
issn = {1557-7317},
journal = {Commun. ACM},
keywords = {computer science,data science},
month = {jul},
number = {8},
pages = {59--68},
title = {{Data science: Challenges and directions}},
url = {http://dl.acm.org/citation.cfm?doid=3127343.3015456},
volume = {60},
year = {2017}
}
@article{Arnold2019,
abstract = {{\textcopyright} 2018, The Author(s) 2018. ‘Unexplained residuals' models have been used within lifecourse epidemiology to model an exposure measured longitudinally at several time points in relation to a distal outcome. It has been claimed that these models have several advantages, including: the ability to estimate multiple total causal effects in a single model, and additional insight into the effect on the outcome of greater-than-expected increases in the exposure compared to traditional regression methods. We evaluate these properties and prove mathematically how adjustment for confounding variables must be made within this modelling framework. Importantly, we explicitly place unexplained residual models in a causal framework using directed acyclic graphs. This allows for theoretical justification of appropriate confounder adjustment and provides a framework for extending our results to more complex scenarios than those examined in this paper. We also discuss several interpretational issues relating to unexplained residual models within a causal framework. We argue that unexplained residual models offer no additional insights compared to traditional regression methods, and, in fact, are more challenging to implement; moreover, they artificially reduce estimated standard errors. Consequently, we conclude that unexplained residual models, if used, must be implemented with great care.},
author = {Arnold, K. F. and Ellison, G. T.H. and Gadd, S. C. and Textor, J and Tennant, P. W.G. and Heppenstall, A and Gilthorpe, M. S.},
doi = {10.1177/0962280218756158},
issn = {14770334},
journal = {Stat. Methods Med. Res.},
keywords = {Unexplained residuals model,causal inference,conditional analysis,conditional growth,conditional regression model,conditional size,conditional weight,directed acyclic graph,lifecourse epidemiology},
month = {may},
number = {5},
pages = {1347--1364},
publisher = {SAGE PublicationsSage UK: London, England},
title = {{Adjustment for time-invariant and time-varying confounders in ‘unexplained residuals' models for longitudinal data within a causal framework and associated challenges}},
url = {http://journals.sagepub.com/doi/10.1177/0962280218756158},
volume = {28},
year = {2019}
}
@article{Kipf2019,
abstract = {A structured understanding of our world in terms of objects, relations, and hierarchies is an important component of human cognition. Learning such a structured world model from raw sensory data remains a challenge. As a step towards this goal, we introduce Contrastively-trained Structured World Models (C-SWMs). C-SWMs utilize a contrastive approach for representation learning in environments with compositional structure. We structure each state embedding as a set of object representations and their relations, modeled by a graph neural network. This allows objects to be discovered from raw pixel observations without direct supervision as part of the learning process. We evaluate C-SWMs on compositional environments involving multiple interacting objects that can be manipulated independently by an agent, simple Atari games, and a multi-object physics simulation. Our experiments demonstrate that C-SWMs can overcome limitations of models based on pixel reconstruction and outperform typical representatives of this model class in highly structured environments, while learning interpretable object-based representations.},
archivePrefix = {arXiv},
arxivId = {arXiv:1905.00956v1},
author = {Anon},
eprint = {arXiv:1905.00956v1},
month = {nov},
pages = {1--16},
title = {{Contrastive Learning of Structured World Models}},
url = {http://arxiv.org/abs/1911.12247},
year = {2019}
}
@article{Meyer2014,
abstract = {Objective: We study the dependence or independence of reliance and compliance as two responses to alarms to understand the mechanisms behind these responses.$\backslash$nBackground: Alarms, alerts, and other binary cues affect user behavior in complex ways. The suggestion has been made that there are two different responses to alerts—compliance (the tendency to perform an action cued by the alert) and reliance (the tendency to refrain from actions as long as no alert is issued). The study tests the degree to which these two responses are indeed independent.$\backslash$nMethod: An experiment tested the effects of the positive and negative predictive values of the alerts (PPV and NPV) on measures of compliance and reliance based on cutoff settings, response times, and subjective confidence.$\backslash$nResults: For cutoff settings and response times, compliance was unaffected by the irrelevant NPV, whereas reliance depended on the irrelevant PPV. For subjective estimates, there were no significant effects of the irrelevant variables.$\backslash$nConclusion: Results suggest that compliance is relatively stable and unaffected by irrelevant information (the NPV), whereas reliance is also affected by the PPV. The results support the notion that reliance and compliance are separate, but related, forms of trust.$\backslash$nApplication: False alarm rates, which affect PPV, determine both the response to alerts (compliance) and the tendency to limit precautions when no alert is issued (reliance).},
author = {Meyer, Joachim and Wiczorek, Rebecca and G{\"{u}}nzler, Torsten},
doi = {10.1177/0018720813512865},
issn = {15478181},
journal = {Hum. Factors},
keywords = {Alerts,Automation,Compliance,Confdence,Reliance,Signal detection theory,Trust,Warnings},
month = {aug},
number = {5},
pages = {840--849},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Measures of reliance and compliance in aided visual scanning}},
url = {http://journals.sagepub.com/doi/10.1177/0018720813512865},
volume = {56},
year = {2014}
}
@article{Aickelin2019,
author = {Aickelin, Uwe and Chapman, Wendy W. and Hart, Graeme K.},
doi = {10.3389/fdgth.2019.00002},
issn = {2673-253X},
journal = {Front. Digit. Heal.},
month = {dec},
title = {{Health Informatics—Ambitions and Purpose}},
url = {https://www.frontiersin.org/article/10.3389/fdgth.2019.00002/full},
volume = {1},
year = {2019}
}
@article{Moon2019,
abstract = {The high-dimensional data created by high-throughput technologies require visualization tools that reveal data structure and patterns in an intuitive form. We present PHATE, a visualization method that captures both local and global nonlinear structure using an information-geometric distance between data points. We compare PHATE to other tools on a variety of artificial and biological datasets, and find that it consistently preserves a range of patterns in data, including continual progressions, branches and clusters, better than other tools. We define a manifold preservation metric, which we call denoised embedding manifold preservation (DEMaP), and show that PHATE produces lower-dimensional embeddings that are quantitatively better denoised as compared to existing visualization methods. An analysis of a newly generated single-cell RNA sequencing dataset on human germ-layer differentiation demonstrates how PHATE reveals unique biological insight into the main developmental branches, including identification of three previously undescribed subpopulations. We also show that PHATE is applicable to a wide variety of data types, including mass cytometry, single-cell RNA sequencing, Hi-C and gut microbiome data. PHATE, a new data visualization tool, better preserves patterns in high-dimensional data after dimensionality reduction.},
author = {Moon, Kevin R. and van Dijk, David and Wang, Zheng and Gigante, Scott and Burkhardt, Daniel B. and Chen, William S. and Yim, Kristina and van den Elzen, Antonia and Hirn, Matthew J. and Coifman, Ronald R. and Ivanova, Natalia B. and Wolf, Guy and Krishnaswamy, Smita},
doi = {10.1038/s41587-019-0336-3},
issn = {1087-0156},
journal = {Nat. Biotechnol.},
keywords = {Data mining,Machine learning},
month = {dec},
number = {12},
pages = {1482--1492},
publisher = {Nature Publishing Group},
title = {{Visualizing structure and transitions in high-dimensional biological data}},
url = {http://www.nature.com/articles/s41587-019-0336-3},
volume = {37},
year = {2019}
}
@article{Peterson2013,
abstract = {Abstract Background The Global Burden of Disease, Injuries, and Risk Factors Study 2010 (GBD 2010) required age-specific prevalence estimates for over 300 outcomes for all countries. Results of systematic reviews were often very sparse and noisy, so DisMod-MR was frequently used to combine all available data and create estimates. We investigated the robustness of this approach by comparing the negative binomial rate model to alternative rate models using out-of-sample predictive validity. Methods We compared all disease and injury models analysed with DisMod-MR from GBD 2010 with more than four prevalence data points in western Europe. For each disease/injury model, western European prevalence data were partitioned into random 75/25{\%} train/test splits for 1000 replicates. We fitted an age-specific rate model to the training data and used the results to predict values for the test data for each replicate. We compared the bias, median absolute error (MAE), and percent coverage for each replicate to determine if the negative binomial, binomial, normal, or lognormal rate model was superior for each condition. Findings Each metric has its own superior rate model. The lognormal model had the most replicates with smallest bias. The binomial model had the most with minimum MAE. The negative binomial model had the percent coverage closest to the target coverage of 95{\%}. Interpretation Depending on the metric, different rate models are superior. The negative binomial model provides an appealing balance of accuracy, precision, and calibration. Funding Bill {\&} Melinda Gates Foundation.},
author = {Peterson, Hannah M and Flaxman, Abraham D},
doi = {10.1016/s0140-6736(13)61364-1},
issn = {01406736},
journal = {Lancet},
month = {jun},
pages = {S110},
publisher = {Elsevier},
title = {{Meta-regression with DisMod-MR: how robust is the model?}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0140673613613641},
volume = {381},
year = {2013}
}
@article{Rahim2019,
abstract = {Estimating covariances from functional Magnetic Resonance Imaging at rest (r-fMRI) can quantify interactions between brain regions. Also known as brain functional connectivity, it reflects inter-subject variations in behavior and cognition, and characterizes neuropathologies. Yet, with noisy and short time-series, as in r-fMRI, covariance estimation is challenging and calls for penalization, as with shrinkage approaches. We introduce population shrinkage of covariance estimator (PoSCE) : a covariance estimator that integrates prior knowledge of covariance distribution over a large population, leading to a non-isotropic shrinkage. The shrinkage is tailored to the Riemannian geometry of symmetric positive definite matrices. It is coupled with a probabilistic modeling of the individual and population covariance distributions. Experiments on two large r-fMRI datasets (HCP n=815, CamCAN n=626) show that PoSCE has a better bias-variance trade-off than existing covariance estimates: this estimator relates better functional-connectivity measures to cognition while capturing well intra-subject functional connectivity.},
author = {Rahim, Mehdi and Thirion, Bertrand and Varoquaux, Ga{\"{e}}l},
doi = {10.1016/j.media.2019.03.001},
issn = {13618423},
journal = {Med. Image Anal.},
keywords = {Covariance,Functional connectivity,Population models,Shrinkage},
month = {may},
pages = {138--148},
publisher = {Elsevier},
title = {{Population shrinkage of covariance (PoSCE) for better individual brain functional-connectivity estimation}},
url = {https://www.sciencedirect.com/science/article/pii/S1361841518301014},
volume = {54},
year = {2019}
}
@incollection{Smyth2005,
abstract = {A survey is given of differential expression analyses using the linear modeling features of the limma package. The chapter starts with the simplest replicated designs and progresses through experiments with two or more groups, direct designs, factorial designs and time course experiments. Experiments with technical as well as biological replication are considered. Empirical Bayes test statistics are explained. The use of quality weights, adaptive background correction and control spots in conjunction with linear modelling is illustrated on the $\beta$7 data.},
address = {New York},
archivePrefix = {arXiv},
arxivId = {16495579},
author = {Smyth, G. K.},
booktitle = {Bioinforma. Comput. Biol. Solut. Using R Bioconductor},
eprint = {16495579},
isbn = {0-387-25146-4},
issn = {1544-6115},
pages = {397--420},
pmid = {16495579},
publisher = {Springer-Verlag},
title = {{limma: Linear Models for Microarray Data}},
url = {http://link.springer.com/10.1007/0-387-29362-0{\_}23},
year = {2005}
}
@article{Mozafari2019,
abstract = {Application of deep convolutional spiking neural networks (SNNs) to artificial intelligence (AI) tasks has recently gained a lot of interest since SNNs are hardware-friendly and energy-efficient. Unlike the non-spiking counterparts, most of the existing SNN simulation frameworks are not practically efficient enough for large-scale AI tasks. In this paper, we introduce SpykeTorch, an open-source high-speed simulation framework based on PyTorch. This framework simulates convolutional SNNs with at most one spike per neuron and the rank-order encoding scheme. In terms of learning rules, both spike-timing-dependent plasticity (STDP) and reward-modulated STDP (R-STDP) are implemented, but other rules could be implemented easily. Apart from the aforementioned properties, SpykeTorch is highly generic and capable of reproducing the results of various studies. Computations in the proposed framework are tensor-based and totally done by PyTorch functions, which in turn brings the ability of just-in-time optimization for running on CPUs, GPUs, or Multi-GPU platforms.},
archivePrefix = {arXiv},
arxivId = {1903.02440},
author = {Mozafari, Milad and Ganjtabesh, Mohammad and Nowzari-Dalini, Abbas and Masquelier, Timoth{\'{e}}e},
doi = {10.3389/fnins.2019.00625},
eprint = {1903.02440},
issn = {1662-453X},
journal = {Front. Neurosci.},
keywords = {Convolutional spiking neural networks,GPU acceleration,One spike per neuron,STDP,Tensor-based computing,Time-to-first-spike coding,reward-modulated STDP},
month = {jul},
pages = {625},
publisher = {Frontiers},
title = {{SpykeTorch: Efficient Simulation of Convolutional Spiking Neural Networks with at most one Spike per Neuron}},
url = {https://www.frontiersin.org/article/10.3389/fnins.2019.00625/full http://arxiv.org/abs/1903.02440},
volume = {13},
year = {2019}
}
@article{Nicholson2015a,
abstract = {The vector autoregression (VAR) has long proven to be an effective method for modeling the joint dynamics of macroeconomic time series, as well as for forecasting. One major shortcoming of the VAR that has limited its applicability is its heavy parameterization: the parameter space grows quadratically with the number of series included, quickly exhausting the available degrees of freedom. Consequently, using VARs for forecasting is intractable for low-frequency, high-dimensional macroeconomic data. However, empirical evidence suggests that VARs that incorporate more component series tend to result in more accurate forecasts. Most conventional methods that allow for the estimation of large VARs either require ad hoc subjective specifications or are computationally infeasible. Moreover, as global economies become more intricately intertwined, there has been a substantial interest in incorporating the impact of stochastic, unmodeled exogenous variables. Vector autoregression with exogenous variables (VARX) extends the VAR to allow for the inclusion of unmodeled variables, but faces similar dimensionality challenges. This paper introduces the VARX-L framework, a structured family of VARX models, and provides a methodology that allows for both efficient estimation and accurate forecasting in high-dimensional analysis. VARX-L adapts several prominent scalar regression regularization techniques to a vector time series context, which greatly reduces the parameter space of VAR and VARX models. We also highlight a compelling extension that allows for shrinking toward reference models, such as a vector random walk. We demonstrate the efficacy of VARX-L in both low- and high-dimensional macroeconomic forecasting applications and simulated data examples. Our methodology is easy to reproduce in a publicly available R package.},
archivePrefix = {arXiv},
arxivId = {1508.07497},
author = {Nicholson, William B. and Matteson, David S. and Bien, Jacob},
doi = {10.1016/j.ijforecast.2017.01.003},
eprint = {1508.07497},
issn = {01692070},
journal = {Int. J. Forecast.},
keywords = {Big data,Forecasting,Group lasso,Macroeconometrics,Time series},
month = {aug},
number = {3},
pages = {627--651},
title = {{VARX-L: Structured regularization for large vector autoregressions with exogenous variables}},
url = {http://arxiv.org/abs/1508.07497},
volume = {33},
year = {2017}
}
@article{Shi2019,
abstract = {This paper addresses the use of neural networks for the estimation of treatment effects from observational data. Generally, estimation proceeds in two stages. First, we fit models for the expected outcome and the probability of treatment (propensity score) for each unit. Second, we plug these fitted models into a downstream estimator of the effect. Neural networks are a natural choice for the models in the first step. The question we address is: how can we adapt the design and training of the neural networks used in the first step in order to improve the quality of the final estimate of the treatment effect? We propose two adaptations based on insights from the statistical literature on the estimation of treatment effects. The first is a new architecture, the Dragonnet, that exploits the sufficiency of the propensity score for estimation adjustment. The second is a regularization procedure, targeted regularization, that induces a bias towards models that have non-parametrically optimal asymptotic properties `out-of-the-box`. Studies on benchmark datasets for causal inference show these adaptations outperform existing methods. Code is available at github.com/claudiashi57/dragonnet},
archivePrefix = {arXiv},
arxivId = {1906.02120},
author = {Shi, Claudia and Blei, David M. and Veitch, Victor},
eprint = {1906.02120},
month = {jun},
title = {{Adapting Neural Networks for the Estimation of Treatment Effects}},
url = {http://arxiv.org/abs/1906.02120},
year = {2019}
}
@article{Galante2018,
abstract = {Background: The rising number of young people going to university has led to concerns about an increasing demand for student mental health services. We aimed to assess whether provision of mindfulness courses to university students would improve their resilience to stress. Methods: We did this pragmatic randomised controlled trial at the University of Cambridge, UK. Students aged 18 years or older with no severe mental illness or crisis (self-assessed) were randomly assigned (1:1), via remote survey software using computer-generated random numbers, to receive either an 8 week mindfulness course adapted for university students (Mindfulness Skills for Students [MSS]) plus mental health support as usual, or mental health support as usual alone. Participants and the study management team were aware of group allocation, but allocation was concealed from the researchers, outcome assessors, and study statistician. The primary outcome was self-reported psychological distress during the examination period, as measured with the Clinical Outcomes in Routine Evaluation Outcome Measure (CORE–OM), with higher scores indicating more distress. The primary analysis was by intention to treat. This trial is registered with the Australia and New Zealand Clinical Trials Registry, number ACTRN12615001160527. Findings: Between Sept 28, 2015, and Jan 15, 2016, we randomly assigned 616 students to the MSS group (n=309) or the support as usual group (n=307). 453 (74{\%}) participants completed the CORE–OM during the examination period and 182 (59{\%}) MSS participants completed at least half of the course. MSS reduced distress scores during the examination period compared with support as usual, with mean CORE–OM scores of 0{\textperiodcentered}87 (SD 0{\textperiodcentered}50) in 237 MSS participants versus 1{\textperiodcentered}11 (0{\textperiodcentered}57) in 216 support as usual participants (adjusted mean difference −0{\textperiodcentered}14, 95{\%} CI −0{\textperiodcentered}22 to −0{\textperiodcentered}06; p=0{\textperiodcentered}001), showing a moderate effect size ($\beta$ −0{\textperiodcentered}44, 95{\%} CI −0{\textperiodcentered}60 to −0{\textperiodcentered}29; p{\textless}0{\textperiodcentered}0001). 123 (57{\%}) of 214 participants in the support as usual group had distress scores above an accepted clinical threshold compared with 88 (37{\%}) of 235 participants in the MSS group. On average, six students (95{\%} CI four to ten) needed to be offered the MSS course to prevent one from experiencing clinical levels of distress. No participants had adverse reactions related to self-harm, suicidality, or harm to others. Interpretation: Our findings show that provision of mindfulness training could be an effective component of a wider student mental health strategy. Further comparative effectiveness research with inclusion of controls for non-specific effects is needed to define a range of additional, effective interventions to increase resilience to stress in university students. Funding: University of Cambridge and National Institute for Health Research Collaboration for Leadership in Applied Health Research and Care East of England.},
author = {Galante, Julieta and Dufour, G{\'{e}}raldine and Vainre, Maris and Wagner, Adam P and Stochl, Jan and Benton, Alice and Lathia, Neal and Howarth, Emma and Jones, Peter B},
doi = {10.1016/S2468-2667(17)30231-1},
issn = {24682667},
journal = {Lancet Public Heal.},
month = {feb},
number = {2},
pages = {e72--e81},
pmid = {29422189},
publisher = {Elsevier},
title = {{A mindfulness-based intervention to increase resilience to stress in university students (the Mindful Student Study): a pragmatic randomised controlled trial}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/29422189 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5813792},
volume = {3},
year = {2018}
}
@article{Choi2016a,
abstract = {Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two important challenges remain: -Data insufficiency:Often in healthcare predictive modeling, the sample size is insufficient for deep learning methods to achieve satisfactory results. -Interpretation:The representations learned by deep learning methods should align with medical knowledge. To address these challenges, we propose a GRaph-based Attention Model, GRAM that supplements electronic health records (EHR) with hierarchical information inherent to medical ontologies. Based on the data volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism. We compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart failure prediction task. Compared to the basic RNN, GRAM achieved 10{\%} higher accuracy for predicting diseases rarely observed in the training data and 3{\%} improved area under the ROC curve for predicting heart failure using an order of magnitude less training data. Additionally, unlike other methods, the medical concept representations learned by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts.},
archivePrefix = {arXiv},
arxivId = {1611.07012},
author = {Choi, Edward and Bahadori, Mohammad Taha and Song, Le and Stewart, Walter F. and Sun, Jimeng},
doi = {10.1145/3097983.3098126},
eprint = {1611.07012},
isbn = {9781450348874},
issn = {03092402},
month = {nov},
pmid = {26498097},
title = {{GRAM: Graph-based Attention Model for Healthcare Representation Learning}},
url = {https://openreview.net/forum?id=SkgewU5ll http://arxiv.org/abs/1611.07012},
year = {2016}
}
@article{Tangherloni2019,
abstract = {Autoencoders (AEs) have been effectively used to capture the non-linearities among gene interactions of single-cell RNA sequencing (scRNA-Seq) data. However, their integration with the common scRNA-Seq bioinformatics pipelines still poses a challenge. Here, we introduce scAEspy, a unifying tool that embodies five of the most advanced AEs and different loss functions, including two novel AEs that we developed. scAEspy allows the integration of data generated using different scRNA-Seq platforms. We benchmarked scAEspy against principal component analysis (PCA) on five public datasets, showing that our new AEs outperform the existing solutions, achieving more than 20{\%} increase of the Rand Index in the identification of cell clusters.},
author = {Tangherloni, Andrea and Ricciuti, Federico and Besozzi, Daniela and Lio, Pietro and Cvejic, Ana},
doi = {10.1101/727867},
journal = {bioRxiv Bioinforma.},
month = {aug},
pages = {1--12},
publisher = {Cold Spring Harbor Laboratory},
title = {{scAEspy: a unifying tool based on autoencoders for the analysis of single-cell RNA sequencing data}},
url = {http://biorxiv.org/cgi/content/short/727867v1?rss=1},
year = {2019}
}
@article{Boelen2007,
abstract = {Few studies have examined treatments for complicated grief-a debilitating condition that can develop after the loss of a loved one. This study compared the effectiveness of cognitive-behavioral therapy with a nonspecific treatment with supportive counseling (SC). Using a minimization method, 54 mourners with clinically significant levels of complicated grief were allocated to 1 of 3 treatment conditions: (a) a condition of 6 sessions of cognitive restructuring (CR) and 6 sessions of exposure therapy (ET; CR + ET), (b) a condition in which these interventions were applied in reversed order (ET + CR), and (c) 12 sessions of SC. Outcomes showed that the 2 cognitive-behavioral therapy conditions produced more improvement in complicated grief and general psychopathology than SC in the completers and intention-to-treat groups. Comparison of the cognitive-behavioral conditions showed that "pure" exposure was more effective than "pure" cognitive restructuring, that adding ET to CR led to more additional improvement than adding CR to ET, and that ET + CR was more efficacious than CR + ET. Effect sizes of ET + CR were encouraging and compare favorably with those found in earlier bereavement intervention studies. {\textcopyright} 2007 American Psychological Association.},
author = {Boelen, Paul A. and de Keijser, Jos and van den Hout, Marcel A. and van den Bout, Jan},
doi = {10.1037/0022-006X.75.2.277},
issn = {0022006X},
journal = {J. Consult. Clin. Psychol.},
keywords = {cognitive-behavioral therapy,death and dying,grief},
month = {apr},
number = {2},
pages = {277--284},
pmid = {17469885},
title = {{Treatment of Complicated Grief: A Comparison Between Cognitive-Behavioral Therapy and Supportive Counseling}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17469885 http://doi.apa.org/getdoi.cfm?doi=10.1037/0022-006X.75.2.277},
volume = {75},
year = {2007}
}
@article{Wang2014e,
abstract = {Falling rule lists are classification models consisting of an ordered list of if-then rules, where (i) the order of rules determines which example should be classified by each rule, and (ii) the estimated probability of success decreases monotonically down the list. These kinds of rule lists are inspired by healthcare applications where patients would be stratified into risk sets and the highest at-risk patients should be considered first. We provide a Bayesian framework for learning falling rule lists that does not rely on traditional greedy decision tree learning methods.},
archivePrefix = {arXiv},
arxivId = {1411.5899},
author = {Wang, Fulton and Rudin, Cynthia},
eprint = {1411.5899},
issn = {15337928},
journal = {arXiv},
month = {nov},
pages = {1--13},
title = {{Falling Rule Lists}},
url = {http://arxiv.org/abs/1411.5899},
volume = {38},
year = {2014}
}
@book{Venables2002,
abstract = {... Long 1997; Venables and Ripley 1997), and negative binomial (Long 1997; Venables and Ripley 1997). All three of these regression models are parametric ...},
address = {New York, NY},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Venables, W. N. and Ripley, B. D.},
doi = {10.1007/978-0-387-21706-2},
eprint = {arXiv:1011.1669v3},
isbn = {978-1-4419-3008-8},
issn = {0040-1706},
pmid = {2002022925},
publisher = {Springer New York},
series = {Statistics and Computing},
title = {{Modern Applied Statistics with S}},
url = {http://link.springer.com/10.1007/978-0-387-21706-2},
year = {2002}
}
@article{Rajkomar2018,
abstract = {Predictive modeling with electronic health record (EHR) data is anticipated to drive personalized medicine and improve healthcare quality. Constructing predictive statistical models typically requires extraction of curated predictor variables from normalized EHR data, a labor-intensive process that discards the vast majority of information in each patient's record. We propose a representation of patients' entire, raw EHR records based on the Fast Healthcare Interoperability Resources (FHIR) format. We demonstrate that deep learning methods using this representation are capable of accurately predicting multiple medical events from multiple centers without site-specific data harmonization. We validated our approach using de-identified EHR data from two U.S. academic medical centers with 216,221 adult patients hospitalized for at least 24 hours. In the sequential format we propose, this volume of EHR data unrolled into a total of 46,864,534,945 data points, including clinical notes. Deep learning models achieved high accuracy for tasks such as predicting in-hospital mortality (AUROC across sites 0.93-0.94), 30-day unplanned readmission (AUROC 0.75-0.76), prolonged length of stay (AUROC 0.85-0.86), and all of a patient's final discharge diagnoses (frequency-weighted AUROC 0.90). These models outperformed state-of-the-art traditional predictive models in all cases. We also present a case-study of a neural-network attribution system, which illustrates how clinicians can gain some transparency into the predictions. We believe that this approach can be used to create accurate and scalable predictions for a variety of clinical scenarios, complete with explanations that directly highlight evidence in the patient's chart.},
archivePrefix = {arXiv},
arxivId = {1801.07860},
author = {Rajkomar, Alvin and Oren, Eyal and Chen, Kai and Dai, Andrew M. and Hajaj, Nissan and Liu, Peter J. and Liu, Xiaobing and Sun, Mimi and Sundberg, Patrik and Yee, Hector and Zhang, Kun and Duggan, Gavin E. and Flores, Gerardo and Hardt, Michaela and Irvine, Jamie and Le, Quoc and Litsch, Kurt and Marcus, Jake and Mossin, Alexander and Tansuwan, Justin and Wang, De and Wexler, James and Wilson, Jimbo and Ludwig, Dana and Volchenboum, Samuel L. and Chou, Katherine and Pearson, Michael and Madabushi, Srinivasan and Shah, Nigam H. and Butte, Atul J. and Howell, Michael and Cui, Claire and Corrado, Greg and Dean, Jeffrey},
doi = {10.1038/s41746-018-0029-1},
eprint = {1801.07860},
isbn = {2398-6352},
issn = {2398-6352},
journal = {npj Digit. Med.},
keywords = {Machine learning,Medical research},
month = {dec},
number = {1},
pages = {18},
publisher = {Nature Publishing Group},
title = {{Scalable and accurate deep learning for electronic health records}},
url = {http://www.nature.com/articles/s41746-018-0029-1 http://arxiv.org/abs/1801.07860{\%}0Ahttp://dx.doi.org/10.1038/s41746-018-0029-1},
volume = {1},
year = {2018}
}
@misc{banerjee_software_bayes_regression,
author = {Banerjee, Soumya},
doi = {10.5281/zenodo.2543776},
title = {{Code for Bayesian Linear Regression}},
url = {https://uk.mathworks.com/matlabcentral/fileexchange/48511-bayesian-linear-regression},
year = {2019}
}
@article{Wang2019,
abstract = {The quest of `can machines think' and `can machines do what human do' are quests that drive the development of artificial intelligence. Although recent artificial intelligence succeeds in many data intensive applications, it still lacks the ability of learning from limited exemplars and fast generalizing to new tasks. To tackle this problem, one has to turn to machine learning, which supports the scientific study of artificial intelligence. Particularly, a machine learning problem called Few-Shot Learning (FSL) targets at this case. It can rapidly generalize to new tasks of limited supervised experience by turning to prior knowledge, which mimics human's ability to acquire knowledge from few examples through generalization and analogy. It has been seen as a test-bed for real artificial intelligence, a way to reduce laborious data gathering and computationally costly training, and antidote for rare cases learning. With extensive works on FSL emerging, we give a comprehensive survey for it. We first give the formal definition for FSL. Then we point out the core issues of FSL, which turns the problem from "how to solve FSL" to "how to deal with the core issues". Accordingly, existing works from the birth of FSL to the most recent published ones are categorized in a unified taxonomy, with thorough discussion of the pros and cons for different categories. Finally, we envision possible future directions for FSL in terms of problem setup, techniques, applications and theory, hoping to provide insights to both beginners and experienced researchers.},
archivePrefix = {arXiv},
arxivId = {1904.05046},
author = {Wang, Yaqing and Yao, Quanming},
eprint = {1904.05046},
month = {apr},
title = {{Few-shot Learning: A Survey}},
url = {http://arxiv.org/abs/1904.05046},
year = {2019}
}
@inproceedings{Falke2009a,
abstract = {An approach based on term rewriting techniques for the automated termination analysis of imperative programs operating on integers is presented. An imperative program is transformed into rewrite rules with constraints from quantifier-free Presburger arithmetic. Any computation in the imperative program corresponds to a rewrite sequence, and termination of the rewrite system thus implies termination of the imperative program. Termination of the rewrite system is analyzed using a decision procedure for Presburger arithmetic that identifies possible chains of rewrite rules, and automatically generated polynomial interpretations are used to show finiteness of such chains. An implementation of the approach has been evaluated on a large collection of imperative programs, thus demonstrating its effectiveness and practicality.},
author = {Falke, Stephan and Kapur, Deepak},
booktitle = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
doi = {10.1007/978-3-642-02959-2_22},
isbn = {3642029582},
issn = {03029743},
pages = {277--293},
publisher = {Springer, Berlin, Heidelberg},
title = {{A term rewriting approach to the automated termination analysis of imperative programs}},
url = {http://link.springer.com/10.1007/978-3-642-02959-2{\_}22},
volume = {5663 LNAI},
year = {2009}
}
@article{Zhavoronkov2019,
abstract = {We have developed a deep generative model, generative tensorial reinforcement learning (GENTRL), for de novo small-molecule design. GENTRL optimizes synthetic feasibility, novelty, and biological activity. We used GENTRL to discover potent inhibitors of discoidin domain receptor 1 (DDR1), a kinase target implicated in fibrosis and other diseases, in 21 days. Four compounds were active in biochemical assays, and two were validated in cell-based assays. One lead candidate was tested and demonstrated favorable pharmacokinetics in mice. A machine learning model allows the identification of new small-molecule kinase inhibitors in days.},
author = {Zhavoronkov, Alex and Ivanenkov, Yan A. and Aliper, Alex and Veselov, Mark S. and Aladinskiy, Vladimir A. and Aladinskaya, Anastasiya V. and Terentiev, Victor A. and Polykovskiy, Daniil A. and Kuznetsov, Maksim D. and Asadulaev, Arip and Volkov, Yury and Zholus, Artem and Shayakhmetov, Rim R. and Zhebrak, Alexander and Minaeva, Lidiya I. and Zagribelnyy, Bogdan A. and Lee, Lennart H. and Soll, Richard and Madge, David and Xing, Li and Guo, Tao and Aspuru-Guzik, Al{\'{a}}n},
doi = {10.1038/s41587-019-0224-x},
issn = {1087-0156},
journal = {Nat. Biotechnol.},
keywords = {Cheminformatics,Computational chemistry,Drug discovery and development,Virtual drug screening},
month = {sep},
number = {9},
pages = {1038--1040},
publisher = {Nature Publishing Group},
title = {{Deep learning enables rapid identification of potent DDR1 kinase inhibitors}},
url = {http://www.nature.com/articles/s41587-019-0224-x},
volume = {37},
year = {2019}
}
@misc{Wins,
author = {Winston, Patrick Henry.},
title = {{Getting Grades out of the Way}},
url = {http://web.mit.edu/fnl/volume/204/winston.html},
urldate = {2019-12-29}
}
@inproceedings{Olson2016,
abstract = {Over the past decade, data science and machine learning has grown from a mysterious art form to a staple tool across a variety of fields in academia, business, and government. In this paper, we introduce the concept of tree-based pipeline optimization for automating one of the most tedious parts of machine learning---pipeline design. We implement a Tree-based Pipeline Optimization Tool (TPOT) and demonstrate its effectiveness on a series of simulated and real-world genetic data sets. In particular, we show that TPOT can build machine learning pipelines that achieve competitive classification accuracy and discover novel pipeline operators---such as synthetic feature constructors---that significantly improve classification accuracy on these data sets. We also highlight the current challenges to pipeline optimization, such as the tendency to produce pipelines that overfit the data, and suggest future research paths to overcome these challenges. As such, this work represents an early step toward fully automating machine learning pipeline design.},
archivePrefix = {arXiv},
arxivId = {1601.07925},
author = {Olson, Randal S. and Urbanowicz, Ryan J. and Andrews, Peter C. and Lavender, Nicole A. and Kidd, La Creis and Moore, Jason H.},
booktitle = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
doi = {10.1007/978-3-319-31204-0_9},
eprint = {1601.07925},
isbn = {9783319312033},
issn = {16113349},
keywords = {Data science,Genetic programming,Hyperparameter optimization,Machine learning,Pipeline optimization},
month = {mar},
pages = {123--137},
publisher = {Springer, Cham},
title = {{Automating biomedical data science through tree-based pipeline optimization}},
url = {http://link.springer.com/10.1007/978-3-319-31204-0{\_}9},
volume = {9597},
year = {2016}
}
@misc{tensorflow_probability,
title = {{TensorFlow Probability}},
url = {https://www.tensorflow.org/probability},
urldate = {2019-10-11}
}
@article{Browaeys2019,
abstract = {Computational methods that model how gene expression of a cell is influenced by interacting cells are lacking. We present NicheNet (https://github.com/saeyslab/nichenetr), a method that predicts ligand–target links between interacting cells by combining their expression data with prior knowledge on signaling and gene regulatory networks. We applied NicheNet to tumor and immune cell microenvironment data and demonstrate that NicheNet can infer active ligands and their gene regulatory effects on interacting cells.},
author = {Browaeys, Robin and Saelens, Wouter and Saeys, Yvan},
doi = {10.1038/s41592-019-0667-5},
issn = {15487105},
journal = {Nat. Methods},
pmid = {31819264},
publisher = {Nature Research},
title = {{NicheNet: modeling intercellular communication by linking ligands to target genes}},
year = {2019}
}
@misc{rstanarm_software,
author = {Goodrich, Ben and Gabry, Jonah and Ali, Imad and Brilleman, Sam},
title = {{rstanarm: Bayesian applied regression modeling via Stan}},
url = {ttp://mc-stan.org/},
year = {2018}
}
@article{Fernandes2018,
abstract = {{\textcopyright} 2018 The Author(s). Research into suicide prevention has been hampered by methodological limitations such as low sample size and recall bias. Recently, Natural Language Processing (NLP) strategies have been used with Electronic Health Records to increase information extraction from free text notes as well as structured fields concerning suicidality and this allows access to much larger cohorts than previously possible. This paper presents two novel NLP approaches-a rule-based approach to classify the presence of suicide ideation and a hybrid machine learning and rule-based approach to identify suicide attempts in a psychiatric clinical database. Good performance of the two classifiers in the evaluation study suggest they can be used to accurately detect mentions of suicide ideation and attempt within free-text documents in this psychiatric database. The novelty of the two approaches lies in the malleability of each classifier if a need to refine performance, or meet alternate classification requirements arises. The algorithms can also be adapted to fit infrastructures of other clinical datasets given sufficient clinical recording practice knowledge, without dependency on medical codes or additional data extraction of known risk factors to predict suicidal behaviour.},
author = {Fernandes, Andrea C. and Dutta, Rina and Velupillai, Sumithra and Sanyal, Jyoti and Stewart, Robert and Chandran, David},
doi = {10.1038/s41598-018-25773-2},
issn = {20452322},
journal = {Sci. Rep.},
keywords = {Epidemiology,Risk factors},
month = {dec},
number = {1},
pages = {7426},
publisher = {Nature Publishing Group},
title = {{Identifying Suicide Ideation and Suicidal Attempts in a Psychiatric Clinical Research Database using Natural Language Processing}},
url = {http://www.nature.com/articles/s41598-018-25773-2},
volume = {8},
year = {2018}
}
@article{Banerjee2018d,
abstract = {The thymus is the primary organ for the generation of naive T cells, a key component of the immune system. Tolerance of T cells to self is achieved primarily in the thymic medulla, where immature T cells (thymocytes) sample self-peptides presented by medullary thymic epithelial cells (mTECs). A sufficiently strong interaction activates the thymocytes leading to negative selection. A key question of current interest is whether there is any structure in the manner in which mTECs present peptides: can any mTEC present any peptide at any time, or are there particular patterns of correlated peptide presentation? We investigate this question using a mathematical model of negative selection. We find that correlated patterns of peptide presentation may be advantageous in negatively selecting low-degeneracy thymocytes (that is, those thymocytes which respond to relatively few peptides). We also quantify the probability that an auto-reactive thymocyte exits the thymus before it encounters a cognate antigen. The results suggest that heterogeneity of gene co-expression in mTECs has an effect on the probability of escape of autoreactive thymocytes.},
author = {Banerjee, Soumya and Chapman, S. Jonathan},
doi = {10.1098/rsif.2018.0311},
issn = {1742-5689},
journal = {J. R. Soc. Interface},
month = {nov},
number = {148},
pages = {20180311},
publisher = {The Royal Society},
title = {{Influence of correlated antigen presentation on T-cell negative selection in the thymus}},
url = {http://rsif.royalsocietypublishing.org/lookup/doi/10.1098/rsif.2018.0311},
volume = {15},
year = {2018}
}
@article{Wells2012,
abstract = {that used data sources very diff erent from ours. At the National Confi dential Inquiry, we collect individual-level suicide data from clinicians as well as data from providers of mental health services. We make every eff ort to make our data available through reports and regular updates on our website. This is in aggregated and anonymised form rather than raw data because of the sensitive nature of the information, the need to protect individual identities, and our data-sharing agreements with National Health Service organisations. The purpose of crisis-resolution teams is to provide an alternative to psychiatric inpatient treatment. By providing care to at-risk individuals who would formerly have been admit-ted, such teams might contribute to a less pressured inpatient environment. In this context it is not diffi cult to see how implementation of these services would contribute to a fall in inpatient suicide. This is the focus of some of our ongoing research.},
author = {Wells, J Elisabeth and Cross, Nick B and Richardson, Ann K},
doi = {10.1016/S0140-6736(12)61012-5},
issn = {1474547X},
journal = {Lancet},
month = {jun},
number = {9834},
pages = {2338},
pmid = {22726509},
publisher = {Elsevier},
title = {{Toxicity profile of lithium}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22726509},
volume = {379},
year = {2012}
}
@article{Gutierrez-Sacristan2018,
abstract = {Motivation: Ribosomal RNA profiling has become crucial to studying microbial communities, but meaningful taxonomic analysis and inter-comparison of such data are still hampered by technical limitations, between-study design variability and inconsistencies between taxonomies used. Results: Here we present MAPseq, a framework for reference-based rRNA sequence analysis that is up to 30{\%} more accurate (F1/2 score) and up to one hundred times faster than existing solutions, providing in a single run multiple taxonomy classifications and hierarchical OTU mappings, for rRNA sequences in both amplicon and shotgun sequencing strategies, and for datasets of virtually any size. Availability: Source code and binaries are freely available at https://github.com/jfmrod/mapseq Contact:},
author = {Guti{\'{e}}rrez-Sacrist{\'{a}}n, Alba and Bravo, {\`{A}}lex and Giannoula, Alexia and Mayer, Miguel A and Sanz, Ferran and Furlong, Laura I},
doi = {10.1093/bioinformatics/bty315},
editor = {Kelso, Janet},
issn = {14602059},
journal = {Bioinformatics},
month = {sep},
number = {18},
pages = {3228--3230},
publisher = {Narnia},
title = {{comoRbidity: An R package for the systematic analysis of disease comorbidities}},
url = {https://academic.oup.com/bioinformatics/article/34/18/3228/4979545},
volume = {34},
year = {2018}
}
@article{Ding2017,
abstract = {INTRODUCTION Chronic heart failure (CHF) is a life-threatening chronic disease characterised by periodic exacerbations and recurrent hospitalisations. In the management of CHF, patient compliance with evidence-based clinical guidelines is essential, but remains difficult practically. The objective of this study is to examine whether an Innovative Telemonitoring Enhanced Care Programme for CHF (ITEC-CHF) improves patients' compliance, and associated health and economic outcomes. METHODS AND ANALYSIS An open multicentre randomised controlled trial has been designed. Patients will be recruited and randomised to receive either ITEC-CHF (n=150) or usual care CHF (n=150) for at least 6 months. ITEC-CHF combines usual care and an additional telemonitoring service including remote weight monitoring, structured telephone support and nurse-led collaborative care. The primary outcomes are the compliance rates with the best-practice guidelines for daily weight monitoring. The secondary outcomes include the compliance with other guideline recommendations (health maintenance, medication, diet and exercise), health (health-related quality of life, risk factors, functional capacity and psychological states) and economic outcomes related to the use of healthcare resources such as hospital readmissions and general practitioner/emergency department visits. ETHICS AND DISSEMINATION The clinical trial has been approved by Peninsula Health Human Research Ethics Committee (HREC Reference: HREC/14/PH/27), Royal Perth Hospital Human Research Ethics Committee (Reference: 15-081) and the Curtin University Human Research Ethics Committee (Reference: HR 181/2014). We will disseminate the final results to the public via conferences and journal publications. A final study report will also be provided to the ethics committees. TRIAL REGISTRATION NUMBER Registered with Australian New Zealand Clinical Trial Registry (ACTRN12614000916640).},
author = {Ding, Hang and Jayasena, Rajiv and Maiorana, Andrew and Dowling, Alison and Chen, Sheau Huey and Karunanithi, Mohan and Layland, Jamie and Edwards, Iain},
doi = {10.1136/bmjopen-2017-017550},
isbn = {1261400091664},
issn = {20446055},
journal = {BMJ Open},
keywords = {compliance,heart failure,internet,telehealth},
month = {oct},
number = {10},
pages = {e017550},
pmid = {28993389},
publisher = {British Medical Journal Publishing Group},
title = {{Innovative Telemonitoring Enhanced Care Programme for Chronic Heart Failure (ITEC-CHF) to improve guideline compliance and collaborative care: Protocol of a multicentre randomised controlled trial}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28993389 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5640081},
volume = {7},
year = {2017}
}
@inproceedings{Clifton2012,
abstract = {Health informatics is a field in which the disci- plines of software engineering and machine learning necessarily co-exist. This discussion paper considers the interaction of software engineering and machine learning, set within the con- text of health informatics, where the scale of clinical practice requires new engineering approaches from both disciplines.We introduce applications implemented in large on-going research programmes undertaken between the Departments of Engi- neering Science and Computer Science at Oxford University, the Oxford University Hospitals NHS Trust, and the Guy's and St Thomas' NHS Foundation Trust, London.},
author = {Clifton, David A. and Gibbons, Jeremy and Davies, Jim and Tarassenko, Lionel},
booktitle = {2012 First Int. Work. Realiz. AI Synerg. Softw. Eng.},
doi = {10.1109/RAISE.2012.6227968},
isbn = {978-1-4673-1753-5},
issn = {978-1-4673-1753-5},
pages = {37--41},
publisher = {IEEE Press},
title = {{Machine learning and software engineering in health informatics}},
url = {https://dl.acm.org/citation.cfm?id=2666535 http://ieeexplore.ieee.org/document/6227968/},
year = {2012}
}
@article{Hohman2018,
abstract = {Deep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance. Standardized toolkits for building neural networks have helped democratize deep learning; visual analytics systems have now been developed to support model explanation, interpretation, debugging, and improvement. We present a survey of the role of visual analytics in deep learning research, which highlights its short yet impactful history and thoroughly summarizes the state-of-the-art using a human-centered interrogative framework, focusing on the Five W's and How (Why, Who, What, How, When, and Where). We conclude by highlighting research directions and open research problems. This survey helps researchers and practitioners in both visual analytics and deep learning to quickly learn key aspects of this young and rapidly growing body of research, whose impact spans a diverse range of domains.},
archivePrefix = {arXiv},
arxivId = {1801.06889},
author = {Hohman, Fred Matthew and Kahng, Minsuk and Pienta, Robert and Chau, Duen Horng},
doi = {10.1109/TVCG.2018.2843369},
eprint = {1801.06889},
isbn = {10772626 (ISSN)},
issn = {10772626},
journal = {IEEE Trans. Vis. Comput. Graph.},
keywords = {Deep learning,information visualization,neural networks,visual analytics},
pages = {1--1},
title = {{Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers}},
url = {https://ieeexplore.ieee.org/document/8371286/},
year = {2018}
}
@article{Rizzo2017,
abstract = {The thyroid axis is particularly prone to interactions with a wide variety of drugs, whose list increases year by year. Hypothyroidism is the most frequent consequence of drug-induced thyroid dysfunction. The main mechanisms involved in the development of primary hypothyroidism are: inhibition of the synthesis and/or release of thyroid hormones, immune mechanisms related to the use of interferon and other cytokines, and the induction of thyroiditis associated with the use of tyrosine kinase inhibitors and drugs blocking the receptors for vascular endothelial growth factor. Central hypothyroidism may be induced by inhibition of thyroid-stimulating hormone (bexarotene or corticosteroids) or by immunological mechanisms (anti-CTLA4 or anti-PD-1 antibody drugs). It is also important to recognize those drugs that generate hypothyroidism by interaction in its treatment, either by reducing the absorption or by altering the transport and metabolism of levothyroxine. Thus, it is strongly recommended to evaluate thyroid function prior to the prescription of medications such as amiodarone, lithium, or interferon, and the new biological therapies that show important interaction with thyroid and endocrine function in general.},
author = {Rizzo, Leonardo F L and Mana, Daniela L and Serra, H{\'{e}}ctor A},
issn = {0025-7680},
journal = {Medicina (B. Aires).},
keywords = {drugs,hypothyroidism,thyroid},
number = {5},
pages = {394--404},
pmid = {29044016},
title = {{Drug-induced hypothyroidism.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/29044016},
volume = {77},
year = {2017}
}
@article{Marshall2018,
abstract = {Currently, there is no consistent model for visually or formally representing the architecture of AI systems. This lack of representation brings interpretability, correctness and completeness challenges in the description of existing models and systems. DIAL (The Diagrammatic AI Language) has been created with the aspiration of being an "engineering schematic" for AI Systems. It is presented here as a starting point for a community dialogue towards a common diagrammatic language for AI Systems.},
archivePrefix = {arXiv},
arxivId = {1812.11142},
author = {Marshall, Guy and Freitas, Andr{\'{e}}},
eprint = {1812.11142},
month = {dec},
title = {{The Diagrammatic AI Language (DIAL): Version 0.1}},
url = {http://arxiv.org/abs/1812.11142},
year = {2018}
}
@article{Collobert2011,
abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.},
archivePrefix = {arXiv},
arxivId = {1103.0398},
author = {Collobert, Ronan and Weston, Jason and Bottou, Leon and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
eprint = {1103.0398},
month = {mar},
title = {{Natural Language Processing (almost) from Scratch}},
url = {http://arxiv.org/abs/1103.0398},
year = {2011}
}
@article{Sweeney2015,
abstract = {The association between mental illness and poor physical health and socioeconomic outcomes has been well established. In the twenty-first century, the challenge of how mental illnesses, such as psychosis, are managed in the provision of public health services remains complex. Developing effective clinical mental health support and interventions for individuals requires a coordinated and robust mental health system supported by social as well as health policy that places a priority on addressing socioeconomic disadvantage in mental health cohorts. This paper, thus, examines the complex relationship between socioeconomic disadvantage, family/social supports, physical health, and health service utilization in a community sample of 402 participants diagnosed with psychosis. The paper utilizes quantitative data collected from the 2010 Survey of High Impact Psychosis research project conducted in a socioeconomically disadvantaged region of Adelaide, SA, Australia. Participants (42{\%} female) provided information about socioeconomic status, education, employment, physical health, contact with family and friends, and health service utilization. The paper highlights that socioeconomic disadvantage is related to increased self-reported use of emergency departments, decreased use of general practitioners for mental health reasons, higher body mass index, less family contact, and less social support. In particular, the paper explores the multifaceted relationship between socioeconomic disadvantage and poor health confronting individuals with psychosis, highlighting the complex link between socioeconomic disadvantage and poor health. It emphasizes that mental health service usage for those with higher levels of socioeconomic disadvantage differs from those experiencing lower levels of socioeconomic disadvantage. The paper also stresses that the development of health policy and practice that seeks to redress the socioeconomic and health inequalities created by this disadvantage be an important focus for mental health services. Such health policy would provide accessible treatment programs and linked pathways to illness recovery and diminish the pressure on the delivery of health services. Consequently, the development of policy and practice that seeks to redress the socioeconomic and health inequalities created by disadvantage should be an important focus for the improvement of mental health services.},
author = {Sweeney, Shaun and Air, Tracy and Zannettino, Lana and Galletly, Cherrie},
doi = {10.3389/fpubh.2015.00259},
issn = {2296-2565 (Print)},
journal = {Front. Public Heal.},
keywords = {health,health service delivery,poverty,psychosis,socioeconomic disadvantage},
pages = {259},
pmid = {26636059},
publisher = {Frontiers Media SA},
title = {{Psychosis, Socioeconomic Disadvantage, and Health Service Use in South Australia: Findings from the Second Australian National Survey of Psychosis}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26636059 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4653578},
volume = {3},
year = {2015}
}
@article{Banerjee2018c,
abstract = {The confluence of massive amounts of openly available data, sophisticated machine learning algorithms and an enlightened citizenry willing to engage in data science presents novel opportunities for crowd sourced data science for social good. In this submission, I present vignettes of data science projects that I have been involved in and which have impact in various spheres of life and on social good.},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.07313},
author = {Banerjee, Soumya},
doi = {10.13140/RG.2.1.1846.6002},
eprint = {arXiv:1509.07313},
issn = {1334-4676},
journal = {Interdiscip. Descr. Complex Syst.},
keywords = {data science,social good},
number = {1},
pages = {88--91},
title = {{Citizen Data Science for Social Good: Case Studies and Vignettes from Recent Projects}},
url = {http://indecs.eu/index.php?s=x{\&}y=2018{\&}p=88-91 https://www.researchgate.net/publication/283119113{\_}Citizen{\_}Data{\_}Science{\_}for{\_}Social{\_}Good{\_}Case{\_}Studies{\_}and{\_}Vignettes{\_}from{\_}Recent{\_}Projects},
volume = {16},
year = {2018}
}
@article{Nazabal2018,
abstract = {Variational autoencoders (VAEs), as well as other generative models, have been shown to be efficient and accurate to capture the latent structure of vast amounts of complex high-dimensional data. However, existing VAEs can still not directly handle data that are heterogenous (mixed continuous and discrete) or incomplete (with missing data at random), which is indeed common in real-world applications. In this paper, we propose a general framework to design VAEs, suitable for fitting incomplete heterogenous data. The proposed HI-VAE includes likelihood models for real-valued, positive real valued, interval, categorical, ordinal and count data, and allows to estimate (and potentially impute) missing data accurately. Furthermore, HI-VAE presents competitive predictive performance in supervised tasks, outperforming super- vised models when trained on incomplete data},
archivePrefix = {arXiv},
arxivId = {1807.03653},
author = {Nazabal, Alfredo and Olmos, Pablo M. and Ghahramani, Zoubin and Valera, Isabel},
eprint = {1807.03653},
month = {jul},
title = {{Handling Incomplete Heterogeneous Data using VAEs}},
url = {http://arxiv.org/abs/1807.03653},
year = {2018}
}
@inproceedings{Mashayekhi2015,
abstract = {{\textcopyright} Springer International Publishing Switzerland 2015. Random forest (RF) is a tree-based learning method, which exhibits a high ability to generalize on real data sets. Nevertheless, a possible limitation of RF is that it generates a forest consisting of many trees and rules, thus it is viewed as a black box model. In this paper, the RF+HC methods for rule extraction from RF are proposed. Once the RF is built, a hill climbing algorithm is used to search for a rule set such that it reduces the number of rules dramatically, which significantly improves comprehensibility of the underlying model built by RF. The proposed methods are evaluated on eighteen UCI and four microarray data sets. Our experimental results show that the proposed methods outperform one of the state-of-the-art methods in terms of scalability and comprehensibility while preserving the same level of accuracy.},
author = {Mashayekhi, Morteza and Gras, Robin},
booktitle = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
doi = {10.1007/978-3-319-18356-5_20},
isbn = {9783319183558},
issn = {16113349},
keywords = {Hill climbing,Random forest,Rule extraction},
pages = {223--237},
title = {{Rule extraction from random forest: The RF+HC methods}},
url = {http://link.springer.com/10.1007/978-3-319-18356-5{\_}20},
volume = {9091},
year = {2015}
}
@article{Statnikov2013,
abstract = {BACKGROUND: Recent advances in next-generation DNA sequencing enable rapid high-throughput quantitation of microbial community composition in human samples, opening up a new field of microbiomics. One of the promises of this field is linking abundances of microbial taxa to phenotypic and physiological states, which can inform development of new diagnostic, personalized medicine, and forensic modalities. Prior research has demonstrated the feasibility of applying machine learning methods to perform body site and subject classification with microbiomic data. However, it is currently unknown which classifiers perform best among the many available alternatives for classification with microbiomic data.

RESULTS: In this work, we performed a systematic comparison of 18 major classification methods, 5 feature selection methods, and 2 accuracy metrics using 8 datasets spanning 1,802 human samples and various classification tasks: body site and subject classification and diagnosis.

CONCLUSIONS: We found that random forests, support vector machines, kernel ridge regression, and Bayesian logistic regression with Laplace priors are the most effective machine learning techniques for performing accurate classification from these microbiomic data.},
author = {Statnikov, Alexander and Henaff, Mikael and Narendra, Varun and Konganti, Kranti and Li, Zhiguo and Yang, Liying and Pei, Zhiheng and Blaser, Martin J and Aliferis, Constantin F and Alekseyenko, Alexander V},
doi = {10.1186/2049-2618-1-11},
issn = {2049-2618},
journal = {Microbiome},
language = {en},
month = {jan},
number = {1},
pages = {11},
pmid = {24456583},
publisher = {BioMed Central Ltd},
title = {{A comprehensive evaluation of multicategory classification methods for microbiomic data.}},
url = {http://www.microbiomejournal.com/content/1/1/11},
volume = {1},
year = {2013}
}
@misc{Banerjee2019a,
author = {Banerjee, Soumya and Ghose, Joyeeta},
doi = {10.17605/OSF.IO/25GNZ},
keywords = {Project,complex systems,data analytics,dynamical systems,machine learning,teaching},
publisher = {OSF},
title = {{Teaching resources for data analytics in complex systems}},
url = {https://osf.io/25gnz/},
urldate = {2019-04-06},
year = {2019}
}
@article{Madero2017,
abstract = {BACKGROUND AND OBJECTIVES Although anthropometric measures of body fat are associated with development of CKD, they may not be able to distinguish between various forms of fat and therefore may be less accurate than computed tomography (CT) measures. We compared the association of CT and anthropometric measures of obesity with kidney outcomes in the Health Aging and Body Composition Study. DESIGN, SETTING, PARTICIPANTS, {\&} MEASUREMENTS Participants were recruited from March of 1997 through July of 1998. CT measures included visceral abdominal fat (VAT), subcutaneous adipose tissue (SAT), and intermuscular fat area (IMAT), whereas anthropometric measures included waist circumference (WC) and body mass index (BMI). Kidney outcomes included kidney function (KF) decline (30{\%} decrease in eGFRcysC in follow-up at either year 3 or 10) or incident CKD (follow-up eGFRcysC≤60 ml/min per 1.73 m(2) in individuals with baseline GFR{\textgreater}60 ml/min per 1.73 m(2)). Multivariable logistic regression models and Poisson regression models were used to evaluate the association with decline in KF and incident kidney disease, respectively. We also assessed for the independent associations among the exposure measures by including them in the same model. RESULTS Two-thousand four-hundred and eighty-nine individuals were included. Mean age was 74±3 years, 49{\%} were men, 39{\%} were black, 59{\%} were hypertensive, and 15{\%} were diabetic. KF decline occurred in 17{\%} of the population, whereas incident CKD also occurred in 17{\%} of those at risk. In continuous models, SAT, VAT, IMAT, BMI, and WC (per SD increase) were all significantly associated with KF decline. There was a significant interaction between VAT and CKD with regard to KF decline (P=0.01). Only VAT, BMI, and WC were associated with incident CKD. Only VAT remained a significant risk factor for incident CKD when other exposure variables were included in the same model. There was no association between any measure of obesity and kidney outcomes when creatinine values at years 3 and 10 were used to estimate changes in eGFR. CONCLUSIONS Anthropometric measures of body fat appear to provide as consistent estimates of KF decline risk as CT measures in elders.},
author = {Madero, Magdalena and Katz, Ronit and Murphy, Rachel and Newman, Anne and Patel, Kushang and Ix, Joachim and Peralta, Carmen and Satterfield, Suzanne and Fried, Linda and Shlipak, Michael and Sarnak, Mark},
doi = {10.2215/CJN.07010716},
issn = {1555905X},
journal = {Clin. J. Am. Soc. Nephrol.},
keywords = {Abdominal,Aged,Body Composition,Body Mass Index,Chronic,Follow-Up Studies,Humans,Intra-Abdominal Fat,Logistic Models,Male,Renal Insufficiency,Subcutaneous Fat,Tomography,Waist Circumference,X-Ray Computed,chronic kidney disease,diabetes mellitus,obesity,renal function decline,risk factors},
month = {jun},
number = {6},
pages = {893--903},
pmid = {28522656},
publisher = {American Society of Nephrology},
title = {{Comparison between different measures of body fat with kidney function decline and incident CKD}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28522656 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5460706},
volume = {12},
year = {2017}
}
@article{DeLusignan2019,
abstract = {Background: The European Medicines Agency (EMA) requires vaccine manufacturers to conduct enhanced real-time surveillance of seasonal influenza vaccination. The EMA has specified a list of adverse events of interest to be monitored. The EMA sets out 3 different ways to conduct such surveillance: (1) active surveillance, (2) enhanced passive surveillance, or (3) electronic health record data mining (EHR-DM). English general practice (GP) is a suitable setting to implement enhanced passive surveillance and EHR-DM. Objective: This study aimed to test the feasibility of conducting enhanced passive surveillance in GP using the yellow card scheme (adverse events of interest reporting cards) to determine if it has any advantages over EHR-DM alone. Methods: A total of 9 GPs in England participated, of which 3 tested the feasibility of enhanced passive surveillance and the other 6 EHR-DM alone. The 3 that tested EPS provided patients with yellow (adverse events) cards for patients to report any adverse events. Data were extracted from all 9 GPs' EHRs between weeks 35 and 49 (08/24/2015 to 12/06/2015), the main period of influenza vaccination. We conducted weekly analysis and end-of-study analyses. Results: Our GPs were largely distributed across England with a registered population of 81,040. In the week 49 report, 15,863/81,040 people (19.57{\%} of the registered practice population) were vaccinated. In the EPS practices, staff managed to hand out the cards to 61.25{\%} (4150/6776) of the vaccinees, and of these cards, 1.98{\%} (82/4150) were returned to the GP offices. Adverse events of interests were reported by 113 /7223 people (1.56{\%}) in the enhanced passive surveillance practices, compared with 322/8640 people (3.73{\%}) in the EHR-DM practices. Conclusions: Overall, we demonstrated that GPs EHR-DM was an appropriate method of enhanced surveillance. However, the use of yellow cards, in enhanced passive surveillance practices, did not enhance the collection of adverse events of interests as demonstrated in this study. Their return rate was poor, data entry from them was not straightforward, and there were issues with data reconciliation. We concluded that customized cards prespecifying the EMA's adverse events of interests, combined with EHR-DM, were needed to maximize data collection.},
author = {{De Lusignan}, Simon and Correa, Ana and {Dos Santos}, Ga{\"{e}}l and Meyer, Nadia and Haguinet, Fran{\c{c}}ois and Webb, Rebecca and McGee, Christopher and Byford, Rachel and Yonova, Ivelina and Pathirannehelage, Sameera and Ferreira, Filipa Matos and Jones, Simon},
doi = {10.2196/12016},
issn = {14388871},
journal = {J. Med. Internet Res.},
keywords = {Computerized,Drug-related side effects and adverse reactions,England,General practice,Human,Influenza,Influenza vaccines,Medical records systems,Safety management,Vaccines},
month = {nov},
number = {11},
pages = {e12016},
pmid = {31724955},
publisher = {Journal of Medical Internet Research},
title = {{Enhanced safety surveillance of influenza vaccines in general practice, winter 2015-16: Feasibility study}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/31724955 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC6913774},
volume = {21},
year = {2019}
}
@inproceedings{Abadi2016,
abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
archivePrefix = {arXiv},
arxivId = {1605.08695},
author = {Abadi, Mart{\'{i}}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {1605.08695},
month = {may},
title = {{TensorFlow: A system for large-scale machine learning}},
url = {http://arxiv.org/abs/1605.08695},
year = {2016}
}
@article{Kaminsky2019,
abstract = {Simulation studies are often used to predict the expected impact of control measures in infectious disease outbreaks. Typically, two independent sets of simulations are conducted, one with the intervention, and one without, and epidemic sizes (or some related metric) are compared to estimate the effect of the intervention. Since it is possible that controlled epidemics are larger than uncontrolled ones if there is substantial stochastic variation between epidemics, uncertainty intervals from this approach can include a negative effect even for an effective intervention. To more precisely estimate the number of cases an intervention will prevent within a single epidemic, here we develop a ‘single-world' approach to matching simulations of controlled epidemics to their exact uncontrolled counterfactual. Our method borrows concepts from percolation approaches, prunes out possible epidemic histories and creates potential epidemic graphs (i.e. a mathematical representation of all consistent epidemics) that can...},
author = {Kaminsky, Joshua and Keegan, Lindsay T. and Metcalf, C. Jessica E. and Lessler, Justin},
doi = {10.1098/rstb.2018.0279},
issn = {0962-8436},
journal = {Philos. Trans. R. Soc. B Biol. Sci.},
keywords = {counterfactuals,infectious disease dynamics,infectious disease epidemiology,network models},
month = {jul},
number = {1776},
pages = {20180279},
publisher = {The Royal Society},
title = {{Perfect counterfactuals for epidemic simulations}},
url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2018.0279},
volume = {374},
year = {2019}
}
@article{Baldwin2018,
abstract = {Objective Childhood victimization is an important risk factor for later immune-related disorders. Previous evidence has demonstrated that childhood victimization is associated with elevated levels of inflammation biomarkers measured decades after exposure. However, it is unclear whether this association is (1) already detectable in young people, (2) different in males and females, and (3) confounded by genetic liability to inflammation. Here we sought to address these questions. Method Participants were 2232 children followed from birth to age 18 years as part of the Environmental Risk (E-Risk) Longitudinal Twin Study. Childhood victimization was measured prospectively from birth to age 12 years. Inflammation was measured through C-reactive protein (CRP) levels in dried blood spots at age 18 years. Latent genetic liability for high inflammation levels was assessed through a twin-based method. Results Greater exposure to childhood victimization was associated with higher CRP levels at age 18 (serum-equivalent means were 0.65 in non-victimized Study members, 0.74 in those exposed to one victimization type, and 0.81 in those exposed to poly-victimization; p = 0.018). However, this association was driven by a significant association in females (serum-equivalent means were 0.75 in non-victimized females, 0.87 in those exposed to one type of victimization, and 1.19 in those exposed to poly-victimization; p = 0.010), while no significant association was observed in males (p = 0.19). Victimized females showed elevated CRP levels independent of latent genetic influence, as well as childhood socioeconomic status, and waist-hip ratio and body temperature at the time of CRP assessment. Conclusion Childhood victimization is associated with elevated CRP levels in young women, independent of latent genetic influences and other key risk factors. These results strengthen causal inference about the effects of childhood victimization on inflammation levels in females by accounting for potential genetic confounding.},
author = {Baldwin, Jessie R. and Arseneault, Louise and Caspi, Avshalom and Fisher, Helen L. and Moffitt, Terrie E. and Odgers, Candice L. and Pariante, Carmine and Ambler, Antony and Dove, Rosamund and Kepa, Agnieszka and Matthews, Timothy and Menard, Anne and Sugden, Karen and Williams, Benjamin and Danese, Andrea},
doi = {10.1016/j.bbi.2017.08.025},
issn = {10902139},
journal = {Brain. Behav. Immun.},
keywords = {Bullying victimization,C-reactive protein,Childhood maltreatment,Childhood victimization,Early life stress,Inflammation,Sex differences},
month = {jan},
pages = {211--217},
publisher = {Academic Press},
title = {{Childhood victimization and inflammation in young adulthood: A genetically sensitive cohort study}},
url = {https://www.sciencedirect.com/science/article/pii/S0889159117304075?via{\%}3Dihub},
volume = {67},
year = {2018}
}
@article{Nestor2018,
abstract = {Machine learning for healthcare often trains models on de-identified datasets with randomly-shifted calendar dates, ignoring the fact that data were generated under hospital operation practices that change over time. These changing practices induce definitive changes in observed data which confound evaluations which do not account for dates and limit the generalisability of date-agnostic models. In this work, we establish the magnitude of this problem on MIMIC, a public hospital dataset, and showcase a simple solution. We augment MIMIC with the year in which care was provided and show that a model trained using standard feature representations will significantly degrade in quality over time. We find a deterioration of 0.3 AUC when evaluating mortality prediction on data from 10 years later. We find a similar deterioration of 0.15 AUC for length-of-stay. In contrast, we demonstrate that clinically-oriented aggregates of raw features significantly mitigate future deterioration. Our suggested aggregated representations, when retrained yearly, have prediction quality comparable to year-agnostic models.},
archivePrefix = {arXiv},
arxivId = {1811.12583},
author = {Nestor, Bret and McDermott, Matthew B. A. and Chauhan, Geeticka and Naumann, Tristan and Hughes, Michael C. and Goldenberg, Anna and Ghassemi, Marzyeh},
eprint = {1811.12583},
month = {nov},
title = {{Rethinking clinical prediction: Why machine learning must consider year of care and feature aggregation}},
url = {http://arxiv.org/abs/1811.12583},
year = {2018}
}
@article{Lage2018,
abstract = {We often desire our models to be interpretable as well as accurate. Prior work on optimizing models for interpretability has relied on easy-to-quantify proxies for interpretability, such as sparsity or the number of operations required. In this work, we optimize for interpretability by directly including humans in the optimization loop. We develop an algorithm that minimizes the number of user studies to find models that are both predictive and interpretable and demonstrate our approach on several data sets. Our human subjects results show trends towards different proxy notions of interpretability on different datasets, which suggests that different proxies are preferred on different tasks.},
archivePrefix = {arXiv},
arxivId = {1805.11571},
author = {Lage, Isaac and Ross, Andrew Slavin and Kim, Been and Gershman, Samuel J. and Doshi-Velez, Finale},
eprint = {1805.11571},
month = {may},
title = {{Human-in-the-Loop Interpretability Prior}},
url = {http://arxiv.org/abs/1805.11571},
year = {2018}
}
@article{Hussein,
author = {Hussein, Ali},
doi = {10.31730/OSF.IO/9PCMN},
keywords = {Computational Engineering,Computer Engineering,Engineering,Other Computer Engineering,Other Engineering,graph embeddings,node2vec,representation learning,word2vec},
publisher = {AfricArXiv},
title = {{POSTER| Address2vec: Generating Vector Embeddings for Blockchain Analytics}},
url = {https://osf.io/preprints/africarxiv/9pcmn/}
}
@article{Brodeur2017,
abstract = {We introduce HoME: a Household Multimodal Environment for artificial agents to learn from vision, audio, semantics, physics, and interaction with objects and other agents, all within a realistic context. HoME integrates over 45,000 diverse 3D house layouts based on the SUNCG dataset, a scale which may facilitate learning, generalization, and transfer. HoME is an open-source, OpenAI Gym-compatible platform extensible to tasks in reinforcement learning, language grounding, sound-based navigation, robotics, multi-agent learning, and more. We hope HoME better enables artificial agents to learn as humans do: in an interactive, multimodal, and richly contextualized setting.},
archivePrefix = {arXiv},
arxivId = {1711.11017},
author = {Brodeur, Simon and Perez, Ethan and Anand, Ankesh and Golemo, Florian and Celotti, Luca and Strub, Florian and Rouat, Jean and Larochelle, Hugo and Courville, Aaron},
doi = {10.1080/03067310903094529},
eprint = {1711.11017},
issn = {0306-7319},
month = {nov},
title = {{HoME: a Household Multimodal Environment}},
url = {http://arxiv.org/abs/1711.11017},
year = {2017}
}
@article{Riza2015,
abstract = {Fuzzy rule-based systems (FRBSs) are a well-known method family within soft computing. They are based on fuzzy concepts to address complex real-world problems. We present the R package frbs which implements the most widely used FRBS models, namely, Mamdani and Takagi Sugeno Kang (TSK) ones, as well as some common variants. In addition a host of learning methods for FRBSs, where the models are constructed from data, are implemented. In this way, accurate and interpretable systems can be built for data analysis and modeling tasks. In this paper, we also provide some examples on the usage of the package and a comparison with other common classification and regression methods available in R. {\textcopyright} 2015, American Statistical Association. All rights reserved.},
author = {Riza, Lala Septem and Bergmeir, Christoph and Herrera, Francisco and Ben{\'{i}}tez, Jos{\'{e}} Manuel},
doi = {10.18637/jss.v065.i06},
issn = {1548-7660},
journal = {J. Stat. Softw.},
month = {jun},
number = {6},
pages = {1--30},
title = {{frbs : Fuzzy Rule-Based Systems for Classification and Regression in R}},
url = {http://www.jstatsoft.org/v65/i06/},
volume = {65},
year = {2015}
}
@article{Banerjeeb,
author = {soumya Banerjee},
doi = {10.31219/OSF.IO/UJYDR},
keywords = {Applied Mathematics,Artificial Intelligence and Robotics,Computer Sciences,Education,Other Education,Physical Sciences and Mathematics},
publisher = {OSF Preprints},
title = {{Derivations and theory for Bayesian linear regression}},
url = {https://osf.io/ujydr/}
}
@book{Morgan2014,
abstract = {In this second edition of Counterfactuals and Causal Inference, completely revised and expanded, the essential features of the counterfactual approach to observational data analysis are presented with examples from the social, demographic, and health sciences. Alternative estimation techniques are first introduced using both the potential outcome model and causal graphs; after which, conditioning techniques, such as matching and regression, are presented from a potential outcomes perspective. For research scenarios in which important determinants of causal exposure are unobserved, alternative techniques, such as instrumental variable estimators, longitudinal methods, and estimation via causal mechanisms, are then presented. The importance of causal effect heterogeneity is stressed throughout the book, and the need for deep causal explanation via mechanisms is discussed.},
author = {Morgan, Stephen L. and Winship, Christopher},
booktitle = {Counterfactuals and Causal Inference},
doi = {10.1017/cbo9781107587991},
title = {{Counterfactuals and Causal Inference}},
year = {2014}
}
@article{Kemp2008,
abstract = {Algorithms for finding structure in data have become increasingly important both as tools for scientific data analysis and as models of human learning, yet they suffer from a critical limitation. Scientists discover qualitatively new forms of structure in observed data: For instance, Linnaeus recognized the hierarchical organization of biological species, and Mendeleev recognized the periodic structure of the chemical elements. Analogous insights play a pivotal role in cognitive development: Children discover that object category labels can be organized into hierarchies, friendship networks are organized into cliques, and comparative relations (e.g., "bigger than" or "better than") respect a transitive order. Standard algorithms, however, can only learn structures of a single form that must be specified in advance: For instance, algorithms for hierarchical clustering create tree structures, whereas algorithms for dimensionality-reduction create low-dimensional spaces. Here, we present a computational model that learns structures of many different forms and that discovers which form is best for a given dataset. The model makes probabilistic inferences over a space of graph grammars representing trees, linear orders, multidimensional spaces, rings, dominance hierarchies, cliques, and other forms and successfully discovers the underlying structure of a variety of physical, biological, and social domains. Our approach brings structure learning methods closer to human abilities and may lead to a deeper computational understanding of cognitive development. {\textcopyright} 2008 by The National Academy of Sciences of the USA.},
author = {Kemp, Charles and Tenenbaum, Joshua B.},
doi = {10.1073/pnas.0802631105},
issn = {00278424},
journal = {Proc. Natl. Acad. Sci. U. S. A.},
keywords = {Cognitive development,Structure discovery,Unsupervised learning},
month = {aug},
number = {31},
pages = {10687--10692},
publisher = {National Academy of Sciences},
title = {{The discovery of structural form}},
volume = {105},
year = {2008}
}
@article{Barnett2012,
abstract = {Background Long-term disorders are the main challenge facing health-care systems worldwide, but health systems are largely configured for individual diseases rather than multimorbidity. We examined the distribution of multimorbidity, and of comorbidity of physical and mental health disorders, in relation to age and socioeconomic deprivation. Methods In a cross-sectional study we extracted data on 40 morbidities from a database of 1 751 841 people registered with 314 medical practices in Scotland as of March, 2007. We analysed the data according to the number of morbidities, disorder type (physical or mental), sex, age, and socioeconomic status. We defined multimorbidity as the presence of two or more disorders. Findings 42.2{\%} (95{\%} CI 42.1-42.3) of all patients had one or more morbidities, and 23.2{\%} (23.08-23.21) were multimorbid. Although the prevalence of multimorbidity increased substantially with age and was present in most people aged 65 years and older, the absolute number of people with multimorbidity was higher in those younger than 65 years (210 500 vs 194 996). Onset of multimorbidity occurred 10-15 years earlier in people living in the most deprived areas compared with the most affluent, with socioeconomic deprivation particularly associated with multimorbidity that included mental health disorders (prevalence of both physical and mental health disorder 11.0{\%}, 95{\%} CI 10.9-11.2{\%} in most deprived area vs 5.9{\%}, 5.8{\%}-6.0{\%} in least deprived). The presence of a mental health disorder increased as the number of physical morbidities increased (adjusted odds ratio 6.74, 95{\%} CI 6.59-6.90 for five or more disorders vs 1.95, 1.93-1.98 for one disorder), and was much greater in more deprived than in less deprived people (2.28, 2.21-2.32 vs 1.08, 1.05-1.11). Interpretation Our findings challenge the single-disease framework by which most health care, medical research, and medical education is configured. A complementary strategy is needed, supporting generalist clinicians to provide personalised, comprehensive continuity of care, especially in socioeconomically deprived areas. Funding Scottish Government Chief Scientist Office.},
author = {Barnett, Karen and Mercer, Stewart W. and Norbury, Michael and Watt, Graham and Wyke, Sally and Guthrie, Bruce},
doi = {10.1016/S0140-6736(12)60240-2},
issn = {1474547X},
journal = {Lancet},
number = {9836},
pages = {37--43},
pmid = {22579043},
publisher = {Lancet Publishing Group},
title = {{Epidemiology of multimorbidity and implications for health care, research, and medical education: A cross-sectional study}},
volume = {380},
year = {2012}
}
@inproceedings{Tyshetskiy2016,
abstract = {Modelling and forecasting port throughput enables stakeholders to make efficient decisions ranging from management of port development, to infrastructure investments, operational restructuring and tariffs policy. Accurate forecasting of port throughput is also critical for long-term resource allocation and short-term strategic planning. In turn, efficient decision- making enhances the competitiveness of a port. However, in the era of big data we are faced with the enviable dilemma of having too much information. We pose the question: is more information always better for forecasting? We suggest that more information comes at the cost of more parameters of the forecasting model that need to be estimated. We compare multiple forecasting models of varying degrees of complexity and quantify the effect of the amount of data on model forecasting accuracy. Our methodology serves as a guideline for practitioners in this field. We also enjoin caution that even in the era of big data more information may not always be better. It would be advisable for analysts to weigh the costs of adding more data: the ultimate decision would depend on the problem, amount of data and the kind of models being used.},
address = {Hamburg, Germany},
author = {Tyshetskiy, Yuriy and Banerjee, Soumya and Mathews, George and Vitsounis, Thomas},
booktitle = {Annu. Conf. Int. Assoc. Marit. Econ.},
file = {::},
keywords = {australia port throughput,big data,forecasting,machine learning},
pages = {1--22},
title = {{Forecasting Australian port throughput : Lessons and Pitfalls in the era of Big Data}},
url = {https://www.nicta.com.au/pub-download/full/9307/},
year = {2016}
}
@article{Baksi2018,
abstract = {Realization of the importance of microbiome studies, coupled with the decreasing sequencing cost, has led to the exponential growth of microbiome data. A number of these microbiome studies have focused on understanding changes in the microbial community over time. Such longitudinal microbiome studies have the potential to offer unique insights pertaining to the microbial social networks as well as their responses to perturbations. In this communication, we introduce a web based framework called ‘TIME' (Temporal Insights into Microbial Ecology'), developed specifically to obtain meaningful insights from microbiome time series data. The TIME web-server is designed to accept a wide range of popular formats as input with options to preprocess and filter the data. Multiple samples, defined by a series of longitudinal time points along with their metadata information, can be compared in order to interactively visualize the temporal variations. In addition to standard microbiome data analytics, the web server implements popular time series analysis methods like Dynamic time warping, Granger causality and Dickey Fuller test to generate interactive layouts for facilitating easy biological inferences. Apart from this, a new metric for comparing metagenomic time series data has been introduced to effectively visualize the similarities/differences in the trends of the resident microbial groups. Augmenting the visualizations with the stationarity information pertaining to the microbial groups is utilized to predict the microbial competition as well as community structure. Additionally, the ‘causality graph analysis' module incorporated in TIME allows predicting taxa that might have a higher influence on community structure in different conditions. TIME also allows users to easily identify potential taxonomic markers from a longitudinal microbiome analysis. We illustrate the utility of the web-server features on a few published time series microbiome data and demonstrate the ease with which it can be used to perform complex analysis. Availability: https://web.rniapps.net/time},
author = {Baksi, Krishanu D and Kuntal, Bhusan K and Mande, Sharmila S},
doi = {10.3389/fmicb.2018.00036},
issn = {1664302X},
journal = {Front. Microbiol.},
keywords = {Clustering,Community state,Granger causality algorithm,Microbiome,Time series,Visualization,Web server},
number = {JAN},
pages = {36},
pmid = {29416530},
publisher = {Frontiers Media SA},
title = {{'TIME': A web application for obtaining Insights into Microbial Ecology using longitudinal microbiome data}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/29416530 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5787560},
volume = {9},
year = {2018}
}
@article{Ma2011,
abstract = {In analysis of bioinformatics data, a unique challenge arises from the high dimensionality of measurements. Without loss of generality, we use genomic study with gene expression measurements as a representative example but note that analysis techniques discussed in this article are also applicable to other types of bioinformatics studies. Principal component analysis (PCA) is a classic dimension reduction approach. It constructs linear combinations of gene expressions, called principal components (PCs). The PCs are orthogonal to each other, can effectively explain variation of gene expressions, and may have a much lower dimensionality. PCA is computationally simple and can be realized using many existing software packages. This article consists of the following parts. First, we review the standard PCA technique and their applications in bioinformatics data analysis. Second, we describe recent 'non-standard' applications of PCA, including accommodating interactions among genes, pathways and network modules and conducting PCA with estimating equations as opposed to gene expressions. Third, we introduce several recently proposed PCA-based techniques, including the supervised PCA, sparse PCA and functional PCA. The supervised PCA and sparse PCA have been shown to have better empirical performance than the standard PCA. The functional PCA can analyze time-course gene expression data. Last, we raise the awareness of several critical but unsolved problems related to PCA. The goal of this article is to make bioinformatics researchers aware of the PCA technique and more importantly its most recent development, so that this simple yet effective dimension reduction technique can be better employed in bioinformatics data analysis.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Ma, Shuangge and Dai, Ying},
doi = {10.1093/bib/bbq090},
eprint = {9605103},
isbn = {1467-5463},
issn = {14675463},
journal = {Brief. Bioinform.},
keywords = {Bioinformatics methodologies,Dimension reduction,Gene expression,Principal component analysis},
month = {nov},
number = {6},
pages = {714--722},
pmid = {21242203},
primaryClass = {cs},
publisher = {Oxford University Press},
title = {{Principal component analysis based Methods in bioinformatics studies}},
url = {https://academic.oup.com/bib/article-lookup/doi/10.1093/bib/bbq090},
volume = {12},
year = {2011}
}
@misc{Petar,
author = {Veli{\v{c}}kovi{\'{c}}, Petar},
title = {{The resurgence of structure in deep neural networks}},
url = {https://www.repository.cam.ac.uk/handle/1810/292230},
urldate = {2019-05-11}
}
@misc{gae_github,
title = {{tkipf/gae: Implementation of Graph Auto-Encoders in TensorFlow}},
url = {https://github.com/tkipf/gae},
urldate = {2019-11-29}
}
@article{Velickovic2017,
abstract = {We analyse multimodal time-series data corresponding to weight, sleep and steps measurements. We focus on predicting whether a user will successfully achieve his/her weight objective. For this, we design several deep long short-term memory (LSTM) architectures, including a novel cross-modal LSTM (X-LSTM), and demonstrate their superiority over baseline approaches. The X-LSTM improves parameter efficiency by processing each modality separately and allowing for information flow between them by way of recurrent cross-connections. We present a general hyperparameter optimisation technique for X-LSTMs, which allows us to significantly improve on the LSTM and a prior state-of-the-art cross-modal approach, using a comparable number of parameters. Finally, we visualise the model's predictions, revealing implications about latent variables in this task.},
archivePrefix = {arXiv},
arxivId = {1709.08073},
author = {Veli{\v{c}}kovi{\'{c}}, Petar and Karazija, Laurynas and Lane, Nicholas D. and Bhattacharya, Sourav and Liberis, Edgar and Li{\`{o}}, Pietro and Chieh, Angela and Bellahsen, Otmane and Vegreville, Matthieu},
doi = {10.1145/3240925.3240937},
eprint = {1709.08073},
isbn = {9781450364508},
month = {sep},
title = {{Cross-modal Recurrent Models for Weight Objective Prediction from Multimodal Time-series Data}},
url = {http://arxiv.org/abs/1709.08073 http://dx.doi.org/10.1145/3240925.3240937},
year = {2017}
}
@misc{R_software,
address = {Vienna, Austria},
author = {{R Core Team}},
title = {{R: A Language and Environment for Statistical Computing}},
url = {https://www.r-project.org/},
year = {2017}
}
@inproceedings{Davis2006a,
abstract = {Receiver Operator Characteristic (ROC) curves are commonly used to present results for binary decision problems in machine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm's performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has properties much like the convex hull in ROC space; we show an efficient algorithm for computing this curve. Finally, we also note differences in the two types of curves are significant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that optimize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {1609.07195},
author = {Davis, Jesse and Goadrich, Mark},
booktitle = {Proc. 23rd Int. Conf. Mach. Learn. - ICML '06},
doi = {10.1145/1143844.1143874},
eprint = {1609.07195},
isbn = {1595933832},
issn = {14710080},
pages = {233--240},
pmid = {19165215},
publisher = {ACM Press},
title = {{The relationship between Precision-Recall and ROC curves}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143874},
year = {2006}
}
@misc{Trojanowski1998,
abstract = {$\backslash$nThe presence of Lewy bodies (LBs) in dopaminergic neurons of the substantia nigra pars compacta, as well as neuron loss and gliosis, is a diagnostic hallmark of Parkinson disease (PD), but LBs also are seen in other cortical and subcortical neurons of the PD brain.1- 2 Additionally, LBs also occur in similar populations of neurons in the brains of patients with the classic clinical and pathological features of Alzheimer disease (AD).1- 2 Furthermore, the presence of numerous cortical intraneuronal LBs, but only rare AD neurofibrillary tangles and senile plaques in the brains of patients with an AD-like dementia, defines a neurodegenerative disorder known as dementia with LBs (DLB).1- 2 Ultrastructural examination of LBs has revealed masses of aggregated 7- to 25-nm-diameter filaments that appear similar to neurofilaments (NFs), but the precise molecular composition of LBs, including the abnormal filaments in these intracytoplasmic neuronal inclusions, remains to be clarified.1- 2 Indeed, the biological significance of LBs, especially the role that they might play in the degeneration of neurons in LB disorders, is still enigmatic (Figure 1).$\backslash$n},
archivePrefix = {arXiv},
arxivId = {arXiv:1201.3585v2},
author = {Trojanowski, John Q. and Lee, Virginia M.-Y.},
booktitle = {Arch. Neurol.},
doi = {10.1001/archneur.55.2.151},
eprint = {arXiv:1201.3585v2},
isbn = {0003-9942 (Print)$\backslash$r0003-9942 (Linking)},
issn = {00039942},
keywords = {lewy bodies,lewy body disease,parkinson disease},
month = {feb},
number = {2},
pages = {151--152},
pmid = {9482355},
publisher = {American Medical Association},
title = {{Aggregation of neurofilament and $\alpha$-synuclein proteins in Lewy bodies: Implications for the pathogenesis of Parkinson disease and Lewy body dementia}},
url = {http://archneur.jamanetwork.com/article.aspx?doi=10.1001/archneur.55.2.151},
volume = {55},
year = {1998}
}
@article{Aiff2015,
abstract = {Long-term lithium treatment is associated with end-stage renal disease, but there is little evidence of a clinically significant reduction in renal function in most patients. We previously found that 1.5{\%} of people who took lithium from the 1960s and 1970s developed end-stage renal disease; however, none of the patients who started after 1980 had end-stage renal disease. Here we aimed to study the prevalence and extent of kidney damage during the course of long-term lithium treatment since 1980. We retrieved serum lithium and creatinine levels from 4879 patients examined between 1 January 1981 and 31 December 2010. Only patients who started their lithium treatment during the study period and had at least 10 years of cumulative treatment were included. The study group comprised 630 adult patients (402 women and 228 men) with normal creatinine levels at the start of lithium treatment. There was a yearly increase in median serum creatinine levels already from the first year of treatment. About one-third of the patients who had taken lithium for 10-29 years had evidence of chronic renal failure but only 5{\%} were in the severe or very severe category. The results indicate that a substantial proportion of adult patients who are treated with lithium for more than a decade develop signs of renal functional impairment, also when treated according to modern therapeutic principles. Our results emphasise that lithium treatment requires continuous monitoring of kidney function.},
author = {Aiff, Harald and Attman, Per Ola and Aurell, Mattias and Bendz, Hans and Ramsauer, Bernd and Sch{\"{o}}n, Staffan and Svedlund, Jan},
doi = {10.1177/0269881115573808},
editor = {Nutt, David J and Blier, Pierre},
issn = {14617285},
journal = {J. Psychopharmacol.},
keywords = {Lithium,adverse effects,bipolar disorder,chronic,kidney disease,renal failure},
month = {may},
number = {5},
pages = {608--614},
title = {{Effects of 10 to 30 years of lithium treatment on kidney function}},
url = {http://journals.sagepub.com/doi/10.1177/0269881115573808},
volume = {29},
year = {2015}
}
@misc{Banerjeed,
author = {Banerjee, Soumya},
title = {{neelsoumya / deseq2{\_}diffexpression{\_}vignette — Bitbucket}},
url = {https://bitbucket.org/neelsoumya/deseq2{\_}diffexpression{\_}vignette/src/master/},
urldate = {2019-12-11}
}
@article{Marks1978,
author = {Marks, Charles E. and Fodor, Jerry A.},
doi = {10.2307/2184356},
issn = {00318108},
journal = {Philos. Rev.},
month = {jan},
number = {1},
pages = {108},
title = {{The Language of Thought.}},
url = {https://www.jstor.org/stable/2184356?origin=crossref},
volume = {87},
year = {1978}
}
@inproceedings{Avati2017,
abstract = {Improving the quality of end-of-life care for hospitalized patients is a priority for healthcare organizations. Studies have shown that physicians tend to over-estimate prognoses, which in combination with treatment inertia results in a mismatch between patients wishes and actual care at the end of life. We describe a method to address this problem using Deep Learning and Electronic Health Record (EHR) data, which is currently being piloted, with Institutional Review Board approval, at an academic medical center. The EHR data of admitted patients are automatically evaluated by an algorithm, which brings patients who are likely to benefit from palliative care services to the attention of the Palliative Care team. The algorithm is a Deep Neural Network trained on the EHR data from previous years, to predict all-cause 3-12 month mortality of patients as a proxy for patients that could benefit from palliative care. Our predictions enable the Palliative Care team to take a proactive approach in reaching out to such patients, rather than relying on referrals from treating physicians, or conduct time consuming chart reviews of all patients. We also present a novel interpretation technique which we use to provide explanations of the model's predictions.},
author = {Avati, Anand and Jung, Kenneth and Harman, Stephanie and Downing, Lance and Ng, Andrew and Shah, Nigam H.},
booktitle = {Proc. - 2017 IEEE Int. Conf. Bioinforma. Biomed. BIBM 2017},
doi = {10.1109/BIBM.2017.8217669},
isbn = {9781509030491},
issn = {1472-6947},
keywords = {Health Informatics,Information Systems and Communication Service,Management of Computing and Information Systems},
month = {dec},
number = {S4},
pages = {311--316},
publisher = {BioMed Central},
title = {{Improving palliative care with deep learning}},
url = {https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-018-0677-8},
volume = {2017-Janua},
year = {2017}
}
@article{Scholkopf2019,
abstract = {Graphical causal inference as pioneered by Judea Pearl arose from research on artificial intelligence (AI), and for a long time had little connection to the field of machine learning. This article discusses where links have been and should be established, introducing key concepts along the way. It argues that the hard open problems of machine learning and AI are intrinsically related to causality, and explains how the field is beginning to understand them.},
archivePrefix = {arXiv},
arxivId = {1911.10500},
author = {Sch{\"{o}}lkopf, Bernhard},
eprint = {1911.10500},
month = {nov},
title = {{Causality for Machine Learning}},
url = {http://arxiv.org/abs/1911.10500},
year = {2019}
}
@article{Falster2017,
author = {Falster, Daniel S and Fitzjohn, Richard G and Pennell, Matthew W. and Cornwell, William K.},
doi = {10.7287/peerj.preprints.3401v1},
issn = {2167-9843},
journal = {PeerJ Prepr.},
keywords = {Data sharing,Meta-analysis,Semantic versioning,Version control},
month = {nov},
pages = {5:e3401v1},
publisher = {PeerJ Inc.},
title = {{Versioned data: why it is needed and how it can be achieved (easily and cheaply)}},
url = {https://peerj.com/preprints/3401/},
year = {2017}
}
@article{Mnih2015,
abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
issn = {0028-0836},
journal = {Nature},
keywords = {Computer science},
month = {feb},
number = {7540},
pages = {529--533},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {http://www.nature.com/articles/nature14236},
volume = {518},
year = {2015}
}
@article{Lundberg2020,
author = {Lundberg, Scott M and Erion, Gabriel and Chen, Hugh and Degrave, Alex and Prutkin, Jordan M and Nair, Bala and Katz, Ronit and Himmelfarb, Jonathan and Bansal, Nisha and Lee, Su-in},
doi = {10.1038/s42256-019-0138-9},
isbn = {4225601901389},
issn = {2522-5839},
journal = {Nat. Mach. Intell.},
number = {January},
publisher = {Springer US},
title = {{with explainable AI for trees}},
url = {http://dx.doi.org/10.1038/s42256-019-0138-9},
volume = {2},
year = {2020}
}
@article{Kohonen1982,
abstract = {This work contains a theoretical study and computer simulations of a new self-organizing process. The principal discovery is that in a simple network of adaptive physical elements which receives signals from a primary event space, the signal representations are automatically mapped onto a set of output responses in such a way that the responses acquire the same topological order as that of the primary events. In other words, a principle has been discovered which facilitates the automatic formation of topologically correct maps of features of observable events. The basic self-organizing system is a one- or two-dimensional array of processing units resembling a network of threshold-logic units, and characterized by short-range lateral feedback between neighbouring units. Several types of computer simulations are used to demonstrate the ordering process as well as the conditions under which it fails. {\textcopyright} 1982 Springer-Verlag.},
author = {Kohonen, Teuvo},
doi = {10.1007/BF00337288},
issn = {03401200},
journal = {Biol. Cybern.},
number = {1},
pages = {59--69},
publisher = {Springer-Verlag},
title = {{Self-organized formation of topologically correct feature maps}},
url = {http://link.springer.com/10.1007/BF00337288},
volume = {43},
year = {1982}
}
@article{Wang2015a,
abstract = {We present a machine learning algorithm for building clas-sifiers that are comprised of a small number of disjunctions of conjunctions (or 's of and 's). An example of a classifier of this form is as follows: If X satisfies (conditions A1, A2, and A3) OR (conditions B1 and B2) OR (conditions C1 and C2), then we predict that Y=1, ELSE predict Y=0. Mod-els of this form have the advantage of being interpretable to human experts, since they produce a set of conditions that concisely describe a specific class. In our Bayesian model, there are prior parameters that the user can set in order for the model to have a desired size and shape to conform with a domain-specific definition of interpretability. Our method has a major advantage over classical associative classification methods in that it is not greedy. We present an approximate MAP inference technique involving association rule mining and simulated annealing, which allows the method to scale nicely. Our interest in developing this model is to use it to create a predictive model of user behavior with respect to in-vehicle context-aware advertising. This is part of an effort to create the connected vehicle, where context data might be collected from the vehicle in order to benefit the driver and passengers. We present several predictive models of user behavior based on data collected from Mechanical Turk; these models are accurate, yet interpretable. We also quantify the effect on prediction accuracy of having contex-tual attributes.},
archivePrefix = {arXiv},
arxivId = {1504.07614},
author = {Wang, Tong and Rudin, Cynthia and Doshi-Velez, Finale and Liu, Yimin and Klampfl, Erica and Macneille, Perry},
eprint = {1504.07614},
keywords = {Bayesian modeling,association rules,interpretable classifier},
month = {apr},
pages = {1--40},
title = {{Bayesian Or's of And's for Interpretable Classification with Application to Context Aware Recommender Systems}},
url = {http://arxiv.org/abs/1504.07614},
year = {2015}
}
@article{Lonnberg2017,
abstract = {Differentiation of na{\"{i}}ve CD4(+) T cells into functionally distinct T helper subsets is crucial for the orchestration of immune responses. Due to extensive heterogeneity and multiple overlapping transcriptional programs in differentiating T cell populations, this process has remained a challenge for systematic dissection in vivo. By using single-cell transcriptomics and computational analysis using a temporal mixtures of Gaussian processes model, termed GPfates, we reconstructed the developmental trajectories of Th1 and Tfh cells during blood-stage Plasmodium infection in mice. By tracking clonality using endogenous TCR sequences, we first demonstrated that Th1/Tfh bifurcation had occurred at both population and single-clone levels. Next, we identified genes whose expression was associated with Th1 or Tfh fates, and demonstrated a T-cell intrinsic role for Galectin-1 in supporting a Th1 differentiation. We also revealed the close molecular relationship between Th1 and IL-10-producing Tr1 cells in this infection. Th1 and Tfh fates emerged from a highly proliferative precursor that upregulated aerobic glycolysis and accelerated cell cycling as cytokine expression began. Dynamic gene expression of chemokine receptors around bifurcation predicted roles for cell-cell in driving Th1/Tfh fates. In particular, we found that precursor Th cells were coached towards a Th1 but not a Tfh fate by inflammatory monocytes. Thus, by integrating genomic and computational approaches, our study has provided two unique resources, a database www.PlasmoTH.org, which facilitates discovery of novel factors controlling Th1/Tfh fate commitment, and more generally, GPfates, a modelling framework for characterizing cell differentiation towards multiple fates.},
author = {L{\"{o}}nnberg, Tapio and Svensson, Valentine and James, Kylie R and Fernandez-Ruiz, Daniel and Sebina, Ismail and Montandon, Ruddy and Soon, Megan S F and Fogg, Lily G and Nair, Arya Sheela and Liligeto, Urijah N. and Stubbington, Michael J T and Ly, Lam-Ha and Bagger, Frederik Otzen and Zwiessele, Max and Lawrence, Neil D and Souza-Fonseca-Guimaraes, Fernando and Bunn, Patrick T and Engwerda, Christian R and Heath, William R and Billker, Oliver and Stegle, Oliver and Haque, Ashraful and Teichmann, Sarah A},
doi = {10.1126/sciimmunol.aal2192},
issn = {2470-9468},
journal = {Sci. Immunol.},
month = {mar},
number = {9},
pages = {eaal2192},
pmid = {28345074},
publisher = {Europe PMC Funders},
title = {{Single-cell RNA-seq and computational analysis using temporal mixture modeling resolves T H 1/T FH fate bifurcation in malaria}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28345074 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5365145 http://immunology.sciencemag.org/lookup/doi/10.1126/sciimmunol.aal2192},
volume = {2},
year = {2017}
}
@techreport{pilot_mit,
author = {Teitelman, W},
title = {{PILOT: A STEP TOWARDS MAN-COMPUTER SYMBIOSIS}},
url = {https://dl.acm.org/citation.cfm?id=889931}
}
@article{Athanasiadis2017,
abstract = {The success of marker-based approaches for dissecting haematopoiesis in mouse and human is reliant on the presence of well-defined cell surface markers specific for diverse progenitor populations. An inherent problem with this approach is that the presence of specific cell surface markers does not directly reflect the transcriptional state of a cell. Here, we used a marker-free approach to computationally reconstruct the blood lineage tree in zebrafish and order cells along their differentiation trajectory, based on their global transcriptional differences. Within the population of transcriptionally similar stem and progenitor cells, our analysis reveals considerable cell-to-cell differences in their probability to transition to another committed state. Once fate decision is executed, the suppression of transcription of ribosomal genes and upregulation of lineage-specific factors coordinately controls lineage differentiation. Evolutionary analysis further demonstrates that this haematopoietic programme is highly conserved between zebrafish and higher vertebrates.},
author = {Athanasiadis, Emmanouil I. and Botthof, Jan G. and Andres, Helena and Ferreira, Lauren and Lio, Pietro and Cvejic, Ana},
doi = {10.1038/s41467-017-02305-6},
issn = {20411723},
journal = {Nat. Commun.},
month = {dec},
number = {1},
pages = {2045},
title = {{Single-cell RNA-sequencing uncovers transcriptional states and fate decisions in haematopoiesis}},
volume = {8},
year = {2017}
}
@article{Levey1999,
abstract = {Serum creatinine concentration is widely used as an index of renal function, but this concentration is affected by factors other than glomerular filtration rate (GFR).},
archivePrefix = {arXiv},
arxivId = {1805.06921},
author = {Levey, Andrew S and Bosch, Juan P and Lewis, Julia Breyer and Greene, Tom and Rogers, Nancy and Roth, David},
doi = {10.7326/0003-4819-130-6-199903160-00002},
eprint = {1805.06921},
isbn = {0003-4819},
issn = {00034819},
journal = {Ann. Intern. Med.},
month = {mar},
number = {6},
pages = {461--470},
pmid = {10075613},
title = {{A more accurate method to estimate glomerular filtration rate from serum creatinine: A new prediction equation}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10075613},
volume = {130},
year = {1999}
}
@article{Bica2019,
abstract = {The estimation of treatment effects is a pervasive problem in medicine. Existing methods for estimating treatment effects from longitudinal observational data assume that there are no hidden confounders. This assumption is not testable in practice and, if it does not hold, leads to biased estimates. In this paper, we develop the Time Series Deconfounder, a method that leverages the assignment of multiple treatments over time to enable the estimation of treatment effects even in the presence of hidden confounders. The Time Series Deconfounder uses a novel recurrent neural network architecture with multitask output to build a factor model over time and infer substitute confounders that render the assigned treatments conditionally independent. Then it performs causal inference using the substitute confounders. We provide a theoretical analysis for obtaining unbiased causal effects of time-varying exposures using the Time Series Deconfounder. Using simulations we show the effectiveness of our method in deconfounding the estimation of treatment responses in longitudinal data.},
archivePrefix = {arXiv},
arxivId = {1902.00450},
author = {Bica, Ioana and Alaa, Ahmed M. and van der Schaar, Mihaela},
eprint = {1902.00450},
month = {feb},
title = {{Time Series Deconfounder: Estimating Treatment Effects over Time in the Presence of Hidden Confounders}},
url = {http://arxiv.org/abs/1902.00450},
year = {2019}
}
@inproceedings{Spasov2018,
author = {Spasov, Simeon E. and Passamonti, Luca and Duggento, Andrea and Lio, Pietro and Toschi, Nicola},
booktitle = {2018 40th Annu. Int. Conf. IEEE Eng. Med. Biol. Soc.},
doi = {10.1109/EMBC.2018.8512468},
isbn = {978-1-5386-3646-6},
issn = {1557170X},
month = {jul},
pages = {1271--1274},
publisher = {IEEE},
title = {{A Multi-modal Convolutional Neural Network Framework for the Prediction of Alzheimer's Disease}},
url = {https://ieeexplore.ieee.org/document/8512468/},
year = {2018}
}
@book{Hernan,
author = {Hernan, Miguel and Robins, Jamie},
title = {{Causal Inference Book}},
url = {https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/}
}
@article{Polson2011,
abstract = {This paper argues that the half-Cauchy distribution should replace the inverse-Gamma distribution as a default prior for a top-level scale parameter in Bayesian hierarchical models, at least for cases where a proper prior is necessary. Our arguments involve a blend of Bayesian and frequentist reasoning, and are intended to complement the original case made by Gelman (2006) in support of the folded-t family of priors. First, we generalize the half-Cauchy prior to the wider class of hypergeometric inverted-beta priors. We derive expressions for posterior moments and marginal densities when these priors are used for a top-level normal variance in a Bayesian hierarchical model. We go on to prove a proposition that, together with the results for moments and marginals, allows us to characterize the frequentist risk of the Bayes estimators under all global-shrinkage priors in the class. These theoretical results, in turn, allow us to study the frequentist properties of the half-Cauchy prior versus a wide class of alternatives. The half-Cauchy occupies a sensible 'middle ground' within this class: it performs very well near the origin, but does not lead to drastic compromises in other parts of the parameter space. This provides an alternative, classical justification for the repeated, routine use of this prior. We also consider situations where the underlying mean vector is sparse, where we argue that the usual conjugate choice of an inverse-gamma prior is particularly inappropriate, and can lead to highly distorted posterior inferences. Finally, we briefly summarize some open issues in the specification of default priors for scale terms in hierarchical models.},
archivePrefix = {arXiv},
arxivId = {1104.4937},
author = {Polson, Nicholas G. and Scott, James G.},
doi = {10.1214/12-BA730},
eprint = {1104.4937},
isbn = {1936-0975},
issn = {19360975},
journal = {Bayesian Anal.},
keywords = {Hierarchical models,Normal scale mixtures,Shrinkage},
month = {apr},
number = {4},
pages = {887--902},
title = {{On the half-cauchy prior for a global scale parameter}},
url = {http://arxiv.org/abs/1104.4937},
volume = {7},
year = {2012}
}
@article{Vu2019,
abstract = {Data mining tools have been increasingly used in health research, with the promise of accelerating discoveries. Lift is a standard association metric in the data mining community. However, health researchers struggle with the interpretation of lift. As a result, dissemination of data mining results can be met with hesitation. The relative risk and odds ratio are standard association measures in the health domain, due to their straightforward interpretation and comparability across populations. We aimed to investigate the lift-relative risk and the lift-odds ratio relationships, and provide tools to convert lift to the relative risk and odds ratio. We derived equations linking lift-relative risk and lift-odds ratio. We discussed how lift, relative risk, and odds ratio behave numerically with varying association strengths and exposure prevalence levels. The lift-relative risk relationship was further illustrated using a high-dimensional dataset which examines the association of exposure to airborne pollutants and adverse birth outcomes. We conducted spatial association rule mining using the Kingfisher algorithm, which identified association rules using its built-in lift metric. We directly estimated relative risks and odds ratios from 2 by 2 tables for each identified rule. These values were compared to the corresponding lift values, and relative risks and odds ratios were computed using the derived equations. As the exposure-outcome association strengthens, the odds ratio and relative risk move away from 1 faster numerically than lift, i.e. |log (odds ratio)| ≥ |log (relative risk)| ≥ |log (lift)|. In addition, lift is bounded by the smaller of the inverse probability of outcome or exposure, i.e. lift≤ min (1/P(O), 1/P(E)). Unlike the relative risk and odds ratio, lift depends on the exposure prevalence for fixed outcomes. For example, when an exposure A and a less prevalent exposure B have the same relative risk for an outcome, exposure A has a lower lift than B. Lift, relative risk, and odds ratio are positively correlated and share the same null value. However, lift depends on the exposure prevalence, and thus is not straightforward to interpret or to use to compare association strength. Tools are provided to obtain the relative risk and odds ratio from lift.},
author = {Vu, Khanh and Clark, Rebecca A. and Bellinger, Colin and Erickson, Graham and Osornio-Vargas, Alvaro and Za{\"{i}}ane, Osmar R. and Yuan, Yan},
doi = {10.1186/s12911-019-0838-4},
issn = {14726947},
journal = {BMC Med. Inform. Decis. Mak.},
keywords = {Air pollution,Association rule mining,Data mining,Environmental health,Interestingness measures,Lift,Odds ratio,Relative risk},
month = {dec},
number = {1},
pages = {112},
publisher = {BioMed Central},
title = {{The index lift in data mining has a close relationship with the association measure relative risk in epidemiological studies}},
url = {https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-019-0838-4},
volume = {19},
year = {2019}
}
@article{Cunningham2013,
abstract = {This software article describes the GATE family of open source text analysis tools and processes. GATE is one of the most widely used systems of its type with yearly download rates of tens of thousands and many active users in both academic and industrial contexts. In this paper we report three examples of GATE-based systems operating in the life sciences and in medicine. First, in genome-wide association studies which have contributed to discovery of a head and neck cancer mutation association. Second, medical records analysis which has significantly increased the statistical power of treatment/outcome models in the UK's largest psychiatric patient cohort. Third, richer constructs in drug-related searching. We also explore the ways in which the GATE family supports the various stages of the lifecycle present in our examples. We conclude that the deployment of text mining for document abstraction or rich search and navigation is best thought of as a process, and that with the right computational tools and data collection strategies this process can be made defined and repeatable. The GATE research programme is now 20 years old and has grown from its roots as a specialist development tool for text processing to become a rather comprehensive ecosystem, bringing together software developers, language engineers and research staff from diverse fields. GATE now has a strong claim to cover a uniquely wide range of the lifecycle of text analysis systems. It forms a focal point for the integration and reuse of advances that have been made by many people (the majority outside of the authors' own group) who work in text processing for biomedicine and other areas. GATE is available online {\textless}1{\textgreater} under GNU open source licences and runs on all major operating systems. Support is available from an active user and developer community and also on a commercial basis.},
author = {Cunningham, Hamish and Tablan, Valentin and Roberts, Angus and Bontcheva, Kalina},
doi = {10.1371/journal.pcbi.1002854},
editor = {Prlic, Andreas},
isbn = {1553-7358 (Linking)},
issn = {1553734X},
journal = {PLoS Comput. Biol.},
month = {feb},
number = {2},
pages = {e1002854},
pmid = {23408875},
publisher = {Public Library of Science},
title = {{Getting More Out of Biomedical Documents with GATE's Full Lifecycle Open Source Text Analytics}},
url = {http://dx.plos.org/10.1371/journal.pcbi.1002854},
volume = {9},
year = {2013}
}
@article{Newman2010,
abstract = {The Detention Health Advisory Group (DeHAG) applauds Patrick McGorry's statements about the risks associated with Australian immigration detention, outlined in your Feb 6 Editorial (see record [rid]2010-02609-003[/rid]). The quoted study from the University of Wollongong examined the health of individuals within Australian detention centres; not surprisingly it showed that lengthy detention is associated with negative mental health consequences. You stated that DeHAG should respond to the study, but in fact we advised the Department of Immigration and Citizenship to commission this research. The quoted current number of people in detention was incorrect. There has been a decline in numbers over the past 5 years. In addition, the authors note that DeHAG will continue to advocate for minors, to ensure that they are cared for in the most humane environments possible, and for better review processes to avoid lengthy detention to ensure that our immigration process continues to improve to be as compassionate as possible. Thank you for raising awareness of this important issue. (PsycINFO Database Record (c) 2015 APA, all rights reserved). (journal abstract)},
author = {Newman, Louise and Lightfoot, Tim and Singleton, Gillian and Aroche, Jorge and Yong, Choong Siew and Eagar, Sandra and Gordon, Amanda and Kotala, Paul and Whelan, Anna and Whittaker, Maxine},
doi = {10.1016/S0140-6736(10)60571-5},
issn = {01406736},
journal = {Lancet},
month = {apr},
number = {9723},
pages = {1344--1345},
pmid = {20399975},
publisher = {Elsevier},
title = {{Mental illness in Australian immigration detention centres}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20399975},
volume = {375},
year = {2010}
}
@misc{conditional_logistic_regression,
title = {{conditional logistic regression clogit function | R Documentation}},
url = {https://www.rdocumentation.org/packages/survival/versions/2.41-2/topics/clogit}
}
@article{Krishna2004,
author = {Krishna, S. and Sahay, Sundeep and Walsham, Geoff},
doi = {10.1145/975817.975818},
issn = {00010782},
journal = {Commun. ACM},
month = {apr},
number = {4},
pages = {62--66},
title = {{Managing cross-cultural issues in global software outsourcing}},
url = {http://portal.acm.org/citation.cfm?doid=975817.975818},
volume = {47},
year = {2004}
}
@misc{Chen2018c,
author = {Chen, Shanquan and Li, Jun and Wang, Dan and Fung, Hong and yi Wong, Lai and Zhao, Lu},
booktitle = {Lancet},
doi = {10.1016/S0140-6736(18)30499-9},
isbn = {9080170437},
issn = {1474547X},
month = {apr},
number = {10130},
pages = {1572},
pmid = {29695339},
publisher = {Elsevier},
title = {{The hepatitis B epidemic in China should receive more attention}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/29695339},
volume = {391},
year = {2018}
}
@article{Velickovic2017a,
abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
archivePrefix = {arXiv},
arxivId = {1710.10903},
author = {Veli{\v{c}}kovi{\'{c}}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`{o}}, Pietro and Bengio, Yoshua},
doi = {10.4271/821240},
eprint = {1710.10903},
isbn = {1710.10903v3},
month = {oct},
pmid = {18491688},
title = {{Graph Attention Networks}},
url = {http://arxiv.org/abs/1710.10903},
year = {2017}
}
@article{VandeMeent2018,
abstract = {This document is designed to be a first-year graduate-level introduction to probabilistic programming. It not only provides a thorough background for anyone wishing to use a probabilistic programming system, but also introduces the techniques needed to design and build these systems. It is aimed at people who have an undergraduate-level understanding of either or, ideally, both probabilistic machine learning and programming languages. We start with a discussion of model-based reasoning and explain why conditioning as a foundational computation is central to the fields of probabilistic machine learning and artificial intelligence. We then introduce a simple first-order probabilistic programming language (PPL) whose programs define static-computation-graph, finite-variable-cardinality models. In the context of this restricted PPL we introduce fundamental inference algorithms and describe how they can be implemented in the context of models denoted by probabilistic programs. In the second part of this document, we introduce a higher-order probabilistic programming language, with a functionality analogous to that of established programming languages. This affords the opportunity to define models with dynamic computation graphs, at the cost of requiring inference methods that generate samples by repeatedly executing the program. Foundational inference algorithms for this kind of probabilistic programming language are explained in the context of an interface between program executions and an inference controller. This document closes with a chapter on advanced topics which we believe to be, at the time of writing, interesting directions for probabilistic programming research; directions that point towards a tight integration with deep neural network research and the development of systems for next-generation artificial intelligence applications.},
archivePrefix = {arXiv},
arxivId = {1809.10756},
author = {van de Meent, Jan-Willem and Paige, Brooks and Yang, Hongseok and Wood, Frank},
eprint = {1809.10756},
month = {sep},
title = {{An Introduction to Probabilistic Programming}},
url = {http://arxiv.org/abs/1809.10756},
year = {2018}
}
@article{Markham2019,
author = {Markham, Annette N. and Pereira, Gabriel},
doi = {10.3389/fdata.2019.00035},
issn = {2624-909X},
journal = {Front. Big Data},
month = {oct},
title = {{Experimenting With Algorithms and Memory-Making: Lived Experience and Future-Oriented Ethics in Critical Data Science}},
url = {https://www.frontiersin.org/article/10.3389/fdata.2019.00035/full},
volume = {2},
year = {2019}
}
@article{Prescott2018,
abstract = {A vital stage in the mathematical modelling of real-world systems is to calibrate a model's parameters to observed data. Likelihood-free parameter inference methods, such as Approximate Bayesian Computation, build Monte Carlo samples of the uncertain parameter distribution by comparing the data with large numbers of model simulations. However, the computational expense of generating these simulations forms a significant bottleneck in the practical application of such methods. We identify how simulations of cheap, low-fidelity models have been used separately in two complementary ways to reduce the computational expense of building these samples, at the cost of introducing additional variance to the resulting parameter estimates. We explore how these approaches can be unified so that cost and benefit are optimally balanced, and we characterise the optimal choice of how often to simulate from cheap, low-fidelity models in place of expensive, high-fidelity models in Monte Carlo ABC algorithms. The resulting early accept/reject multifidelity ABC algorithm that we propose is shown to give improved performance over existing multifidelity and high-fidelity approaches.},
archivePrefix = {arXiv},
arxivId = {1811.09550},
author = {Prescott, Thomas P and Baker, Ruth E},
eprint = {1811.09550},
month = {nov},
title = {{Multifidelity Approximate Bayesian Computation}},
url = {http://arxiv.org/abs/1811.09550},
year = {2018}
}
@article{Ying2018,
abstract = {Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs---a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose DiffPool, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DiffPool learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DiffPool yields an average improvement of 5-10{\%} accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1806.08804},
author = {Ying, Rex and You, Jiaxuan and Morris, Christopher and Ren, Xiang and Hamilton, William L. and Leskovec, Jure},
doi = {arXiv:1806.08804v3},
eprint = {1806.08804},
isbn = {1558606041},
issn = {18160948},
month = {jun},
pmid = {397311},
title = {{Hierarchical Graph Representation Learning with Differentiable Pooling}},
url = {http://arxiv.org/abs/1806.08804},
year = {2018}
}
@article{Hunte2014,
abstract = {The evolutionary advantage of humans is in our unique ability to process stories – we have highly evolved 'narrative organs.' Through storytelling, vicarious knowledge, even guarded knowledge, is used to help our species survive. We learn, regardless of whether the story being told is 'truth' or 'fiction.' Humans place themselves in stories, as both observer and participant, to create a 'neural balance' or sweet spot that allows them to be immersed in a story without being entirely threatened by it – and this involvement in story leads to the formation of empathy – an empathy that is integral to forging a future humanity. It is through empathy, we argue, that stories have the power to save us. The hippocampi process narrative details. Situated alongside are the amygdalae – organs that place the reader in the story. The temporal lobes store 'story nuggets.' Finally there's the frontal cortex to inhibit full participation in narrative, so that the story can be experienced vicariously.},
author = {Hunte, Bem Le and Golembiewski, Jan A},
doi = {10.4172/2151-6200.100073},
journal = {Arts Soc. Sci. J.},
month = {jul},
number = {02},
pages = {1--4},
publisher = {AstonJournals},
title = {{Stories Have the Power to Save us: A Neurological Framework for the Imperative to Tell Stories}},
url = {https://www.omicsonline.org/open-access/stories-have-the-power-to-save-us-a-neurological-framework-for-the-imperative-to-tell-stories-2151-6200.100073.php?aid=31734},
volume = {05},
year = {2016}
}
@article{Malhi2011a,
abstract = {Bipolar disorder is a recurrent chronic illness distinguished by periods of mania and depression. Lithium has been used for about 60 years as a 'mood stabilizer' for bipolar disorder with proven efficacy in preventing relapse of both mania and depression. Despite its long history and ongoing use in current management of bipolar disorder, the optimal dosing of lithium is still the subject of ongoing debate. This article aims to evaluate different dosing schedules, in the light of the unique pharmacokinetic and pharmacodynamic properties of lithium, as well as its adverse-effect and toxicity profiles. This is all the more important given the narrow therapeutic index of lithium. Current recommendations mostly advocate that lithium be administered in multiple daily doses. However, single daily or alternate daily schedules may be viable options for administration. Multiple daily schedules are thought to be advantageous in maintaining more constant plasma lithium concentrations than single daily regimens, which are associated with significant fluctuations throughout the day. When comparing these two schedules with respect to plasma lithium concentrations, adverse-effect profiles and recurrence of symptoms, there are no significant differences between the two regimens. In fact, a single daily regimen may have added advantages in reducing the risk of long-term renal damage and increasing compliance. The evidence for alternate daily dosing is somewhat varied with regard to symptom recurrence; however, this schedule has been shown to be associated with decreased adverse effects, and further research into this issue is therefore warranted. Presently, therefore, clinicians should consider single daily administration of lithium to potentially minimize adverse effects and enhance compliance. {\textcopyright} 2011 Adis Data Information BV. All rights reserved.},
author = {Malhi, Gin S. and Tanious, Michelle},
doi = {10.2165/11586970-000000000-00000},
issn = {11727047},
journal = {CNS Drugs},
keywords = {Bipolar-disorders,Lithium,adverse reactions,pharmacokinetics,therapeutic use},
month = {apr},
number = {4},
pages = {289--298},
publisher = {Springer International Publishing},
title = {{Optimal frequency of lithium administration in the treatment of bipolar disorder: Clinical and dosing considerations}},
url = {http://link.springer.com/10.2165/11586970-000000000-00000},
volume = {25},
year = {2011}
}
@inproceedings{Vorontsov2015,
abstract = {Probabilistic topic modeling of text collections is a powerful tool for statistical text analysis. In this paper we announce the BigARTM open source project (http://bigartm.org), which provides the parallel online EM algorithm for learning additively regularized multimodal topic models of large collections. We show that BigARTM outperforms other popular packages in quality, runtime and multicriteria functionality.},
author = {Vorontsov, Konstantin and Frei, Oleksandr and Apishev, Murat and Romov, Peter and Dudarenko, Marina},
booktitle = {Commun. Comput. Inf. Sci.},
doi = {10.1007/978-3-319-26123-2_36},
isbn = {9783319261225},
issn = {18650929},
keywords = {Additive regularization of topic models,BigARTM,EM-algorithm,Latent dirichlet allocation,Probabilistic latent sematic analysis,Probabilistic topic modeling,Stochastic matrix factorization},
pages = {370--381},
title = {{Bigartm: Open source library for regularized multimodal topic modeling of large collections}},
url = {http://link.springer.com/10.1007/978-3-319-26123-2{\_}36},
volume = {542},
year = {2015}
}
@article{Diaconis2000,
abstract = {The time has now come when graph theory should be part of the education of every serious student of mathematics and computer science, both for its own sake and to enhance the appreciation of mathematics as a whole. This book is an in-depth account of graph theory, written with such a student in mind; it reflects the current state of the subject and emphasizes connections with other branches of pure mathematics. The volume grew out of the author's earlier book, Graph Theory - An Introductory Course, but its length is well over twice that of its predecessor, allowing it to reveal many exciting new developments in the subject. Recognizing that graph theory is one of several courses competing for the attention of a student, the book contains extensive descriptive passages designed to convey the flavor of the subject and to arouse interest.In addition to a modern treatment of the classical areas of graph theory such as coloring, matching, extremal theory, and algebraic graph theory, the book presents a detailed account of newer topics, including Szemer'edi's Regularity Lemma and its use, Shelah's extension of the Hales-Jewett Theorem, the precise nature of the phase transition in a random graph process, the connection between electrical networks and random walks on graphs, and the Tutte polynomial and its cousins in knot theory.In no other branch of mathematics is it as vital to tackle and solve challenging exercises in order to master the subject. To this end, the book contains an unusually large number of well thought-out exercises: over 600 in total. Although some are straightforward, most of them are substantial, and others will stretch even the most able reader.},
address = {New York, NY},
author = {Diaconis, Persi and Bollobas, Bela},
doi = {10.2307/2669801},
isbn = {978-0-387-98488-9},
issn = {01621459},
journal = {J. Am. Stat. Assoc.},
number = {452},
pages = {1377},
publisher = {Springer New York},
series = {Graduate Texts in Mathematics},
title = {{Modern Graph Theory}},
url = {http://link.springer.com/10.1007/978-1-4612-0619-4},
volume = {95},
year = {2000}
}
@article{Maaten2008,
abstract = {We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence theway in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza- tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
author = {{Van Der Maaten}, Laurens and Hinton, Geoffrey},
issn = {15324435},
journal = {J. Mach. Learn. Res.},
keywords = {Dimensionality reduction,Embedding algorithms,Manifold learning,Multidimensional scaling,Visualization},
number = {Nov},
pages = {2579--2625},
title = {{Visualizing data using t-SNE}},
url = {http://www.jmlr.org/papers/v9/vandermaaten08a.html},
volume = {9},
year = {2008}
}
@article{Nelson2019a,
abstract = {Failure to attend scheduled hospital appointments disrupts clinical management and consumes resource estimated at {\pounds}1 billion annually in the United Kingdom National Health Service alone. Accurate stratification of absence risk can maximize the yield of preventative interventions. The wide multiplicity of potential causes, and the poor performance of systems based on simple, linear, low-dimensional models, suggests complex predictive models of attendance are needed. Here, we quantify the effect of using complex, non-linear, high-dimensional models enabled by machine learning. Models systematically varying in complexity based on logistic regression, support vector machines, random forests, AdaBoost, or gradient boosting machines were trained and evaluated on an unselected set of 22,318 consecutive scheduled magnetic resonance imaging appointments at two UCL hospitals. High-dimensional Gradient Boosting Machine-based models achieved the best performance reported in the literature, exhibiting an area under the receiver operating characteristic curve of 0.852 and average precision of 0.511. Optimal predictive performance required 81 variables. Simulations showed net potential benefit across a wide range of attendance characteristics, peaking at {\pounds}3.15 per appointment at current prevalence and call efficiency. Optimal attendance prediction requires more complex models than have hitherto been applied in the field, reflecting the complex interplay of patient, environmental, and operational causal factors. Far from an exotic luxury, high-dimensional models based on machine learning are likely essential to optimal scheduling amongst other operational aspects of hospital care. High predictive performance is achievable with data from a single institution, obviating the need for aggregating large-scale sensitive data across governance boundaries.},
author = {Nelson, Amy and Herron, Daniel and Rees, Geraint and Nachev, Parashkev},
doi = {10.1038/s41746-019-0103-3},
issn = {2398-6352},
journal = {npj Digit. Med.},
keywords = {Health care economics,Health policy,Magnetic resonance imaging},
month = {dec},
number = {1},
pages = {26},
publisher = {Nature Publishing Group},
title = {{Predicting scheduled hospital attendance with artificial intelligence}},
url = {http://www.nature.com/articles/s41746-019-0103-3},
volume = {2},
year = {2019}
}
@inproceedings{Bengio2007,
abstract = {Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.},
author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
booktitle = {Adv. Neural Inf. Process. Syst.},
doi = {10.7551/mitpress/7503.003.0024},
isbn = {9780262195683},
issn = {10495258},
pages = {153--160},
title = {{Greedy layer-wise training of deep networks}},
url = {https://dl.acm.org/citation.cfm?id=2976476},
year = {2007}
}
@misc{drug_graph_deep_learning_leskovec,
title = {{SNAP: Modeling Polypharmacy using Graph Convolutional Networks}},
url = {http://snap.stanford.edu/decagon/},
urldate = {2019-03-20}
}
@article{Raza,
abstract = {BACKGROUND Many conditions affect renal size. To evaluate abnormalities in renal size, knowledge of standardised values for normal renal dimensions is essential as it shows variability in the values of normal renal size depending on body size, age and ethnicity. Ultrasound, being an easily available, noninvasive, safe and less expensive modality, is widely used for evaluation of renal dimensions and repeated follow-ups. The objectives of this study were to determine renal size by ultrasound in adults without any known renal disease, and to determine the relationship of renal size with body mass index. METHODS Study was conducted in the Department of Diagnostic Radiology, Shifa International Hospital and PIMS Islamabad. Renal size was assessed by ultrasound in 4,035 adult subjects with normal serum creatinine and without any known renal disease, between November 2002 and December 2010. Renal length, width, thickness and volume were obtained and mean renal length and volume were correlated with body mass index and other factors like age, side, gender, weight and height of the subjects. RESULTS Mean renal length on right side was 101.6 +/- 8.9 mm, renal width 42.7 +/- 7.1 mm, and parenchymal thickness 14.4 +/- 2.9 mm. On left side, mean renal length was 102.7 +/- 9.2 mm, width 47.6 +/- 7.0) mm, and parenchymal thickness 15.1 +/- 3.1 mm. Mean renal volume on right was 99.8 +/- 37.2 cm3 and on left was 124.4 +/- 41.3 cm3. Left renal size was significantly larger than right in both genders. Relationship of mean renal length was significant when correlated with age, side, gender, height and weight, and body mass index. Renal volumes also showed a similar relationship with side, gender, height and weight, and body mass index; but with age such a relationship was seen only for left kidney. CONCLUSION Pakistani population has mean renal size smaller than reference values available in international literature. Renal length and volume have a direct relationship with body mass index. Mean renal size is related to the side, age, gender, height and weight as well.},
author = {Raza, Mujahid and Hameed, Amina and Khan, M Imran},
issn = {1025-9589},
journal = {J. Ayub Med. Coll. Abbottabad},
number = {3},
pages = {64--8},
pmid = {23272438},
title = {{Ultrasonographic assessment of renal size and its correlation with body mass index in adults without known renal disease.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23272438},
volume = {23}
}
@article{TIBSHIRANI1997a,
abstract = {I propose a new method for variable selection and shrinkage in Cox's proportional hazards model. My proposal minimizes the log partial likelihood subject to the sum of the absolute values of the parameters being bounded by a constant. Because of the nature of this constraint, it shrinks coefficients and produces some coefficients that are exactly zero. As a result it reduces the estimation variance while providing an interpretable final model. The method is a variation of the 'lasso' proposal of Tibshirani, designed for the linear regression context. Simulations indicate that the lasso can be more accurate than stepwise selection in this setting.},
author = {Tibshirani, Robert},
doi = {10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3},
isbn = {0277-6715 (Print)},
issn = {02776715},
journal = {Stat. Med.},
month = {feb},
number = {4},
pages = {385--395},
pmid = {9044528},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{The lasso method for variable selection in the cox model}},
url = {http://doi.wiley.com/10.1002/{\%}28SICI{\%}291097-0258{\%}2819970228{\%}2916{\%}3A4{\%}3C385{\%}3A{\%}3AAID-SIM380{\%}3E3.0.CO{\%}3B2-3},
volume = {16},
year = {1997}
}
@article{Moore2019,
abstract = {These lecture notes are intended as a supplement to Moore and Mertens' The Nature of Computation or as a standalone resource, and are available to anyone who wants to use them. Comments are welcome, and please let me know if you use these notes in a course. There are 61 exercises. I emphasize that automata are elementary playgrounds where we can explore the issues of deterministic and nondeterministic computation. Unlike P vs. NP, we can prove that nondeterminism is equivalent to determinism, or strictly more powerful than determinism, in finite-state and push-down automata respectively. I also correct several historical and aesthetic injustices: in particular, the Myhill-Nerode theorem and the idea of building minimal DFAs from equivalence classes of prefixes is restored to its rightful place above the Pumping Lemma for regular languages. I also discuss the Pumping Lemma for context-free languages, and briefly discuss counter automata, queue automata, and the connection between unambiguous context-free languages and algebraic generating functions.},
archivePrefix = {arXiv},
arxivId = {1907.12713},
author = {Moore, Cristopher},
eprint = {1907.12713},
month = {jul},
title = {{Lecture Notes on Automata, Languages, and Grammars}},
url = {http://arxiv.org/abs/1907.12713},
year = {2019}
}
@article{VanDenBurg2018,
abstract = {It is well known that data scientists spend the majority of their time on preparing data for analysis. One of the first steps in this preparation phase is to load the data from the raw storage format. Comma-separated value (CSV) files are a popular format for tabular data due to their simplicity and ostensible ease of use. However, formatting standards for CSV files are not followed consistently, so each file requires manual inspection and potentially repair before the data can be loaded, an enormous waste of human effort for a task that should be one of the simplest parts of data science. The first and most essential step in retrieving data from CSV files is deciding on the dialect of the file, such as the cell delimiter and quote character. Existing dialect detection approaches are few and non-robust. In this paper, we propose a dialect detection method based on a novel measure of data consistency of parsed data files. Our method achieves 97{\%} overall accuracy on a large corpus of real-world CSV files and improves the accuracy on messy CSV files by almost 22{\%} compared to existing approaches, including those in the Python standard library.},
archivePrefix = {arXiv},
arxivId = {1811.11242v1},
author = {{Van Den Burg}, Gerrit J J and Naz{\'{a}}bal, Alfredo and Sutton, Charles},
eprint = {1811.11242v1},
keywords = {Comma Separated Values,Data Parsing,Data Wrangling},
month = {nov},
title = {{Wrangling Messy CSV Files by Detecting Row and Type Patterns}},
url = {http://arxiv.org/abs/1811.11242 www.bing.com,search,03/02/15},
year = {2018}
}
@article{Howard2019a,
abstract = {Natural lifeforms specialise to their environmental niches across many levels; from low-level features such as DNA and proteins, through to higher-level artefacts including eyes, limbs, and overarching body plans. We propose Multi-Level Evolution (MLE), a bottom-up automatic process that designs robots across multiple levels and niches them to tasks and environmental conditions. MLE concurrently explores constituent molecular and material 'building blocks', as well as their possible assemblies into specialised morphological and sensorimotor configurations. MLE provides a route to fully harness a recent explosion in available candidate materials and ongoing advances in rapid manufacturing processes. We outline a feasible MLE architecture that realises this vision, highlight the main roadblocks and how they may be overcome, and show robotic applications to which MLE is particularly suited. By forming a research agenda to stimulate discussion between researchers in related fields, we hope to inspire the pursuit of multi-level robotic design all the way from material to machine.},
author = {Howard, David and Eiben, Agoston E. and Kennedy, Danielle Frances and Mouret, Jean-Baptiste and Valencia, Philip and Winkler, Dave},
doi = {10.1038/s42256-018-0009-9},
issn = {2522-5839},
journal = {Nat. Mach. Intell.},
keywords = {Computer science,Materials science},
month = {jan},
number = {1},
pages = {12--19},
publisher = {Nature Publishing Group},
title = {{Evolving embodied intelligence from materials to machines}},
url = {http://www.nature.com/articles/s42256-018-0009-9},
volume = {1},
year = {2019}
}
@misc{software_sqldf,
abstract = {R package version 0.4-11},
author = {Grothendieck, G.},
title = {{sqldf: Manipulate R Data Frames Using SQL}},
url = {https://cran.r-project.org/package=sqldf},
year = {2017}
}
@article{Hart2018,
abstract = {Background Children with a history of maltreatment suffer from altered emotion processing but the neural basis of this phenomenon is unknown. This pioneering functional magnetic resonance imaging (fMRI) study investigated the effects of severe childhood maltreatment on emotion processing while controlling for psychiatric conditions, medication and substance abuse. Method Twenty medication-naive, substance abuse-free adolescents with a history of childhood abuse, 20 psychiatric control adolescents matched on psychiatric diagnoses but with no maltreatment and 27 healthy controls underwent a fMRI emotion discrimination task comprising fearful, angry, sad happy and neutral dynamic facial expressions. Results Maltreated participants responded faster to fearful expressions and demonstrated hyper-activation compared to healthy controls of classical fear-processing regions of ventromedial prefrontal cortex (vmPFC) and anterior cingulate cortex, which survived at a more lenient threshold relative to psychiatric controls. Functional connectivity analysis, furthermore, demonstrated reduced connectivity between left vmPFC and insula for fear in maltreated participants compared to both healthy and psychiatric controls. Conclusions The findings show that people who have experienced childhood maltreatment have enhanced fear perception, both at the behavioural and neurofunctional levels, associated with enhanced fear-related ventromedial fronto-cingulate activation and altered functional connectivity with associated limbic regions. Furthermore, the connectivity adaptations were specific to the maltreatment rather than to the developing psychiatric conditions, whilst the functional changes were only evident at trend level when compared to psychiatric controls, suggesting a continuum. The neurofunctional hypersensitivity of fear-processing networks may be due to childhood over-exposure to fear in people who have been abused.},
author = {Rubia, K. and Hart, H. and Simmons, A. and Mehta, M. A. and Mirza, K. A. H. and Lim, L.},
doi = {10.1017/s0033291716003585},
issn = {0033-2917},
journal = {Psychol. Med.},
keywords = {Child abuse,childhood maltreatment,fear processing,functional connectivity,insula,limbic,prefrontal},
month = {may},
number = {07},
pages = {1--10},
publisher = {Cambridge University Press},
title = {{Altered fear processing in adolescents with a history of severe childhood maltreatment: an fMRI study}},
url = {https://www.cambridge.org/core/product/identifier/S0033291716003585/type/journal{\_}article},
volume = {48},
year = {2018}
}
@article{Cooper2006,
abstract = {A clonally diverse anticipatory repertoire in which each lymphocyte bears a unique antigen receptor is the central feature of the adaptive immune system that evolved in our vertebrate ancestors. The survival advantage gained through adding this type of adaptive immune system to a pre-existing innate immune system led to the evolution of alternative ways for lymphocytes to generate diverse antigen receptors for use in recognizing and repelling pathogen invaders. All jawed vertebrates assemble their antigen-receptor genes through recombinatorial rearrangement of different immunoglobulin or T cell receptor gene segments. The surviving jawless vertebrates, lampreys and hagfish, instead solved the receptor diversification problem by the recombinatorial assembly of leucine-rich-repeat genetic modules to encode variable lymphocyte receptors. The convergent evolution of these remarkably different adaptive immune systems involved innovative genetic modification of innate-immune-system components. {\textcopyright}2006 Elsevier Inc.},
author = {Cooper, Max D. and Alder, Matthew N.},
doi = {10.1016/j.cell.2006.02.001},
issn = {00928674},
journal = {Cell},
month = {feb},
number = {4},
pages = {815--822},
title = {{The evolution of adaptive immune systems}},
volume = {124},
year = {2006}
}
@article{Shen2018,
author = {Yang, Min and Du, Nan and Lei, Kai},
doi = {10.1038/ijo.2015.165},
isbn = {9781538654880},
issn = {0307-0565},
journal = {2018 IEEE Int. Conf. Bioinforma. Biomed.},
keywords = {drug representation learning,drug-drug interaction,feature processing},
month = {dec},
number = {i},
pages = {757--760},
publisher = {IEEE},
title = {{Drug2Vec : Knowledge-aware Feature-driven Method for Drug Representation Learning}},
url = {https://ieeexplore.ieee.org/document/8621390/},
year = {2018}
}
@article{Frauenberger2019,
author = {Frauenberger, Christopher and Purgathofer, Peter},
doi = {10.1145/3329674},
issn = {00010782},
journal = {Commun. ACM},
month = {jun},
number = {7},
pages = {58--64},
title = {{Ways of thinking in informatics}},
url = {http://dl.acm.org/citation.cfm?doid=3342113.3329674},
volume = {62},
year = {2019}
}
@inproceedings{Jaderberg2015,
abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
archivePrefix = {arXiv},
arxivId = {1506.02025},
author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
booktitle = {Adv. Neural Inf. Process. Syst.},
eprint = {1506.02025},
issn = {10495258},
month = {jun},
pages = {2017--2025},
title = {{Spatial transformer networks}},
url = {http://arxiv.org/abs/1506.02025},
volume = {2015-Janua},
year = {2015}
}
@article{Hawkins2018,
abstract = {How the neocortex works is a mystery. In this paper we propose a novel framework for understanding its function. Grid cells are neurons in the entorhinal cortex that represent the location of an animal in its environment. Recent evidence suggests that grid cell-like neurons may also be present in the neocortex. We propose that grid cells exist throughout the neocortex, in every region and in every cortical column. They define a location-based framework for how the neocortex functions. Whereas grid cells in the entorhinal cortex represent the location of one thing, the body relative to its environment, we propose that cortical grid cells simultaneously represent the location of many things. Cortical columns in somatosensory cortex track the location of tactile features relative to the object being touched and cortical columns in visual cortex track the location of visual features relative to the object being viewed. We propose that mechanisms in the entorhinal cortex and hippocampus that evolved for learning the structure of environments are now used by the neocortex to learn the structure of objects. Having a representation of location in each cortical column suggests mechanisms for how the neocortex represents object compositionality and object behaviors. It leads to the hypothesis that every part of the neocortex learns complete models of objects and that there are many models of each object distributed throughout the neocortex. The similarity of circuitry observed in all cortical regions is strong evidence that even high-level cognitive tasks are learned and represented in a location-based framework.},
author = {Hawkins, Jeff and Lewis, Marcus and Klukas, Mirko and Purdy, Scott and Ahmad, Subutai},
doi = {10.1101/442418},
journal = {bioRxiv},
month = {oct},
pages = {442418},
publisher = {Cold Spring Harbor Laboratory},
title = {{A Framework for Intelligence and Cortical Function Based on Grid Cells in the Neocortex}},
url = {https://www.biorxiv.org/content/10.1101/442418v1},
year = {2018}
}
@article{Arstila1999a,
abstract = {Generation and maintenance of an effective repertoire of T cell antigen receptors are essential to the immune system, yet the number of distinct T cell receptors (TCRs) expressed by the estimated 10(12) T cells in the human body is not known. In this study, TCR gene amplification and sequencing showed that there are about 10(6) different beta chains in the blood, each pairing, on the average, with at least 25 different alpha chains. In the memory subset, the diversity decreased to 1 x 10(5) to 2 x 10(5) different beta chains, each pairing with only a single alpha chain. Thus, the na{\"{i}}ve repertoire is highly diverse, whereas the memory compartment, here one-third of the T cell population, contributes less than 1 percent of the total diversity.},
author = {Arstila, T Petteri and Casrouge, Armanda and Baron, V and Even, Jos and Kanellopoulos, Jean and Kourilsky, Philippe},
doi = {10.1126/science.286.5441.958},
isbn = {0036-8075 (Print) 0036-8075 (Linking)},
issn = {00368075},
journal = {Science},
month = {oct},
number = {5441},
pages = {958--961},
pmid = {10542151},
publisher = {American Association for the Advancement of Science},
title = {{A direct estimate of the human alphabeta T cell receptor diversity.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10542151},
volume = {286},
year = {1999}
}
@article{Rudin2018,
abstract = {The authors developed and implemented transparent machine-learning models that call into question the use of black-box machine-learning models in healthcare and criminal justice applications.},
author = {Rudin, Cynthia and Ustunb, Berk},
doi = {10.1287/inte.2018.0957},
isbn = {9783642450297},
issn = {1526551X},
journal = {Interfaces (Providence).},
keywords = {Criminal justice,Healthcare,Interpretability,Machine learning,Recidivism,Scoring systems,Sparse linear models,Transparency,Trust},
month = {oct},
number = {5},
pages = {449--466},
publisher = {INFORMS},
title = {{Optimized scoring systems: Toward trust in machine learning for healthcare and criminal justice}},
url = {http://pubsonline.informs.org/doi/10.1287/inte.2018.0957},
volume = {48},
year = {2018}
}
@article{Kligyte2019,
abstract = {This article explores how transformative higher education approaches can be fostered through an integration of the concepts of third space, Students as Partners (SaP), and transdisciplinarity in practical contexts. We describe a collaborative enquiry that engaged staff and students in a reflexive dialogue centred on the concepts of mutual learning, liminality, emergence, and creativity as enacted in the curriculum of a transdisciplinary undergraduate degree, the Bachelor of Creative Intelligence and Innovation (BCII) at the University of Technology Sydney in Australia. The key insights that emerged through this enquiry were: third spaces in curriculum can be enabled but not constructed, all parties need to embrace uncertainty and a mutual learning mindset, and that “stepping in and out” of such fluid liminal spaces can stimulate creativity. Based on our experience and exploration, we offer some practical recommendations to those seeking to create similar enabling conditions for third spaces in their own undergraduate programs.},
author = {Kligyte, Giedre and Baumber, Alex and {Van der Bijl-Brouwer}, Mieke and Dowd, Cameron and Hazell, Nick and {Le Hunte}, Bem and Newton, Marcus and Roebuck, Dominica and Pratt, Susanne},
doi = {10.15173/ijsap.v3i1.3735},
issn = {2560-7367},
journal = {Int. J. Students as Partners},
keywords = {creativity,liminality,students as partners,third space,transdisciplinary},
month = {may},
number = {1},
pages = {5--21},
title = {{“Stepping in and stepping out”: Enabling creative third spaces through transdisciplinary partnerships}},
url = {https://mulpress.mcmaster.ca/ijsap/article/view/3735},
volume = {3},
year = {2019}
}
@book{Wooldridge2014,
abstract = {Europe, Middle East and Africa ed.},
author = {Wooldridge, Jeffrey M. 1960-},
isbn = {9781408093757},
publisher = {Cengage Learning},
title = {{Introduction to econometrics}},
url = {https://www.cengage.co.uk/books/9781408093757/},
year = {2014}
}
@article{Kiselev2017a,
abstract = {Single-cell RNA-seq enables the quantitative characterization of cell types based on global transcriptome profiles. We present single-cell consensus clustering (SC3), a user-friendly tool for unsupervised clustering, which achieves high accuracy and robustness by combining multiple clustering solutions through a consensus approach (http://bioconductor.org/packages/SC3). We demonstrate that SC3 is capable of identifying subclones from the transcriptomes of neoplastic cells collected from patients.},
author = {Kiselev, Vladimir Yu and Kirschner, Kristina and Schaub, Michael T. and Andrews, Tallulah and Yiu, Andrew and Chandra, Tamir and Natarajan, Kedar N. and Reik, Wolf and Barahona, Mauricio and Green, Anthony R. and Hemberg, Martin},
doi = {10.1038/nmeth.4236},
issn = {15487105},
journal = {Nat. Methods},
month = {apr},
number = {5},
pages = {483--486},
pmid = {28346451},
publisher = {Nature Publishing Group},
title = {{SC3: Consensus clustering of single-cell RNA-seq data}},
volume = {14},
year = {2017}
}
@article{He2019,
abstract = {Dietary supplements (DSs) are widely used. However, consumers know little about the safety and efficacy of DSs. There is a growing interest in accessing health information online; however, health information, especially online information on DSs, is scattered with varying levels of quality. In our previous work, we prototyped a web application, ALOHA, with interactive graph-based visualization to facilitate consumers' browsing of the integrated DIetary Supplement Knowledge base (iDISK) curated from scientific resources, following an iterative user-centered design (UCD) process. Following UCD principles, we carried out two design iterations to enrich the functionalities of ALOHA and enhance its usability. For each iteration, we conducted a usability assessment and design session with a focus group of 8–10 participants and evaluated the usability with a modified System Usability Scale (SUS). Through thematic analysis, we summarized the identified usability issues and conducted a heuristic evaluation to map them to the Gerhardt-Powals' cognitive engineering principles. We derived suggested improvements from each of the usability assessment session and enhanced ALOHA accordingly in the next design iteration. The SUS score in the second design iteration decreased to 52.2 ± 11.0 from 63.75 ± 7.2 in our original work, possibly due to the high number of new functionalities we introduced. By refining existing functionalities to make the user interface simpler, the SUS score increased to 64.4 ± 7.2 in the third design iteration. All participants agreed that such an application is urgently needed to address the gaps in how DS information is currently organized and consumed online. Moreover, most participants thought that the graph-based visualization in ALOHA is a creative and visually appealing format to obtain health information. In this study, we improved a novel interactive visualization platform, ALOHA, for the general public to obtain DS-related information through two UCD design iterations. The lessons learned from the two design iterations could serve as a guide to further enhance ALOHA and the development of other knowledge graph-based applications. Our study also showed that graph-based interactive visualization is a novel and acceptable approach to end-users who are interested in seeking online health information of various domains.},
author = {He, Xing and Zhang, Rui and Rizvi, Rubina and Vasilakes, Jake and Yang, Xi and Guo, Yi and He, Zhe and Prosperi, Mattia and Huo, Jinhai and Alpert, Jordan and Bian, Jiang},
doi = {10.1186/s12911-019-0857-1},
issn = {1472-6947},
journal = {BMC Med. Inform. Decis. Mak.},
keywords = {Health Informatics,Information Systems and Communication Service,Management of Computing and Information Systems},
month = {aug},
number = {S4},
pages = {150},
publisher = {BioMed Central},
title = {{ALOHA: developing an interactive graph-based visualization for dietary supplement knowledge graph through user-centered design}},
url = {https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-019-0857-1},
volume = {19},
year = {2019}
}
@article{Arute2019,
abstract = {This is the updated supplementary information to accompany "Quantum supremacy using a programmable superconducting processor", an article published in the October 24, 2019 issue of Nature. The main article is freely available at https://www.nature.com/articles/s41586-019-1666-5 Summary of changes relative to the supplementary information dated October 8, 2019: Ref [49] is now published; Correction of notation and definition for variational distance in Sec XI B. Correction of corresponding equation in footnote (Ref [101]); Down-sampling of images in Figs. S1 and S4 to comply with arXiv storage limit; Miscellaneous typographical corrections; Clarification in Fig. S8 caption; URL for experimental data repository added to Ref [54], https://doi.org/10.5061/dryad.k6t1rj8},
archivePrefix = {arXiv},
arxivId = {1910.11333},
author = {Arute, Frank and Arya, Kunal and Babbush, Ryan and Bacon, Dave and Bardin, Joseph C. and Barends, Rami and Biswas, Rupak and Boixo, Sergio and Brandao, Fernando G. S. L. and Buell, David A. and Burkett, Brian and Chen, Yu and Chen, Zijun and Chiaro, Ben and Collins, Roberto and Courtney, William and Dunsworth, Andrew and Farhi, Edward and Foxen, Brooks and Fowler, Austin and Gidney, Craig and Giustina, Marissa and Graff, Rob and Guerin, Keith and Habegger, Steve and Harrigan, Matthew P. and Hartmann, Michael J. and Ho, Alan and Hoffmann, Markus R. and Huang, Trent and Humble, Travis S. and Isakov, Sergei V. and Jeffrey, Evan and Jiang, Zhang and Kafri, Dvir and Kechedzhi, Kostyantyn and Kelly, Julian and Klimov, Paul V. and Knysh, Sergey and Korotkov, Alexander N. and Kostritsa, Fedor and Landhuis, David and Lindmark, Mike and Lucero, Erik and Lyakh, Dmitry and Mandra, Salvatore and McClean, Jarrod R. and McEwen, Matthew and Megrant, Anthony and Mi, Xiao and Michielsen, Kristel and Mohseni, Masoud and Mutus, Josh and Naaman, Ofer and Neeley, Matthew and Neill, Charles and Niu, Murphy Yuezhen and Ostby, Eric and Petukhov, Andre and Platt, John C. and Quintana, Chris and Rieffel, Eleanor G. and Roushan, Pedram and Rubin, Nicholas C. and Sank, Daniel and Satzinger, Kevin J. and Smelyanskiy, Vadim and Sung, Kevin J. and Trevithick, Matthew D. and Vainsencher, Amit and Villalonga, Benjamin and White, Theodore and Yao, Z. Jamie and Yeh, Ping and Zalcman, Adam and Neven, Hartmut and Martinis, John M.},
doi = {10.1038/s41586-019-1666-5},
eprint = {1910.11333},
issn = {0028-0836},
journal = {Nature},
keywords = {Quantum information,Quantum physics},
month = {oct},
number = {7779},
pages = {505--510},
publisher = {Nature Publishing Group},
title = {{Quantum supremacy using a programmable superconducting processor}},
url = {http://www.nature.com/articles/s41586-019-1666-5 http://arxiv.org/abs/1910.11333{\%}0Ahttp://dx.doi.org/10.1038/s41586-019-1666-5},
volume = {574},
year = {2019}
}
@misc{Hinton2007,
abstract = {The uniformity of the cortical architecture and the ability of functions to move to different areas of cortex following early damage strongly suggest that there is a single basic learning algorithm for extracting underlying structure from richly structured, high-dimensional sensory data. There have been many attempts to design such an algorithm, but until recently they all suffered from serious computational weaknesses. This chapter describes several of the proposed algorithms and shows how they can be combined to produce hybrid methods that work efficiently in networks with many layers and millions of adaptive connections. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
author = {Hinton, Geoffrey E.},
booktitle = {Prog. Brain Res.},
doi = {10.1016/S0079-6123(06)65034-6},
isbn = {0444528237},
issn = {00796123},
keywords = {Boltzmann machines,contrastive divergence,feature discovery,generative models,learning algorithms,multilayer neural networks,shape recognition,unsupervised learning,wake-sleep algorithm},
pages = {535--547},
title = {{To recognize shapes, first learn to generate images}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0079612306650346},
volume = {165},
year = {2007}
}
@article{Lopez2018,
abstract = {Single-cell transcriptome measurements can reveal unexplored biological diversity, but they suffer from technical noise and bias that must be modeled to account for the resulting uncertainty in downstream analyses. Here we introduce single-cell variational inference (scVI), a ready-to-use scalable framework for the probabilistic representation and analysis of gene expression in single cells ( https://github.com/YosefLab/scVI ). scVI uses stochastic optimization and deep neural networks to aggregate information across similar cells and genes and to approximate the distributions that underlie observed expression values, while accounting for batch effects and limited sensitivity. We used scVI for a range of fundamental analysis tasks including batch correction, visualization, clustering, and differential expression, and achieved high accuracy for each task.},
author = {Lopez, Romain and Regier, Jeffrey and Cole, Michael B. and Jordan, Michael I. and Yosef, Nir},
doi = {10.1038/s41592-018-0229-2},
issn = {15487105},
journal = {Nat. Methods},
keywords = {Computational biology and bioinformatics,Computational models},
month = {dec},
number = {12},
pages = {1053--1058},
pmid = {30504886},
publisher = {Nature Publishing Group},
title = {{Deep generative modeling for single-cell transcriptomics}},
url = {http://www.nature.com/articles/s41592-018-0229-2},
volume = {15},
year = {2018}
}
@article{Zolezzi2018,
abstract = {Introduction: Lithium is commonly used to treat various psychiatric disorders and is particularly effective in the maintenance phase of bipolar disorder. Unfortunately, this drug has a narrow therapeutic index and, if not monitored regularly, can result in toxicity. Therefore, for lithium to be prescribed safely, clinicians must ensure that patients are well educated on lithium toxicity, its prevention, and symptom recognition. This article summarizes studies that investigated lithium education strategies to help promote the safe use of lithium. Methods: Four electronic databases were searched using key terms and subject headings. Reference lists of relevant papers were also reviewed. The search was limited to literature published in English, without year limits. Eligible studies examined lithium patient education and the impact on patients' knowledge of safe lithium use. Results: Of a total of 517 citations that were retrieved from the electronic database search, 12 were selected for inclusion in this review. Most of the studies included assessed the effect of lithium education on various aspects of patients' knowledge, including but not limited to, lithium toxicity. Of the studies assessing the correlation between lithium knowledge and lithium levels, most demonstrated a positive correlation between lithium knowledge and lithium levels that were more stable and within the higher end of the recommended therapeutic range. Conclusions: Studies evaluating lithium patient education and its effect on improving the safe use of lithium are limited. Nevertheless, this literature review highlights that lithium patient education is critical to promote its safe use.},
author = {Zolezzi, Monica and Eltorki, Yassin Hassan and Almaamoon, Mahmoud and Fathy, Mahmoud and Omar, Nabil E.},
doi = {10.9740/mhc.2018.01.041},
issn = {2168-9709},
journal = {Ment. Heal. Clin.},
keywords = {lithium patient education,lithium safety outcomes,lithium toxicity},
month = {jan},
number = {1},
pages = {41--48},
pmid = {29955544},
title = {{Outcomes of patient education practices to optimize the safe use of lithium: A literature review}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/29955544 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC6007520 http://mhc.cpnp.org/doi/10.9740/mhc.2018.01.041},
volume = {8},
year = {2018}
}
@misc{umap_github_visualization_page,
title = {{Understanding UMAP}},
url = {https://pair-code.github.io/understanding-umap/},
urldate = {2019-11-06}
}
@inproceedings{Karazija2018,
abstract = {This paper introduces a way to learn cross-modal convolutional neural network (X-CNN) architectures from a base convolutional network (CNN) and the training data to reduce the design cost and enable applying cross-modal networks in sparse data environments. Two approaches for building X-CNNs are presented. The base approach learns the topology in a data-driven manner, by using measurements performed on the base CNN and supplied data. The iterative approach performs further optimisation of the topology through a combined learning procedure, simultaneously learning the topology and training the network. The approaches were evaluated agains examples of hand-designed X-CNNs and their base variants, showing superior performance and, in some cases, gaining an additional 9{\%} of accuracy. From further considerations, we conclude that the presented methodology takes less time than any manual approach would, whilst also significantly reducing the design complexity. The application of the methods is fully automated and implemented in Xsertion library.},
archivePrefix = {arXiv},
arxivId = {arXiv:1805.00987v1},
author = {Karazija, Laurynas and Veli{\v{c}}kovi{\'{c}}, Petar and Li{\`{o}}, Pietro},
booktitle = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
doi = {10.1007/978-3-319-92537-0_7},
eprint = {arXiv:1805.00987v1},
isbn = {9783319925363},
issn = {16113349},
keywords = {Deep learning,Evolutionary neural networks,Model selection and structure learning,Optimisation algorithms},
month = {may},
pages = {54--63},
title = {{Automatic inference of cross-modal connection topologies for X-CNNs}},
url = {http://arxiv.org/abs/1805.00987},
volume = {10878 LNCS},
year = {2018}
}
@article{Davis1998,
abstract = {The "Naive Physics Manifesto" of Pat Hayes (1978) proposes a large-scale project to develop a formal theory encompassing the entire knowledge of physics of naive reasoners, expressed in a declarative symbolic form. The theory is organized in clusters of closely interconnected concepts and axioms. More recent work on the representation of commonsense physical knowledge has followed a somewhat different methodology. The goal has been to develop a competence theory powerful enough to justify commonsense physical inferences, and the research is organized in micro-worlds, each microworld covering a small range of physical phenomena. In this article, I compare the advantages and disadvantages of the two approaches.},
author = {Davis, Ernest},
doi = {10.1609/AIMAG.V19I4.1424},
issn = {07384602},
journal = {AI Mag.},
month = {dec},
number = {4},
pages = {51--79},
title = {{The Naive Physics Perplex}},
url = {https://www.aaai.org/ojs/index.php/aimagazine/article/view/1424},
volume = {19},
year = {1998}
}
@article{DiPatti2008,
abstract = {In this work we propose a mathematical model for the kinetics of tramadol, a synthetic opioid commonly used for treating moderate to severe pain. This novel theoretical framework could result in an objective criterion on how to adjust the assigned dose, depending on the genetic polymorphisms of CYP2D6. The model describes the coupled dynamics of tramadol and the metabolite O-desmethyltramadol. The effect of diffusion of the drug in the blood is here accounted for and we further hypothesize the existence of a time delay in the process of chemical translation from tramadol into metabolites. The system of coupled differential equations is solved numerically and the free parameters adjusted so to interpolate the experimental time series for the intravenous injection setting. Theoretical curves are shown to reproduce correctly the experimental profiles obtained from clinical trials. This enables in turn to extract an estimate of the metabolization rate. A difference in metabolization rate between CYP2D6 poor and extensive metabolizers is also found, and the stereoselectivity in the O-demethylation of tramadol highlighted. Our results allow one to quantify the dose of (+)-tramadol (resp. (-)-tramadol) administered to poor or extensive metabolizers, if the same effect is sought. The latter is here quantified through the blood concentration of (+)-metabolites (resp. (-)-metabolites). {\textcopyright} 2008 Elsevier Ltd. All rights reserved.},
author = {{Di Patti}, F. and Fanelli, D. and Pedersen, R. S. and Giuliani, C. and Torricelli, F.},
doi = {10.1016/j.jtbi.2008.06.005},
issn = {00225193},
journal = {J. Theor. Biol.},
keywords = {Drug metabolism,Genetic polymorphisms,O-desmethyltramadol,Pain therapy and experiments,Personalized medicine},
month = {oct},
number = {3},
pages = {568--574},
pmid = {18640130},
title = {{Modelling the pharmacokinetics of tramadol: On the difference between CYP2D6 extensive and poor metabolizers}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18640130 https://linkinghub.elsevier.com/retrieve/pii/S0022519308003068},
volume = {254},
year = {2008}
}
@article{Chevalier-Boisvert2018,
abstract = {Allowing humans to interactively train artificial agents to understand language instructions is desirable for both practical and scientific reasons, but given the poor data efficiency of the current learning methods, this goal may require substantial research efforts. Here, we introduce the BabyAI research platform to support investigations towards including humans in the loop for grounded language learning. The BabyAI platform comprises an extensible suite of 19 levels of increasing difficulty. The levels gradually lead the agent towards acquiring a combinatorially rich synthetic language which is a proper subset of English. The platform also provides a heuristic expert agent for the purpose of simulating a human teacher. We report baseline results and estimate the amount of human involvement that would be required to train a neural network-based agent on some of the BabyAI levels. We put forward strong evidence that current deep learning methods are not yet sufficiently sample efficient when it comes to learning a language with compositional properties.},
archivePrefix = {arXiv},
arxivId = {1810.08272},
author = {Chevalier-Boisvert, Maxime and Bahdanau, Dzmitry and Lahlou, Salem and Willems, Lucas and Saharia, Chitwan and Nguyen, Thien Huu and Bengio, Yoshua},
eprint = {1810.08272},
month = {oct},
title = {{BabyAI: First Steps Towards Grounded Language Learning With a Human In the Loop}},
url = {http://arxiv.org/abs/1810.08272},
year = {2018}
}
@misc{software_pheatmap,
author = {Kolde, Raivo},
title = {{pheatmap: Pretty Heatmaps}},
url = {https://cran.r-project.org/package=pheatmap},
year = {2018}
}
@article{Ustun2016a,
abstract = {Risk scores are simple classification models that let users make quick risk predictions by adding and subtracting a few small numbers. These models are widely used in medicine and criminal justice, but are difficult to learn from data because they need to be calibrated, sparse, use small integer coefficients and obey application-specific constraints. In this paper, we present a new machine learning approach to learn risk scores. We formulate the risk score problem as a mixed integer nonlinear program, and present a new cutting plane algorithm for non-convex settings to efficiently recover its optimal solution. We improve our algorithm with specialized techniques to generate feasible solutions, narrow the optimality gap, and reduce data-related computation. Our approach can fit risk scores in a way that scales linearly in the number of samples, provides a certificate of optimality, and obeys real-world constraints without parameter tuning or post-processing. We illustrate the performance benefits of this approach through an extensive set of numerical experiments, where we compare risk scores built using our approach to those built using heuristic approaches. We also discuss the practical benefits of our approach through an application where we build a customized risk score for ICU seizure prediction in collaboration with the Massachusetts General Hospital.},
archivePrefix = {arXiv},
arxivId = {1610.00168},
author = {Ustun, Berk and Rudin, Cynthia},
doi = {10.1145/3097983.3098161},
eprint = {1610.00168},
isbn = {9781450348874},
month = {oct},
title = {{Learning Optimized Risk Scores}},
url = {http://arxiv.org/abs/1610.00168},
year = {2016}
}
@inproceedings{Parmar2019,
abstract = {The analysis of biological networks is characterized by the definition of precise linear constraints used to cumulatively reduce the solution space of the computed states of a multi-omic (for instance metabolic, transcriptomic and proteomic) model. In this paper, we attempt, for the first time, to combine metabolic modelling and networked Cox regression, using the metabolic model of the bacterium Helicobacter Pylori. This enables a platform both for quantitative analysis of networked regression, but also testing the findings from network regression (a list of significant vectors and their networked relationships) on in vivo transcriptomic data. Data generated from the model, using flux balance analysis to construct a Pareto front, specifically, a trade-off of Oxygen exchange and growth rate and a trade-off of Carbon Dioxide exchange and growth rate, is analysed and then the model is used to quantify the success of the analysis. It was found that using the analysis, reconstruction of the initial data was considerably more successful than a pure noise alternative. Our methodological approach is quite general and it could be of interest for the wider community of complex networks researchers; it is implemented in a software tool, MoNeRe, which is freely available through the Github platform.},
author = {Parmar, Vandan and Li{\'{o}}, Pietro},
booktitle = {Stud. Comput. Intell.},
doi = {10.1007/978-3-030-05414-4_49},
isbn = {9783030054137},
issn = {1860949X},
month = {dec},
pages = {611--624},
publisher = {Springer, Cham},
title = {{Multi-omic network regression: Methodology, tool and case study}},
url = {http://link.springer.com/10.1007/978-3-030-05414-4{\_}49},
volume = {813},
year = {2019}
}
@article{Poust1976a,
abstract = {The time course of lithium concentration in plasma and RBCs was measured in normal adult males following administration of single and multiple doses of lithium carbonate. From the single dose profiles, it was determined that lithium distribution between plasma and RBCs is not a simple partitioning phenomenon. The single dose time course measurements in both blood components were fit to a two compartment pharmacokinetic model in which the plasma was representative of the central compartment and the RBCs were representative of the tissue compartment. The importance of correction for trapped plasma volume in studies measuring lithium RBC kinetics was emphasized. In this study it was also demonstrated that one can accurately predict plasma and RBC lithium concentrations which are observed following multiple dosing, on the basis of single dose parameters obtained in the same subject.},
author = {Poust, R I and Mallinger, A G and Mallinger, J and Himmelhoch, J M and Hanin, I},
issn = {0098-616X},
journal = {Psychopharmacol. Commun.},
number = {2},
pages = {91--103},
pmid = {981712},
title = {{Pharmacokinetics of lithium in human plasma and erythrocytes.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/981712},
volume = {2},
year = {1976}
}
@article{Bair2004,
abstract = {An important goal of DNA microarray research is to develop tools to diagnose cancer more accurately based on the genetic profile of a tumor. There are several existing techniques in the literature for performing this type of diagnosis. Unfortunately, most of these techniques assume that different subtypes of cancer are already known to exist. Their utility is limited when such subtypes have not been previously identified. Although methods for identifying such subtypes exist, these methods do not work well for all datasets. It would be desirable to develop a procedure to find such subtypes that is applicable in a wide variety of circumstances. Even if no information is known about possible subtypes of a certain form of cancer, clinical information about the patients, such as their survival time, is often available. In this study, we develop some procedures that utilize both the gene expression data and the clinical data to identify subtypes of cancer and use this knowledge to diagnose future patients. These procedures were successfully applied to several publicly available datasets. We present diagnostic procedures that accurately predict the survival of future patients based on the gene expression profile and survival times of previous patients. This has the potential to be a powerful tool for diagnosing and treating cancer.},
author = {Bair, Eric and Tibshirani, Robert},
doi = {10.1371/journal.pbio.0020108},
editor = {{Todd Golub}},
isbn = {1544-9173},
issn = {15449173},
journal = {PLoS Biol.},
month = {apr},
number = {4},
pages = {e108},
pmid = {15094809},
publisher = {Public Library of Science},
title = {{Semi-supervised methods to predict patient survival from gene expression data}},
url = {http://dx.plos.org/10.1371/journal.pbio.0020108},
volume = {2},
year = {2004}
}
@article{Bhattacharya2018d,
abstract = {Patients associated with multiple co-occurring health conditions often face aggravated complications and less favorable outcomes. Co-occurring conditions are especially prevalent among individuals suffering from kidney disease, an increasingly widespread condition affecting 13{\%} of the general population in the US. This study aims to identify and characterize patterns of co-occurring medical conditions in patients employing a probabilistic framework. Specifically, we apply topic modeling in a non-traditional way to find associations across SNOMED-CT codes assigned and recorded in the EHRs of {\textgreater}13,000 patients diagnosed with kidney disease. Unlike most prior work on topic modeling, we apply the method to codes rather than to natural language. Moreover, we quantitatively evaluate the topics, assessing their tightness and distinctiveness, and also assess the medical validity of our results. Our experiments show that each topic is succinctly characterized by a few highly probable and unique disease codes, indicating that the topics are tight. Furthermore, inter-topic distance between each pair of topics is typically high, illustrating distinctiveness. Last, most coded conditions grouped together within a topic, are indeed reported to co-occur in the medical literature. Notably, our results uncover a few indirect associations among conditions that have hitherto not been reported as correlated in the medical literature.},
author = {Bhattacharya, Moumita and Jurkovitz, Claudine and Shatkay, Hagit},
doi = {10.1016/j.jbi.2018.04.008},
issn = {15320464},
journal = {J. Biomed. Inform.},
keywords = {Co-occurring medical conditions,Electronic health records,SNOMED-CT codes,Topic modeling},
month = {jun},
pages = {31--40},
publisher = {Academic Press},
title = {{Co-occurrence of medical conditions: Exposing patterns through probabilistic topic modeling of snomed codes}},
url = {https://www.sciencedirect.com/science/article/pii/S1532046418300728},
volume = {82},
year = {2018}
}
@article{Kriegman2020,
abstract = {Living systems are more robust, diverse, complex, and supportive of human life than any technology yet created. However, our ability to create novel lifeforms is currently limited to varying existing organisms or bioengineering organoids in vitro. Here we show a scalable pipeline for creating functional novel lifeforms: AI methods automatically design diverse candidate lifeforms in silico to perform some desired function, and transferable designs are then created using a cell-based construction toolkit to realize living systems with the predicted behaviors. Although some steps in this pipeline still require manual intervention, complete automation in future would pave the way to designing and deploying unique, bespoke living systems for a wide range of functions.},
author = {Kriegman, Sam and Blackiston, Douglas and Levin, Michael and Bongard, Josh},
doi = {10.1073/pnas.1910837117},
issn = {1091-6490},
journal = {Proc. Natl. Acad. Sci. U. S. A.},
month = {jan},
pmid = {31932426},
title = {{A scalable pipeline for designing reconfigurable organisms.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/31932426},
year = {2020}
}
@misc{article_graph_deep_learning,
title = {{Graph deep learning}},
url = {https://towardsdatascience.com/kegra-deep-learning-on-knowledge-graphs-with-keras-98e340488b93},
urldate = {2019-03-05},
year = {2012}
}
@article{Sokol2020,
author = {Sokol, Kacper and Flach, Peter},
doi = {10.1007/s13218-020-00637-y},
issn = {0933-1875},
journal = {KI - K{\"{u}}nstliche Intelligenz},
month = {feb},
pages = {1--16},
publisher = {Springer Berlin Heidelberg},
title = {{One Explanation Does Not Fit All}},
url = {http://link.springer.com/10.1007/s13218-020-00637-y},
year = {2020}
}
@article{Nichele2017,
abstract = {Recurrent neural networks (RNNs) have been a prominent concept within artificial intelligence. They are inspired by biological neural networks (BNNs) and provide an intuitive and abstract representation of how BNNs work. Derived from the more generic artificial neural networks (ANNs), the recurrent ones are meant to be used for temporal tasks, such as speech recognition, because they are capable of memorizing historic input. However, such networks are very time consuming to train as a result of their inherent nature. Recently, echo state networks and liquid state machines have been proposed as possible RNN alternatives, under the name of reservoir computing (RC). Reservoir computers are far easier to train. In this paper, cellular automata (CAs) are used as a reservoir and are tested on the five-bit memory task (a well-known benchmark within the RC community). The work herein provides a method of mapping binary inputs from the task onto the automata and a recurrent architecture for handling the sequential aspects. Furthermore, a layered (deep) reservoir architecture is proposed. Performances are compared to earlier work, in addition to the single-layer version. Results show that the single cellular automaton (CA) reservoir system yields similar results to state-of-the-art work. The system comprised of two layered reservoirs does show a noticeable improvement compared to a single CA reservoir. This work lays the foundation for implementations of deep learning with CA-based reservoir systems.},
author = {Nichele, Stefano and Molund, Andreas},
doi = {10.25088/ComplexSystems.26.4.319},
issn = {08912513},
journal = {Complex Syst.},
month = {dec},
number = {4},
pages = {319--340},
title = {{Deep learning with cellular automaton-based reservoir computing}},
url = {http://www.complex-systems.com/abstracts/v26{\_}i04{\_}a02/},
volume = {26},
year = {2017}
}
@article{Ishwaran2005,
author = {Ishwaran, Hemant and Rao, J. Sunil},
doi = {10.1214/009053604000001147},
isbn = {0090-5364},
issn = {0090-5364},
journal = {Ann. Stat.},
keywords = {Generalized ridge regression,Zcut,hypervariance,model averaging,model uncertainty,ordinary least squares,penalization,rescaling,shrinkage,stochastic variable selection},
month = {apr},
number = {2},
pages = {730--773},
publisher = {Institute of Mathematical Statistics},
title = {{Spike and slab variable selection: Frequentist and Bayesian strategies}},
url = {http://projecteuclid.org/euclid.aos/1117114335},
volume = {33},
year = {2005}
}
@article{Chen2018d,
abstract = {OBJECTIVES To examine the association between temperature and cause specific mortality, and to quantify the corresponding disease burden attributable to non-optimum ambient temperatures. DESIGN Time series analysis. SETTING 272 main cities in China. POPULATION Non-accidental deaths in 272 cities covered by the Disease Surveillance Point System of China, from January 2013 to December 2015. MAIN OUTCOMES AND MEASURES Daily numbers of deaths from all non-accidental causes and main cardiorespiratory diseases. Potential effect modifiers included demographic, climatic, geographical, and socioeconomic characteristics. The analysis used distributed lag non-linear models to estimate city specific associations, and multivariate meta-regression analysis to obtain the effect estimates at national and regional levels. RESULTS 1 826 186 non-accidental deaths from total causes were recorded in the study period. Temperature and mortality consistently showed inversely J shaped associations. At the national average level, relative to the minimum mortality temperature (22.8°C, 79.1st centile), the mortality risk of extreme cold temperature (at -1.4°C, the 2.5th centile) lasted for more than 14 days, whereas the risk of extreme hot temperature (at 29.0°C, the 97.5th centile) appeared immediately and lasted for two to three days. 14.33{\%} of non-accidental total mortality was attributable to non-optimum temperatures, of which moderate cold (ranging from -1.4 to 22.8°C), moderate heat (22.8 to 29.0°C), extreme cold (-6.4 to -1.4°C), and extreme heat (29.0 to 31.6°C) temperatures corresponded to attributable fractions of 10.49{\%}, 2.08{\%}, 1.14{\%}, and 0.63{\%}, respectively. The attributable fractions were 17.48{\%} for overall cardiovascular disease, 18.76{\%} for coronary heart disease, 16.11{\%} for overall stroke, 14.09{\%} for ischaemic stroke, 18.10{\%} for haemorrhagic stroke, 10.57{\%} for overall respiratory disease, and 12.57{\%} for chronic obstructive pulmonary diseases. The mortality risk and burden were more prominent in the temperate monsoon and subtropical monsoon climatic zones, in specific subgroups (female sex, age ≥75 years, and ≤9 years spent in education), and in cities characterised by higher urbanisations rates and shorter durations of central heating. CONCLUSIONS This nationwide study provides a comprehensive picture of the non-linear associations between ambient temperature and mortality from all natural causes and main cardiorespiratory diseases, as well as the corresponding disease burden that is mainly attributable to moderate cold temperatures in China. The findings on vulnerability characteristics can help improve clinical and public health practices to reduce disease burden associated with current and future abnormal weather.},
author = {Chen, Renjie and Yin, Peng and Wang, Lijun and Liu, Cong and Niu, Yue and Wang, Weidong and Jiang, Yixuan and Liu, Yunning and Liu, Jiangmei and Qi, Jinlei and You, Jinling and Kan, Haidong and Zhou, Maigeng},
doi = {10.1136/bmj.k4306},
isbn = {0-7695-1950-4},
issn = {17561833},
journal = {BMJ},
month = {oct},
pages = {k4306},
pmid = {30381293},
publisher = {British Medical Journal Publishing Group},
title = {{Association between ambient temperature and mortality risk and burden: Time series study in 272 main Chinese cities}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/30381293 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC6207921},
volume = {363},
year = {2018}
}
@inproceedings{KleerJohande,
author = {de Kleer, Johan and Brown, John Seely},
isbn = {0865760438},
pages = {424--437},
title = {{FOUNDATIONS OF ENVISIONING.}},
url = {https://www.aaai.org/Library/AAAI/1982/aaai82-104.php},
year = {1982}
}
@article{Wibisono2016,
abstract = {Accelerated gradient methods play a central role in optimization, achieving optimal rates in many settings. While many generalizations and extensions of Nesterov's original acceleration method have been proposed, it is not yet clear what is the natural scope of the acceleration concept. In this paper, we study accelerated methods from a continuous-time perspective. We show that there is a Lagrangian functional that we call the $\backslash$emph{\{}Bregman Lagrangian{\}} which generates a large class of accelerated methods in continuous time, including (but not limited to) accelerated gradient descent, its non-Euclidean extension, and accelerated higher-order gradient methods. We show that the continuous-time limit of all of these methods correspond to traveling the same curve in spacetime at different speeds. From this perspective, Nesterov's technique and many of its generalizations can be viewed as a systematic way to go from the continuous-time curves generated by the Bregman Lagrangian to a family of discrete-time accelerated algorithms.},
archivePrefix = {arXiv},
arxivId = {1603.04245},
author = {Wibisono, Andre and Wilson, Ashia C and Jordan, Michael I},
doi = {10.1073/pnas.1614734113},
eprint = {1603.04245},
isbn = {9781615679119},
issn = {0027-8424},
journal = {Proc. Natl. Acad. Sci. U. S. A.},
keywords = {Bregman divergence,Lagrangian framework,accelerated methods,convex optimization,mirror descent},
month = {nov},
number = {47},
pages = {E7351--E7358},
pmid = {25246403},
publisher = {National Academy of Sciences},
title = {{A Variational Perspective on Accelerated Methods in Optimization}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27834219 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5127379 http://arxiv.org/abs/1603.04245},
volume = {113},
year = {2016}
}
@article{Swaminathan2019,
abstract = {Summary Background Malnutrition is a major contributor to disease burden in India. To inform subnational action, we aimed to assess the disease burden due to malnutrition and the trends in its indicators in every state of India in relation to Indian and global nutrition targets. Methods We analysed the disease burden attributable to child and maternal malnutrition, and the trends in the malnutrition indicators from 1990 to 2017 in every state of India using all accessible data from multiple sources, as part of Global Burden of Diseases, Injuries, and Risk Factors Study (GBD) 2017. The states were categorised into three groups using their Socio-demographic Index (SDI) calculated by GBD on the basis of per capita income, mean education, and fertility rate in women younger than 25 years. We projected the prevalence of malnutrition indicators for the states of India up to 2030 on the basis of the 1990–2017 trends for comparison with India National Nutrition Mission (NNM) 2022 and WHO and UNICEF 2030 targets. Findings Malnutrition was the predominant risk factor for death in children younger than 5 years of age in every state of India in 2017, accounting for 68{\textperiodcentered}2{\%} (95{\%} UI 65{\textperiodcentered}8–70{\textperiodcentered}7) of the total under-5 deaths, and the leading risk factor for health loss for all ages, responsible for 17{\textperiodcentered}3{\%} (16{\textperiodcentered}3–18{\textperiodcentered}2) of the total disability-adjusted life years (DALYs). The malnutrition DALY rate was much higher in the low SDI than in the middle SDI and high SDI state groups. This rate varied 6{\textperiodcentered}8 times between the states in 2017, and was highest in the states of Uttar Pradesh, Bihar, Assam, and Rajasthan. The prevalence of low birthweight in India in 2017 was 21{\textperiodcentered}4{\%} (20{\textperiodcentered}8–21{\textperiodcentered}9), child stunting 39{\textperiodcentered}3{\%} (38{\textperiodcentered}7–40{\textperiodcentered}1), child wasting 15{\textperiodcentered}7{\%} (15{\textperiodcentered}6–15{\textperiodcentered}9), child underweight 32{\textperiodcentered}7{\%} (32{\textperiodcentered}3–33{\textperiodcentered}1), anaemia in children 59{\textperiodcentered}7{\%} (56{\textperiodcentered}2–63{\textperiodcentered}8), anaemia in women 15–49 years of age 54{\textperiodcentered}4{\%} (53{\textperiodcentered}7–55{\textperiodcentered}2), exclusive breastfeeding 53{\textperiodcentered}3{\%} (51{\textperiodcentered}5–54{\textperiodcentered}9), and child overweight 11{\textperiodcentered}5{\%} (8{\textperiodcentered}5–14{\textperiodcentered}9). If the trends estimated up to 2017 for the indicators in the NNM 2022 continue in India, there would be 8{\textperiodcentered}9{\%} excess prevalence for low birthweight, 9{\textperiodcentered}6{\%} for stunting, 4{\textperiodcentered}8{\%} for underweight, 11{\textperiodcentered}7{\%} for anaemia in children, and 13{\textperiodcentered}8{\%} for anaemia in women relative to the 2022 targets. For the additional indicators in the WHO and UNICEF 2030 targets, the trends up to 2017 would lead to 10{\textperiodcentered}4{\%} excess prevalence for wasting, 14{\textperiodcentered}5{\%} excess prevalence for overweight, and 10{\textperiodcentered}7{\%} less exclusive breastfeeding in 2030. The prevalence of malnutrition indicators, their rates of improvement, and the gaps between projected prevalance and targets vary substantially between the states. Interpretation Malnutrition continues to be the leading risk factor for disease burden in India. It is encouraging that India has set ambitious targets to reduce malnutrition through NNM. The trends up to 2017 indicate that substantially higher rates of improvement will be needed for all malnutrition indicators in most states to achieve the Indian 2022 and the global 2030 targets. The state-specific findings in this report indicate the effort needed in each state, which will be useful in tracking and motivating further progress. Similar subnational analyses might be useful for other low-income and middle-income countries. Funding Bill {\&} Melinda Gates Foundation; Indian Council of Medical Research, Department of Health Research, Ministry of Health and Family Welfare, Government of India.},
author = {Swaminathan, Soumya and Hemalatha, Rajkumar and Pandey, Anamika and Kassebaum, Nicholas J and Laxmaiah, Avula and Longvah, Thingnganing and Lodha, Rakesh and Ramji, Siddarth and Kumar, G Anil and Afshin, Ashkan and Gupta, Subodh S and Kar, Anita and Khera, Ajay K and Mathai, Matthews and Awasthi, Shally and Rasaily, Reeta and Varghese, Chris M and Millear, Anoushka I and Manguerra, Helena and Gardner, William M and Sorenson, Reed and Sankar, Mari J and Purwar, Manorama and Furtado, Melissa and Bansal, Priyanka G and Barber, Ryan and Chakma, Joy K and Chalek, Julian and Dwivedi, Supriya and Fullman, Nancy and Ginnela, Brahmam N and Glenn, Scott D and Godwin, William and Gonmei, Zaozianlungliu and Gupta, Rachita and Jerath, Suparna G and Kant, Rajni and Krish, Varsha and Kumar, Rachakulla H and Ladusingh, Laishram and Meshram, Indrapal I and Mutreja, Parul and Nagalla, Balakrishna and Nimmathota, Arlappa and Odell, Christopher M and Olsen, Helen E and Pati, Ashalata and Pickering, Brandon and Radhakrishna, Kankipati V and Raina, Neena and Rankin, Zane and Saraf, Deepika and Sharma, R S and Sinha, Anju and Varanasi, Bhaskar and Shekhar, Chander and Bekedam, Hendrik J and Reddy, K Srinath and Lim, Stephen S and Hay, Simon I and Dandona, Rakhi and Murray, Christopher J L and Toteja, G S and Dandona, Lalit},
doi = {10.1016/S2352-4642(19)30273-1},
issn = {23524642},
journal = {Lancet Child Adolesc. Heal.},
month = {sep},
number = {0},
publisher = {Elsevier},
title = {{The burden of child and maternal malnutrition and trends in its indicators in the states of India: the Global Burden of Disease Study 1990–2017}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S2352464219302731},
volume = {0},
year = {2019}
}
@article{Ting2018,
abstract = {We develop theory for nonlinear dimensionality reduction (NLDR). A number of NLDR methods have been developed, but there is limited understanding of how these methods work and the relationships between them. There is limited basis for using existing NLDR theory for deriving new algorithms. We provide a novel framework for analysis of NLDR via a connection to the statistical theory of linear smoothers. This allows us to both understand existing methods and derive new ones. We use this connection to smoothing to show that asymptotically, existing NLDR methods correspond to discrete approximations of the solutions of sets of differential equations given a boundary condition. In particular, we can characterize many existing methods in terms of just three limiting differential operators and boundary conditions. Our theory also provides a way to assert that one method is preferable to another; indeed, we show Local Tangent Space Alignment is superior within a class of methods that assume a global coordinate chart defines an isometric embedding of the manifold.},
archivePrefix = {arXiv},
arxivId = {1803.02432},
author = {Ting, Daniel and Jordan, Michael I.},
eprint = {1803.02432},
month = {mar},
title = {{On Nonlinear Dimensionality Reduction, Linear Smoothing and Autoencoding}},
url = {http://arxiv.org/abs/1803.02432},
year = {2018}
}
@article{Gaujoux2013,
abstract = {Summary: Gene expression data are typically generated from heterogeneous biological samples that are composed of multiple cell or tissue types, in varying proportions, each contributing to global gene expression. This heterogeneity is a major confounder in standard analysis such as differential expression analysis, where differences in the relative proportions of the constituent cells may prevent or bias the detection of cell-specific differences. Computational deconvolution of global gene expression is an appealing alternative to costly physical sample separation techniques and enables a more detailed analysis of the underlying biological processes at the cell-type level. To facilitate and popularize the application of such methods, we developed CellMix, an R package that incorporates most state-of-the-art deconvolution methods, into an intuitive and extendible framework, providing a single entry point to explore, assess and disentangle gene expression data from heterogeneous samples.$\backslash$nAvailability and Implementation: The CellMix package builds on R/BioConductor and is available from http://web.cbio.uct.ac.za/∼renaud/CRAN/web/CellMix. It is currently being submitted to BioConductor. The package's vignettes notably contain additional information, examples and references.$\backslash$nContact: renaud@cbio.uct.ac.za},
author = {Gaujoux, Renaud and Seoighe, Cathal},
doi = {10.1093/bioinformatics/btt351},
isbn = {1367-4811 (Electronic)$\backslash$r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
month = {sep},
number = {17},
pages = {2211--2212},
pmid = {23825367},
title = {{CellMix: A comprehensive toolbox for gene expression deconvolution}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23825367 https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btt351},
volume = {29},
year = {2013}
}
@book{Gareth2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Gareth, James and Daniela, Witten and Trevor, Hastie and Robert, Tibshirani},
doi = {10.1016/j.peva.2007.06.006},
eprint = {arXiv:1011.1669v3},
isbn = {9780387781884},
issn = {01665316},
pmid = {10911016},
title = {{Introduction to statistical learning}},
url = {http://faculty.marshall.usc.edu/gareth-james/ISL/},
year = {2015}
}
@article{Neufeld2017a,
abstract = {{\textcopyright} 2017, {\textcopyright} Sharon A.S. Neufeld, Peter B. Jones and Ian M. Goodyer. Purpose: The purpose of this paper is to expand upon policy implications of a recent study assessing adolescent mental health service contact and subsequent depression. Design/methodology/approach: Review of related evidence from academic and grey literature. Findings: Studies assessing the role of mental health services in reducing mental disorder during adolescence are sparse, and even prevalence figures for adolescent mental disorders are out-of-date. Adolescent mental health service contact rates are shown to fall concurrent with budgetary decreases. School-based counselling is highlighted as an important source of help that may be at risk of being cut. Increased training of General Practitioners and school counsellors is needed to improve efficiency in specialist Child and Adolescent Mental Health Services (CAMHS). Practical implications: Longitudinal studies of young people's mental health should include mental health service usage and its relationship with subsequent mental health outcomes. Social implications: Funding cuts to CAMHS must be avoided, school-based counselling must be protected, and service referrers should be better trained. Originality/value: This paper highlights the need for increased CAMHS data, sustained funding, and improved training for this vital service.},
author = {Neufeld, Sharon A.S. and Jones, Peter B. and Goodyer, Ian M.},
doi = {10.1108/JPMH-03-2017-0013},
issn = {20428731},
journal = {J. Public Ment. Health},
keywords = {Child and adolescent mental health services,Community interventions,Depression,Policy},
month = {sep},
number = {3},
pages = {96--99},
publisher = {Emerald Publishing Limited},
title = {{Child and adolescent mental health services: longitudinal data sheds light on current policy for psychological interventions in the community}},
url = {http://www.emeraldinsight.com/doi/10.1108/JPMH-03-2017-0013},
volume = {16},
year = {2017}
}
@article{Mueller2006,
abstract = {We compare the event calculus and temporal action logics (TAL), two formalisms for reasoning about action and change. We prove that, if the formalisms are restricted to integer time, inertial fluents, and relational fluents, and if TAL action type specifications are restricted to definite reassignment of a single fluent, then the formalisms are not equivalent. We argue that equivalence cannot be restored by using more general TAL action type specifications. We prove however that, if the formalisms are further restricted to single-step actions, then they are logically equivalent. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
author = {Mueller, Erik T.},
doi = {10.1016/j.artint.2006.05.001},
issn = {00043702},
journal = {Artif. Intell.},
keywords = {Commonsense reasoning,Event calculus,Reasoning about action and change,Temporal action logics (TAL)},
month = {aug},
number = {11},
pages = {1017--1029},
publisher = {Elsevier Science Publishers Ltd.},
title = {{Event calculus and temporal action logics compared}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370206000567},
volume = {170},
year = {2006}
}
@article{Pirro2019,
author = {Pirro, Laura},
doi = {10.1038/d41586-019-01184-9},
issn = {0028-0836},
journal = {Nature},
month = {apr},
title = {{How agile project management can work for your research}},
url = {http://www.nature.com/articles/d41586-019-01184-9},
year = {2019}
}
@book{openintro_statistics_book,
title = {{OpenIntro: Advanced High School Statistics}},
url = {https://www.openintro.org/stat/textbook.php?stat{\_}book=aps}
}
@inproceedings{Smith2017,
abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds" -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
archivePrefix = {arXiv},
arxivId = {1506.01186},
author = {Smith, Leslie N.},
booktitle = {Proc. - 2017 IEEE Winter Conf. Appl. Comput. Vision, WACV 2017},
doi = {10.1109/WACV.2017.58},
eprint = {1506.01186},
isbn = {9781509048229},
month = {jun},
pages = {464--472},
title = {{Cyclical learning rates for training neural networks}},
url = {http://arxiv.org/abs/1506.01186},
year = {2017}
}
@article{Yates2016,
author = {Yates, Andrew and Akanni, Wasiu and Amode, M. Ridwan and Barrell, Daniel and Billis, Konstantinos and Carvalho-Silva, Denise and Cummins, Carla and Clapham, Peter and Fitzgerald, Stephen and Gil, Laurent and Gir{\'{o}}n, Carlos Garc{\'{i}}a and Gordon, Leo and Hourlier, Thibaut and Hunt, Sarah E. and Janacek, Sophie H. and Johnson, Nathan and Juettemann, Thomas and Keenan, Stephen and Lavidas, Ilias and Martin, Fergal J. and Maurel, Thomas and McLaren, William and Murphy, Daniel N. and Nag, Rishi and Nuhn, Michael and Parker, Anne and Patricio, Mateus and Pignatelli, Miguel and Rahtz, Matthew and Riat, Harpreet Singh and Sheppard, Daniel and Taylor, Kieron and Thormann, Anja and Vullo, Alessandro and Wilder, Steven P. and Zadissa, Amonida and Birney, Ewan and Harrow, Jennifer and Muffato, Matthieu and Perry, Emily and Ruffier, Magali and Spudich, Giulietta and Trevanion, Stephen J. and Cunningham, Fiona and Aken, Bronwen L. and Zerbino, Daniel R. and Flicek, Paul and D.M.J., Ivan Adzhubei and K.R., Rosenbloom and K.D., Pruitt and D.M., Church and J., Harrow and D.R., Zerbino and S.T., Sherry and P.D., Stenson and M.J., Landrum and P.J., Kersey and W., McLaren and H., Zhao and W.J., Kent and P., Danecek and H., Li and B.J., Raney and L.G., Wilming and J.M., Rodriguez and A., Frankish and K.D., Pruitt and J.S., Amberger and D., Welter and M., Shimoyama and J., Lenffer and Z.-L., Hu and J., Sprague and P.C., Ng and X., Zhou and B., Paten and J., Ruan and H., Mi and E.L.L., Sonnhammer and S.A., Forbes and J.A., Tennessen and X., Liu and R., Petryszak and B., Mifsud and M., Lizio and J., Lonsdale and A., Yates and J., Severin and R.J., Kinsella and S., Durinck},
doi = {10.1093/nar/gkv1157},
issn = {0305-1048},
journal = {Nucleic Acids Res.},
keywords = {apache,chordata,datasets,genes,genome,internet,licensure,mice,mobile devices,phenotype,rats,zebrafish},
month = {jan},
number = {D1},
pages = {D710--D716},
publisher = {Oxford University Press},
title = {{Ensembl 2016}},
url = {https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/gkv1157},
volume = {44},
year = {2016}
}
@article{Bronstein2017,
abstract = {Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.},
archivePrefix = {arXiv},
arxivId = {1611.08097},
author = {Bronstein, Michael M. and Bruna, Joan and Lecun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
doi = {10.1109/MSP.2017.2693418},
eprint = {1611.08097},
issn = {10535888},
journal = {IEEE Signal Process. Mag.},
month = {nov},
number = {4},
pages = {18--42},
title = {{Geometric Deep Learning: Going beyond Euclidean data}},
url = {http://arxiv.org/abs/1611.08097 http://dx.doi.org/10.1109/MSP.2017.2693418},
volume = {34},
year = {2017}
}
@inproceedings{Third2016,
abstract = {{\textcopyright} Springer International Publishing AG 2016.The assessment of risk in medicine is a crucial task, and depends on scientific knowledge derived by systematic clinical studies on factors affecting health, as well as on particular knowledge about the current status of a particular patient. Existing non-semantic risk prediction tools are typically based on hardcoded scientific knowledge, and only cover a very limited range of patient states. This makes them rapidly out of date, and limited in application, particularly for patients with multiple co-occurring conditions. In this work we propose an integration of Semantic Web and Quantified Self technologies to create a framework for calculating clinical risk predictions for patients based on self-gathered biometric data. This framework relies on generic, reusable ontologies for representing clinical risk, and sensor readings, and reasoning to support the integration of data represented according to these ontologies. The implemented framework shows a wide range of advantages over existing risk calculation.},
author = {Third, Allan and Gkotsis, George and Kaldoudi, Eleni and Drosatos, George and Portokallidis, Nick and Roumeliotis, Stefanos and Pafili, Kalliopi and Domingue, John},
booktitle = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
doi = {10.1007/978-3-319-46523-4_34},
isbn = {9783319465227},
issn = {16113349},
keywords = {Comorbidities,Health,Knowledge capture,Linked data,Ontology,Risk factor,Scientific modelling,Semantics},
pages = {566--580},
title = {{Integrating medical scientific knowledge with the semantically quantified self}},
url = {http://link.springer.com/10.1007/978-3-319-46523-4{\_}34},
volume = {9981 LNCS},
year = {2016}
}
@article{DeBacco2018,
abstract = {We present a physically inspired model and an efficient algorithm to infer hierarchical rankings of nodes in directed networks. It assigns real-valued ranks to nodes rather than simply ordinal ranks, and it formalizes the assumption that interactions are more likely to occur between individuals with similar ranks. It provides a natural statistical significance test for the inferred hierarchy, and it can be used to perform inference tasks such as predicting the existence or direction of edges. The ranking is obtained by solving a linear system of equations, which is sparse if the network is; thus, the resulting algorithm is extremely efficient and scalable. We illustrate these findings by analyzing real and synthetic data, including data sets from animal behavior, faculty hiring, social support networks, and sports tournaments. We show that our method often outperforms a variety of others, in both speed and accuracy, in recovering the underlying ranks and predicting edge directions.},
archivePrefix = {arXiv},
arxivId = {1709.09002},
author = {{De Bacco}, Caterina and Larremore, Daniel B. and Moore, Cristopher},
doi = {10.1126/sciadv.aar8260},
eprint = {1709.09002},
issn = {23752548},
journal = {Sci. Adv.},
month = {jul},
number = {7},
pages = {eaar8260},
publisher = {American Association for the Advancement of Science},
title = {{A physical model for efficient ranking in networks}},
url = {http://advances.sciencemag.org/lookup/doi/10.1126/sciadv.aar8260},
volume = {4},
year = {2018}
}
@article{Rocher2019,
abstract = {While rich medical, behavioral, and socio-demographic data are key to modern data-driven research, their collection and use raise legitimate privacy concerns. Anonymizing datasets through de-identification and sampling before sharing them has been the main tool used to address those concerns. We here propose a generative copula-based method that can accurately estimate the likelihood of a specific person to be correctly re-identified, even in a heavily incomplete dataset. On 210 populations, our method obtains AUC scores for predicting individual uniqueness ranging from 0.84 to 0.97, with low false-discovery rate. Using our model, we find that 99.98{\%} of Americans would be correctly re-identified in any dataset using 15 demographic attributes. Our results suggest that even heavily sampled anonymized datasets are unlikely to satisfy the modern standards for anonymization set forth by GDPR and seriously challenge the technical and legal adequacy of the de-identification release-and-forget model.},
author = {Rocher, Luc and Hendrickx, Julien M. and de Montjoye, Yves-Alexandre},
doi = {10.1038/s41467-019-10933-3},
issn = {2041-1723},
journal = {Nat. Commun.},
keywords = {Computational science,Social sciences},
month = {dec},
number = {1},
pages = {3069},
publisher = {Nature Publishing Group},
title = {{Estimating the success of re-identifications in incomplete datasets using generative models}},
url = {http://www.nature.com/articles/s41467-019-10933-3},
volume = {10},
year = {2019}
}
@article{Grønbech2019a,
abstract = {Motivation: Models for analysing and making relevant biological inferences from massive amounts of complex single-cell transcriptomic data typically require several individual data- processing steps, each with their own set of hyperparameter choices. With deep generative models one can work directly with count data, make likelihood-based model comparison, learn a latent representation of the cells and capture more of the variability in different cell populations. Results: We propose a novel method based on variational auto-encoders (VAEs) for analysis of single-cell RNA sequencing (scRNA-seq) data. It avoids data preprocessing by using raw count data as input and can robustly estimate the expected gene expression levels and a latent representation for each cell. We tested several count likelihood functions and a variant of the VAE that has a priori clustering in the latent space. We show for several scRNA-seq data sets that our method outperforms recently proposed scRNA-seq methods in clustering cells and that the resulting clusters reflect cell types. Availability and implementation: Our method, called scVAE, is implemented in Python using the TensorFlow machine-learning library, and it is freely available at https://github. com/chgroenbech/scVAE.},
author = {Gr{\o}nbech, Christopher Heje and Vording, Maximillian Fornitz and Timshel, Pascal Nordgren and S{\o}nderby, Casper Kaae and Pers, Tune Hannes and Winther, Ole},
doi = {10.1101/318295},
journal = {bioRxiv},
month = {jan},
pages = {318295},
publisher = {Cold Spring Harbor Laboratory},
title = {{scVAE: Variational auto-encoders for single-cell gene expression data}},
url = {https://www.biorxiv.org/content/10.1101/318295v2 https://www.biorxiv.org/content/early/2019/01/21/318295.full.pdf+html},
year = {2019}
}
@article{Vita2015,
abstract = {The IEDB, www.iedb.org, contains information on immune epitopes-the molecular targets of adaptive immune responses-curated from the published literature and submitted by National Institutes of Health funded epitope discovery efforts. From 2004 to 2012 the IEDB curation of journal articles published since 1960 has caught up to the present day, with {\textgreater}95{\%} of relevant published literature manually curated amounting to more than 15 000 journal articles and more than 704 000 experiments to date. The revised curation target since 2012 has been to make recent research findings quickly available in the IEDB and thereby ensure that it continues to be an up-to-date resource. Having gathered a comprehensive dataset in the IEDB, a complete redesign of the query and reporting interface has been performed in the IEDB 3.0 release to improve how end users can access this information in an intuitive and biologically accurate manner. We here present this most recent release of the IEDB and describe the user testing procedures as well as the use of external ontologies that have enabled it.},
author = {Vita, Randi and Overton, James A. and Greenbaum, Jason A. and Ponomarenko, Julia and Clark, Jason D. and Cantrell, Jason R. and Wheeler, Daniel K. and Gabbard, Joseph L. and Hix, Deborah and Sette, Alessandro and Peters, Bjoern},
doi = {10.1093/nar/gku938},
issn = {13624962},
journal = {Nucleic Acids Res.},
month = {jan},
number = {D1},
pages = {D405--D412},
publisher = {Oxford University Press},
title = {{The immune epitope database (IEDB) 3.0}},
volume = {43},
year = {2015}
}
@article{Douali2015,
abstract = {Personalized medicine is a broad and rapidly advancing field of health care that is informed by each person's unique clinical, genetic, genomic, and environmental information. Health care that embraces personalized medicine is an integrated, coordinated, evidence based approach to individualizing patient care across the continuum. It is very important to make the right treatment decision but before that to obtain a good diagnosis. There are several clinical forms of disease whose symptoms vary depending on the age and etiology. In this study, we investigated and evaluated a model framework, for personalized diagnostic decisions, based on Case Based Fuzzy Cognitive Map (CBFCM, a cognitive process applying the main features of fuzzy logic and neural processors to situations involving imprecision and uncertain descriptions, in a similar way to intuitive human reasoning. We explored the use of this method for modelling clinical practice guidelines.},
author = {Douali, Nassim and {De Roo}, Jos and Sweetman, Pauline and Papageorgiou, Elpiniki I and Dollon, Jullien and Jaulent, Marie-Christine},
issn = {1879-8365},
journal = {Stud. Health Technol. Inform.},
pages = {308--10},
pmid = {25980889},
title = {{Personalized decision support system based on clinical practice guidelines.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25980889},
volume = {211},
year = {2015}
}
@book{Wickham2009,
abstract = {Describes ggplot2, a data visualization package for R that uses the insights from Leland Wilkison's "Grammar of Graphics" to create a powerful and flexible system for creating data graphics. This book is suitable for those who have struggled with displaying their data in an informative and attractive way. 1 Introduction; 2 Getting started with qplot; 3 Mastering the grammar; 4 Build a plot layer by layer; 5 Toolbox; 6 Scales, axes and legends; 7 Positioning; 8 Polishing your plots for publication; 9 Manipulating data; 10 Reducing duplication; Appendices.},
address = {New York},
author = {Wickham, Hadley.},
isbn = {9780387981413},
pages = {212},
publisher = {Springer-Verlag},
title = {ggplot2 : elegant graphics for data analysis},
url = {http://ggplot2.org},
year = {2016}
}
@article{Girvan2002a,
abstract = {A number of recent studies have focused on the statistical properties of networked systems such as social networks and the Worldwide Web. Researchers have concentrated particularly on a few properties that seem to be common to many networks: the small-world property, power-law degree distributions, and network transitivity. In this article, we highlight another property that is found in many networks, the property of community structure, in which network nodes are joined together in tightly knit groups, between which there are only looser connections. We propose a method for detecting such communities, built around the idea of using centrality indices to find community boundaries. We test our method on computer-generated and real-world graphs whose community structure is already known and find that the method detects this known structure with high sensitivity and reliability. We also apply the method to two networks whose community structure is not well known-a collaboration network and a food web-and find that it detects significant and informative community divisions in both cases.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0112110},
author = {Girvan, M and Newman, M E J},
doi = {10.1073/pnas.122653799},
eprint = {0112110},
issn = {00278424},
journal = {Proc. Natl. Acad. Sci. U. S. A.},
month = {jun},
number = {12},
pages = {7821--7826},
pmid = {12060727},
primaryClass = {cond-mat},
publisher = {National Academy of Sciences},
title = {{Community structure in social and biological networks}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12060727 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC122977},
volume = {99},
year = {2002}
}
@article{Oord2018,
abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
archivePrefix = {arXiv},
arxivId = {1807.03748},
author = {van den Oord, Aaron and Li, Yazhe and Vinyals, Oriol},
eprint = {1807.03748},
month = {jul},
title = {{Representation Learning with Contrastive Predictive Coding}},
url = {http://arxiv.org/abs/1807.03748},
year = {2018}
}
@article{Soneson2018,
abstract = {An extensive evaluation of differential expression methods applied to single-cell expression data, using uniformly processed public data in the new conquer resource.},
author = {Soneson, Charlotte and Robinson, Mark D},
doi = {10.1038/nmeth.4612},
issn = {1548-7091},
journal = {Nat. Methods},
keywords = {Databases,Gene expression,RNA sequencing,Statistical methods},
month = {feb},
pmid = {29481549},
publisher = {Nature Publishing Group},
title = {{Bias, robustness and scalability in single-cell differential expression analysis}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.4612},
year = {2018}
}
@article{Viechtbauer2010,
abstract = {Epigenetic biomarkers of aging (the “epigenetic clock”) have the potential to address puzzling findings surrounding mortality rates and incidence of cardio-metabolic disease such as: (1) women consistently exhibiting lower mortality than men despite having higher levels of morbidity; (2) racial/ethnic groups having different mortality rates even after adjusting for socioeconomic differences; (3) the black/white mortality cross-over effect in late adulthood; and (4) Hispanics in the United States having a longer life expectancy than Caucasians despite having a higher burden of traditional cardio-metabolic risk factors. We analyzed blood, saliva, and brain samples from seven different racial/ethnic groups. We assessed the intrinsic epigenetic age acceleration of blood (independent of blood cell counts) and the extrinsic epigenetic aging rates of blood (dependent on blood cell counts and tracks the age of the immune system). In blood, Hispanics and Tsimane Amerindians have lower intrinsic but higher extrinsic epigenetic aging rates than Caucasians. African-Americans have lower extrinsic epigenetic aging rates than Caucasians and Hispanics but no differences were found for the intrinsic measure. Men have higher epigenetic aging rates than women in blood, saliva, and brain tissue. Epigenetic aging rates are significantly associated with sex, race/ethnicity, and to a lesser extent with CHD risk factors, but not with incident CHD outcomes. These results may help elucidate lower than expected mortality rates observed in Hispanics, older African-Americans, and women.},
archivePrefix = {arXiv},
arxivId = {arXiv:1501.0228},
author = {Viechtbauer, Wolfgang},
doi = {10.18637/jss.v036.i03},
eprint = {arXiv:1501.0228},
isbn = {1548-7660},
issn = {1548-7660},
journal = {J. Stat. Softw.},
month = {aug},
number = {3},
pages = {1--48},
pmid = {18291371},
title = {{Conducting Meta-Analyses in R with the metafor Package}},
url = {http://www.jstatsoft.org/v36/i03/},
volume = {36},
year = {2010}
}
@article{Barsacchi2018,
abstract = {Gene expression microarrays provide a characterisation of the transcriptional activity of a particular biological sample. Their high dimensionality hampers the process of pattern recognition and extraction. Several approaches have been proposed for gleaning information about the hidden structure of the data. Among these approaches, deep generative models provide a powerful way for approximating the manifold on which the data reside. Here we develop GEESE, a deep learning based framework that provides novel insight into the manifold learning for gene expression data, employing a metabolic model to constrain the learned representation. We evaluated the proposed framework, showing its ability to capture biologically relevant features, and encoding that features in a much simpler latent space. We showed how using a metabolic model to drive the autoencoder learning process helps in achieving better generalisation to unseen data. GEESE provides a novel perspective on the problem of unsupervised learning for biological data.},
author = {Barsacchi, Marco and Andres-Terre, Helena and Li{\'{o}}, Pietro},
doi = {10.1101/365643},
journal = {bioRxiv},
month = {jul},
pages = {365643},
publisher = {Cold Spring Harbor Laboratory},
title = {{GEESE: Metabolically driven latent space learning for gene expression data}},
url = {https://www.biorxiv.org/content/10.1101/365643v1 https://www.biorxiv.org/content/early/2018/07/11/365643},
year = {2018}
}
@article{Hillar2012,
abstract = {In the article "Distilling free-form natural laws from experimental data", Schmidt and Lipson introduced the idea that free-form natural laws can be learned from experimental measurements in a physical system using symbolic (genetic) regression algorithms. An important claim in this work is that the algorithm finds laws in data without having incorporated any prior knowledge of physics. Upon close inspection, however, we show that their method implicitly incorporates Hamilton's equations of motions and Newton's second law, demystifying how they are able to find Hamiltonians and special classes of Lagrangians from data.},
archivePrefix = {arXiv},
arxivId = {1210.7273},
author = {Hillar, Christopher and Sommer, Friedrich},
eprint = {1210.7273},
month = {oct},
title = {{Comment on the article "Distilling free-form natural laws from experimental data"}},
url = {https://arxiv.org/abs/1210.7273 http://arxiv.org/abs/1210.7273},
year = {2012}
}
@techreport{icd10_who,
abstract = {Provides clinical descriptions, diagnostic guidelines, and codes for all mental and behavioural disorders commonly encountered in clinical psychiatry. The book was developed from chapter V of the Tenth Revision of the International Statistical Classification of Diseases and Related Health Problems (‎ICD-10)‎. The clinical descriptions and diagnostic guidelines were finalized after extensive field testing by over 700 clinicians and researchers in 110 institutes in 40 countries, making this book the product of the largest ever research effort designed to improve psychiatric diagnosis. Every effort has been made to define categories whose existence is scientifically justifiable as well as clinically useful. The classification divides disorders into ten groups according to major common themes or descriptive likeness, a new feature which makes for increased convenience of use. For each disorder, the book provides a full description of the main clinical features and all other important but less specific associated features. Diagnostic guidelines indicate the number, balance, and duration of symptoms usually required before a confident diagnosis can be made. Inclusion and exclusion criteria are also provided, together with conditions to be considered in differential diagnosis. The guidelines are worded so that a degree of flexibility is retained for diagnostic decisions in clinical work, particularly in the situation where provisional diagnosis may have to be made before the clinical picture is entirely clear or information is complete},
author = {Organization, World Health},
title = {{The ICD-10 classification of mental and behavioural disorders : clinical descriptions and diagnostic guidelines. World Health Organization}},
url = {https://apps.who.int/iris/handle/10665/37958},
year = {1992}
}
@article{Gelman1992,
author = {Gelman, Andrew and Rubin, Donald B.},
doi = {10.1214/ss/1177011136},
issn = {0883-4237},
journal = {Stat. Sci.},
month = {nov},
number = {4},
pages = {457--472},
title = {{Inference from Iterative Simulation Using Multiple Sequences}},
url = {http://projecteuclid.org/euclid.ss/1177011136},
volume = {7},
year = {1992}
}
@article{Rej2014,
abstract = {Objective Lithium is an important treatment for mood disorders, but concern about its association with renal disease has contributed to its limited use, particularly in older adults. Because high-quality evidence examining renal disease in this population is lacking, this study aims to quantify the prevalence and identify clinical correlates of renal disease in geriatric lithium users. Methods In a population-based cross-sectional study on 2,480 lithium users aged 70 or more years, the authors searched the provincial administrative health data from Ontario, Canada between April 1, 2005 and March 31, 2011. Prevalence of chronic kidney disease (CKD), acute kidney injury (AKI), and nephrogenic diabetes insipidus (NDI) was measured using International Classification of Diseases, Tenth Revision codes. Logistic regression analyses were used to identify independent correlates of renal disease. Results The 6-year prevalence rates of CKD, AKI, and NDI were 13.9{\%}, 1.3{\%}, and 3.0{\%}, respectively. Hypertension (odds ratio [OR]: 2.05; 95{\%} confidence interval [CI]: 1.50-2.79), diabetes mellitus (OR: 1.86; 95{\%} CI: 1.45-2.38), ischemic heart disease (OR: 1.65; 95{\%} CI: 1.24-2.20), NDI (OR: 2.54; 95{\%} CI: 1.47-4.40), AKI (OR: 11.7; 95{\%} CI: 5.26-26.1), lithium use for more than 2 years (OR: 1.71; 95{\%} CI: 1.05-2.81), loop diuretic use (OR: 1.74; 95{\%} CI: 1.26-2.41), hydrochlorothiazide use (OR: 1.48; 95{\%} CI: 1.07-2.05), and atypical antipsychotic use (OR: 1.49; 95{\%} CI: 1.17-1.89) were all independently associated with CKD. Conclusion Older lithium users have high rates of CKD. Lithium use duration was independently associated with CKD. Longitudinal studies including individuals without lithium exposure will be necessary to confirm whether lithium is indeed a risk factor for CKD in older adults.},
author = {Rej, Soham and Shulman, Kenneth and Herrmann, Nathan and Harel, Ziv and Fischer, Hadas D. and Fung, Kinwah and Gruneir, Andrea},
doi = {10.1016/j.jagp.2014.01.015},
issn = {15457214},
journal = {Am. J. Geriatr. Psychiatry},
keywords = {Lithium,acute kidney disease,chronic kidney disease,drug safety,geriatric,nephrogenic diabetes insipidus},
month = {nov},
number = {11},
pages = {1075--1082},
publisher = {Elsevier},
title = {{Prevalence and correlates of renal disease in older lithium users: A population-based study}},
url = {https://www.sciencedirect.com/science/article/pii/S1064748114000499?via{\%}3Dihub},
volume = {22},
year = {2014}
}
@article{Tran2016,
abstract = {Probabilistic modeling is a powerful approach for analyzing empirical information. We describe Edward, a library for probabilistic modeling. Edward's design reflects an iterative process pioneered by George Box: build a model of a phenomenon, make inferences about the model given data, and criticize the model's fit to the data. Edward supports a broad class of probabilistic models, efficient algorithms for inference, and many techniques for model criticism. The library builds on top of TensorFlow to support distributed training and hardware such as GPUs. Edward enables the development of complex probabilistic models and their algorithms at a massive scale.},
archivePrefix = {arXiv},
arxivId = {1610.09787},
author = {Tran, Dustin and Kucukelbir, Alp and Dieng, Adji B. and Rudolph, Maja and Liang, Dawen and Blei, David M.},
eprint = {1610.09787},
month = {oct},
title = {{Edward: A library for probabilistic modeling, inference, and criticism}},
url = {http://arxiv.org/abs/1610.09787},
year = {2016}
}
@article{Inkster2016,
abstract = {This opinion article briefs social media and advancing psychiatric research. Social networking sites are a part of everyday life for over a billion people worldwide. They show no sign of declining popularity, with social media use increasing at three times the rate of other internet use. Thus, working with Facebook data could further our understanding of the onset and early years of mental illness, a crucial period of interpersonal development. Provided that participant consent is obtained,12 many Facebook measures, which have yet to be extensively examined in psychiatric populations, can be collected for data analysis. Facebook friendship networks allow users to establish and maintain connections. Facebook has held its position as the most popular social networking site since 2008, but other forms of social media should be examined, including those increasingly aimed at children. It is unlikely that online social networking will decrease in popularity, so it is necessary for mental health-care professionals and researchers to engage with them. Although it is unclear how social networking sites might best be leveraged to improve mental health care, they hold considerable promise for having profound implications that could revolutionise mental healthcare. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
author = {Inkster, Becky and Stillwell, David and Kosinski, Michal and Jones, Peter},
doi = {10.1016/S2215-0366(16)30041-4},
issn = {22150374},
journal = {The Lancet Psychiatry},
month = {nov},
number = {11},
pages = {1087--1090},
pmid = {27794373},
publisher = {Elsevier},
title = {{A decade into Facebook: where is psychiatry in the digital age?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27794373},
volume = {3},
year = {2016}
}
@article{Chen2019,
abstract = {This paper focuses on a traditional relation extraction task in the context of limited annotated data and a narrow knowledge domain. We explore this task with a clinical corpus consisting of 200 breast cancer follow-up treatment letters in which 16 distinct types of relations are annotated. We experiment with an approach to extracting typed relations called window-bounded co-occurrence (WBC), which uses an adjustable context window around entity mentions of a relevant type, and compare its performance with a more typical intra-sentential co-occurrence baseline. We further introduce a new bag-of-concepts (BoC) approach to feature engineering based on the state-of-the-art word embeddings and word synonyms. We demonstrate the competitiveness of BoC by comparing with methods of higher complexity, and explore its effectiveness on this small dataset.},
archivePrefix = {arXiv},
arxivId = {1904.10743},
author = {Chen, Jiyu and Verspoor, Karin and Zhai, Zenan},
eprint = {1904.10743},
month = {apr},
title = {{A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data}},
url = {http://arxiv.org/abs/1904.10743},
year = {2019}
}
@article{Harutyunyan2017,
abstract = {Health care is one of the most exciting frontiers in data mining and machine learning. Successful adoption of electronic health records (EHRs) created an explosion in digital clinical data available for analysis, but progress in machine learning for healthcare research has been difficult to measure because of the absence of publicly available benchmark data sets. To address this problem, we propose four clinical prediction benchmarks using data derived from the publicly available Medical Information Mart for Intensive Care (MIMIC-III) database. These tasks cover a range of clinical problems including modeling risk of mortality, forecasting length of stay, detecting physiologic decline, and phenotype classification. We propose strong linear and neural baselines for all four tasks and evaluate the effect of deep supervision, multitask training and data-specific architectural modifications on the performance of neural models.},
archivePrefix = {arXiv},
arxivId = {1703.07771},
author = {Harutyunyan, Hrayr and Khachatrian, Hrant and Kale, David C. and {Ver Steeg}, Greg and Galstyan, Aram},
doi = {10.1038/s41597-019-0103-9},
eprint = {1703.07771},
issn = {20524463},
journal = {Sci. data},
keywords = {Databases,Disease,Machine learning,free survival},
month = {mar},
number = {1},
pages = {96},
publisher = {Nature Publishing Group},
title = {{Multitask learning and benchmarking with clinical time series data}},
url = {http://www.nature.com/articles/s41597-019-0103-9 http://arxiv.org/abs/1703.07771 http://dx.doi.org/10.1038/s41597-019-0103-9},
volume = {6},
year = {2019}
}
@misc{Kools2018,
author = {Kools, Farah R.W. and Mirali, Sara and Holst-Bernal, Stephanie and Nijhof, Sanne L and Cavalli, Giulio and Grandner, Michael A},
booktitle = {Front. Med.},
doi = {10.3389/fmed.2018.00314},
issn = {2296858X},
keywords = {Accountability,Evaluation,Impact,Interdisciplinary collaboration,Publication,Scientific waste,Society,Translational medicine},
number = {NOV},
pages = {314},
pmid = {30483507},
publisher = {Frontiers Media SA},
title = {{Publications are not the finish line: Focusing on societal rather than publication impact}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/30483507 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC6240663},
volume = {5},
year = {2018}
}
@inproceedings{Mwebaze2016,
abstract = {In many fields, superior gains have been obtained by leveraging the computational power of machine learning techniques to solve expert tasks. In this paper we present an application of machine learning to agriculture, solving a particular problem of diagnosis of crop disease based on plant images taken with a smartphone. Two pieces of information are important here, the disease incidence and disease severity. We present a classification system that trains a 5 class classification system to determine the state of disease of a plant. The 5 classes represent a health class and 4 disease classes. We further extend the classification system to classify different severity levels for any of the 4 diseases. Severity levels are assigned classes 1 - 5, 1 being a healthy plant, 5 being a severely diseased plant. We present ways of extracting different features from leaf images and show how different extraction methods result in different performance of the classifier. We finally present the smartphone-based system that uses the classification model learnt to do real-time prediction of the state of health of a farmers garden. This works by the farmer uploading an image of a plant in his garden and obtaining a disease score from a remote server.},
author = {Owomugisha, Godliver and Mwebaze, Ernest},
booktitle = {Proc. - 2016 15th IEEE Int. Conf. Mach. Learn. Appl. ICMLA 2016},
doi = {10.1109/ICMLA.2016.126},
isbn = {9781509061662},
month = {dec},
pages = {158--163},
publisher = {IEEE},
title = {{Machine learning for plant disease incidence and severity measurements from leaf images}},
url = {http://ieeexplore.ieee.org/document/7838138/},
year = {2017}
}
@article{Moni2014,
abstract = {BACKGROUND The diagnosis of comorbidities, which refers to the coexistence of different acute and chronic diseases, is difficult due to the modern extreme specialisation of physicians. We envisage that a software dedicated to comorbidity diagnosis could result in an effective aid to the health practice. RESULTS We have developed an R software comoR to compute novel estimators of the disease comorbidity associations. Starting from an initial diagnosis, genetic and clinical data of a patient the software identifies the risk of disease comorbidity. Then it provides a pipeline with different causal inference packages (e.g. pcalg, qtlnet etc) to predict the causal relationship of diseases. It also provides a pipeline with network regression and survival analysis tools (e.g. Net-Cox, rbsurv etc) to predict more accurate survival probability of patients. The input of this software is the initial diagnosis for a patient and the output provides evidences of disease comorbidity mapping. CONCLUSIONS The functions of the comoR offer flexibility for diagnostic applications to predict disease comorbidities, and can be easily integrated to high-throughput and clinical data analysis pipelines.},
author = {Moni, Mohammad Ali and Li{\`{o}}, Pietro},
doi = {10.1186/2043-9113-4-8},
issn = {20439113},
journal = {J. Clin. Bioinforma.},
keywords = {Comorbidities,Disease associations,Relative risk},
number = {1},
pages = {8},
pmid = {25045465},
title = {{comoR: A software for disease comorbidity risk assessment}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25045465 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4081507 http://jclinbioinformatics.biomedcentral.com/articles/10.1186/2043-9113-4-8},
volume = {4},
year = {2014}
}
@article{Winston2012a,
abstract = {I review history, starting with Turing's seminal paper, reaching back ultimately to when our species started to outperform other primates, searching for the questions that will help us develop a computational account of human intelligence. I answer that the right questions are: What's different between us and the other primates and what's the same. I answer the what's different question by saying that we became symbolic in a way that enabled story understanding, directed perception, and easy communication, and other species did not. I argue against Turing's reasoning-centered suggestions, offering that reasoning is just a special case of story understanding. I answer the what's the same question by noting that our brains are largely engineered in the same exotic way, with information flowing in all directions at once. By way of example, I illustrate how these answers can influence a research program, describing the Genesis system, a system that works with short summaries of stories, provided in English, together with low-level common-sense rules and higher-level concept patterns, likewise expressed in English. Genesis answers questions, notes abstract concepts such as revenge, tells stories in a listener-aware way, and fills in story gaps using precedents. I conclude by suggesting, optimistically, that a genuine computational theory of human intelligence will emerge in the next 50. years if we stick to the right, biologically inspired questions, and work toward biologically informed models. {\textcopyright} 2012 Elsevier B.V.},
author = {Winston, Patrick Henry},
doi = {10.1016/j.bica.2012.03.002},
issn = {2212683X},
journal = {Biol. Inspired Cogn. Archit.},
keywords = {Biologically inspired cognitive models,Directed perception,Evolution of intelligence,Human intelligence,Inner language,Story understanding},
month = {jul},
pages = {92--99},
title = {{The next 50 years: A personal view}},
volume = {1},
year = {2012}
}
@article{Irving2019,
abstract = {Properly aligning advanced AI systems with human values will require resolving many uncertainties related to the psychology of human rationality, emotion, and biases. These can only be resolved empirically through experimentation — if we want to train AI to do what humans want, we need to study humans.},
author = {Irving, Geoffrey and Askell, Amanda},
doi = {10.23915/distill.00014},
issn = {2476-0757},
journal = {Distill},
month = {feb},
number = {2},
pages = {e14},
title = {{AI Safety Needs Social Scientists}},
url = {https://distill.pub/2019/safety-needs-social-scientists},
volume = {4},
year = {2019}
}
@article{Kirkham2013,
abstract = {Lithium has been used in the fields of rheumatology and psychiatry since the 1800s and it is now generally considered to be a gold standard treatment for bipolar disorders. However, lithium is known to have significant side effects and requires close serum level monitoring to ensure levels remain within the therapeutic range to minimize the risk of serious adverse effects or toxicity. This article reviews the monitoring of lithium and reports on the implementation of a regional lithium register and database within Norfolk. Recorded blood results from the Norfolk lithium database were extracted for the first full year of operation across the region, 2005/6, and from the most recent full year 2011/12. The number of lithium monitoring tests, U{\&}Es and thyroid function tests conducted on all people registered on the database were compared between the two sample years. In 2005/6 there were a significant number of people not receiving the recommended number of four or more serum lithium test per year (68.3{\%}) and the majority of people had two or three tests (62{\%}). By 2011/12 this had noticeably increased with the majority of patients having four or more tests per year (68.5{\%}) and the number having only two or three tests reducing dramatically (26.4{\%}). Improved rates of lithium testing and monitoring have been demonstrated since the introduction of the Norfolk database helping to achieve national targets. Consequently, the chances of adverse events from insufficient monitoring have been minimized. {\textcopyright} 2013, SAGE Publications. All rights reserved.},
author = {Kirkham, Emma and Bazire, Stephen and Anderson, Timothy and Wood, John and Grassby, Paul and Desborough, James A.},
doi = {10.1177/2045125313486510},
issn = {20451261},
journal = {Ther. Adv. Psychopharmacol.},
keywords = {bipolar disorder,databases,drug monitoring,factual,lithium,psychiatry},
month = {oct},
number = {5},
pages = {260--265},
publisher = {SAGE PublicationsSage UK: London, England},
title = {{Impact of active monitoring on lithium management in Norfolk}},
url = {http://journals.sagepub.com/doi/10.1177/2045125313486510},
volume = {3},
year = {2013}
}
@article{Laird1987,
abstract = {The ultimate goal of work in cognitive architecture is to provide the foundation for a system capable of general intelligent behavior. That is, the goal is to provide the underlying structure that would enable a system to perform the full range of cognitive tasks, employ the full range of problem solving methods and representations appropriate for the tasks, and learn about all aspects of the tasks and its performance on them. In this article we present SOAR, an implemented proposal for such an architecture. We describe its organizational principles, the system as currently implemented, and demonstrations of its capabilities. {\textcopyright} 1987.},
author = {Laird, John E. and Newell, Allen and Rosenbloom, Paul S.},
doi = {10.1016/0004-3702(87)90050-6},
issn = {00043702},
journal = {Artif. Intell.},
month = {sep},
number = {1},
pages = {1--64},
publisher = {Elsevier},
title = {{SOAR: An architecture for general intelligence}},
url = {https://www.sciencedirect.com/science/article/pii/0004370287900506},
volume = {33},
year = {1987}
}
@book{Winston1984,
abstract = {2nd ed. 1. The intelligent computer -- 2. Description matching and goal reduction -- 3. Exploiting natural constraints -- 4. Exploring alternatives -- 5. Control metaphors -- 6. Problem-solving paradigms -- 7. Logic and theorem proving -- 8. Representing commonsense knowledge -- 9. Language understanding -- 10. Image understanding -- 11. Learning class descriptions from samples -- 12. Learning rules from experience.},
author = {Winston, Patrick Henry.},
isbn = {0201082594},
pages = {527},
publisher = {Addison-Wesley},
title = {{Artificial intelligence}},
url = {https://books.google.co.uk/books/about/Artificial{\_}Intelligence.html?id=VGYJqiAz6{\_}sC},
year = {1984}
}
@article{Howard2019,
abstract = {Major depression is a debilitating psychiatric illness that is typically associated with low mood, anhedonia and a range of comorbidities. Depression has a heritable component that has remained difficult to elucidate with current sample sizes due to the polygenic nature of the disorder. To maximise sample size, we meta-analysed data on 807,553 individuals (246,363 cases and 561,190 controls) from the three largest genome-wide association studies of depression. We identified 102 independent variants, 269 genes, and 15 gene-sets associated with depression, including both genes and gene-pathways associated with synaptic structure and neurotransmission. Further evidence of the importance of prefrontal brain regions in depression was provided by an enrichment analysis. In an independent replication sample of 1,306,354 individuals (414,055 cases and 892,299 controls), 87 of the 102 associated variants were significant following multiple testing correction. Based on the putative genes associated with depression this work also highlights several potential drug repositioning opportunities. These findings advance our understanding of the complex genetic architecture of depression and provide several future avenues for understanding aetiology and developing new treatment approaches.},
author = {Xu, Eileen Y. and Trzaskowski, Maciej and Teismann, Henning and Domschke, Katharina and Rawal, Rajesh and Breen, Gerome and Porteous, David J. and Dannlowski, Udo and Adams, Mark J. and Wigmore, Eleanor M. and Arolt, Volker and Ward, Joey and Davies, Gail and Byrne, Enda M. and Hafferty, Jonathan D. and Barbu, Miruna C. and Whalley, Heather C. and Alloza, Clara and Coleman, Jonathan R. I. and Howard, David M. and Clarke, Toni-Kim and Shen, Xueyi and McIntosh, Andrew M. and Smith, Daniel J. and Ripke, Stephan and Shirali, Masoud and Berger, Klaus and Marioni, Riccardo E. and Baune, Bernhard T. and Deary, Ian J. and Hemani, Gibran and Gibson, Jude and Tian, Chao and Hinds, David A. and Sullivan, Patrick F. and Lewis, Cathryn M. and Wray, Naomi R. and Hagenaars, Saskia P.},
doi = {10.1038/s41593-018-0326-7},
issn = {1097-6256},
journal = {Nat. Neurosci.},
keywords = {Depression,Genome,wide association studies},
month = {mar},
number = {3},
pages = {343--352},
pmid = {30718901},
publisher = {Nature Publishing Group},
title = {{Genome-wide meta-analysis of depression identifies 102 independent variants and highlights the importance of the prefrontal brain regions}},
url = {http://www.nature.com/articles/s41593-018-0326-7},
volume = {22},
year = {2019}
}
@article{Zou2005,
abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together.The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n).By contrast, the lasso is not a very satisfactory variable selection method in the p{\textgreater}{\textgreater}n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
author = {Zou, Hui and Hastie, Trevor},
doi = {10.1111/j.1467-9868.2005.00503.x},
isbn = {1369-7412},
issn = {13697412},
journal = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
keywords = {Grouping effect,LARS algorithm,Lasso,P ≫ n problem,Penalization,Variable selection},
month = {apr},
number = {2},
pages = {301--320},
pmid = {20713001},
publisher = {Blackwell Publishing Ltd},
title = {{Regularization and variable selection via the elastic net}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2005.00503.x},
volume = {67},
year = {2005}
}
@article{Cannoodt2016,
abstract = {Recent developments in single-cell transcriptomics have opened new opportunities for studying dynamic processes in immunology in a high throughput and unbiased manner. Starting from a mixture of cells in different stages of a developmental process, unsupervised trajectory inference algorithms aim to automatically reconstruct the underlying developmental path that cells are following. In this review, we break down the strategies used by this novel class of methods, and organize their components into a common framework, highlighting several practical advantages and disadvantages of the individual methods. We also give an overview of new insights these methods have already provided regarding the wiring and gene regulation of cell differentiation. As the trajectory inference field is still in its infancy, we propose several future developments that will ultimately lead to a global and data-driven way of studying immune cell differentiation.},
author = {Cannoodt, Robrecht and Saelens, Wouter and Saeys, Yvan},
doi = {10.1002/eji.201646347},
issn = {15214141},
journal = {Eur. J. Immunol.},
keywords = {Bioinformatics,Cell differentiation,Single-cell transcriptomics},
month = {nov},
number = {11},
pages = {2496--2506},
pmid = {27682842},
title = {{Computational methods for trajectory inference from single-cell transcriptomics}},
url = {http://doi.wiley.com/10.1002/eji.201646347},
volume = {46},
year = {2016}
}
@article{Franceschet2015,
abstract = {Complex systems and their underlying convoluted networks are ubiquitous; all we need is an eye for them. They pose problems of organized complexity that cannot be approached with a reductionist method. Complexity science and its emergent sister network science both come to grips with the inherent complexity of complex systems with a holistic strategy. The relevance of complexity, however, transcends the sciences. Complex systems and networks are the focal point of a philosophical, cultural, and artistic turn of our tightly interrelated and interdependent postmodern society. I argue that complex systems can be beautiful and can be the object of artification—the neologism refers to processes in which something that is not regarded as art in the traditional sense of the word is changed into art. Complex systems and networks are powerful sources of inspiration for the artful data visualizer and for the generative designer, as well as for the traditional artist.},
author = {Franceschet, Massimo},
doi = {10.25088/ComplexSystems.24.3.249},
issn = {08912513},
journal = {Complex Syst.},
number = {3},
pages = {249--259},
publisher = {Complex Systems Publications, Inc},
title = {{Complex beauty}},
volume = {24},
year = {2015}
}
@article{Marcus2018,
abstract = {The concept of innateness is rarely discussed in the context of artificial intelligence. When it is discussed, or hinted at, it is often the context of trying to reduce the amount of innate machinery in a given system. In this paper, I consider as a test case a recent series of papers by Silver et al (Silver et al., 2017a) on AlphaGo and its successors that have been presented as an argument that a "even in the most challenging of domains: it is possible to train to superhuman level, without human examples or guidance", "starting tabula rasa." I argue that these claims are overstated, for multiple reasons. I close by arguing that artificial intelligence needs greater attention to innateness, and I point to some proposals about what that innateness might look like.},
archivePrefix = {arXiv},
arxivId = {1801.05667},
author = {Marcus, Gary},
eprint = {1801.05667},
month = {jan},
title = {{Innateness, AlphaZero, and Artificial Intelligence}},
url = {http://arxiv.org/abs/1801.05667},
year = {2018}
}
@article{Casey2016,
abstract = {The use and functionality of electronic health records (EHRs) have increased rapidly in the past decade. Although the primary purpose of EHRs is clinical, researchers have used them to conduct epidemiologic investigations, ranging from cross-sectional studies within a given hospital to longitudinal studies on geographically distributed patients. Herein, we describe EHRs, examine their use in population health research, and compare them with traditional epidemiologic methods. We describe diverse research applications that benefit from the large sample sizes and generalizable patient populations afforded by EHRs. These have included reevaluation of prior findings, a range of diseases and subgroups, environmental and social epidemiology, stigmatized conditions, predictive modeling, and evaluation of natural experiments. Although studies using primary data collection methods may have more reliable data and better population retention, EHR-based studies are less expensive and require less time to complete. Fut...},
author = {Casey, Joan A. and Schwartz, Brian S. and Stewart, Walter F. and Adler, Nancy E.},
doi = {10.1146/annurev-publhealth-032315-021353},
issn = {0163-7525},
journal = {Annu. Rev. Public Health},
keywords = {EHR,electronic health records,environmental epidemiology,geographic information systems,health determinants,social epidemiology},
month = {mar},
number = {1},
pages = {61--81},
publisher = {Annual Reviews},
title = {{Using Electronic Health Records for Population Health Research: A Review of Methods and Applications}},
url = {http://www.annualreviews.org/doi/10.1146/annurev-publhealth-032315-021353},
volume = {37},
year = {2016}
}
@article{Celler2018,
abstract = {BACKGROUND: In a home telemonitoring trial, patient adherence with scheduled vital signs measurements is an important aspect that has not been thoroughly studied and for which data in the literature are limited. Levels of adherence have been reported as varying from approximately 40{\%} to 90{\%}, and in most cases, the adherence rate usually dropped off steadily over time. This drop is more evident in the first few weeks or months after the start. Higher adherence rates have been reported for simple types of monitoring and for shorter periods of intervention. If patients do not follow the intended procedure, poorer results than expected may be achieved. Hence, analyzing factors that can influence patient adherence is of great importance. OBJECTIVE: The goal of the research was to present findings on patient adherence with scheduled vital signs measurements in the recently completed Commonwealth Scientific and Industrial Research Organisation (CSIRO) national trial of home telemonitoring of patients (mean age 70.5 years, SD 9.3 years) with chronic conditions (chronic obstructive pulmonary disease, coronary artery disease, hypertensive diseases, congestive heart failure, diabetes, or asthma) carried out at 5 locations along the east coast of Australia. We investigated the ability of chronically ill patients to carry out a daily schedule of vital signs measurements as part of a chronic disease management care plan over periods exceeding 6 months (302 days, SD 135 days) and explored different levels of adherence for different measurements as a function of age, gender, and supervisory models. METHODS: In this study, 113 patients forming the test arm of a Before and After Control Intervention (BACI) home telemonitoring trial were analyzed. Patients were required to monitor on a daily basis a range of vital signs determined by their chronic condition and comorbidities. Vital signs included noninvasive blood pressure, pulse oximetry, spirometry, electrocardiogram (ECG), blood glucose level, body temperature, and body weight. Adherence was calculated as the number of days during which at least 1 measurement was taken over all days where measurements were scheduled. Different levels of adherence for different measurements, as a function of age, gender, and supervisory models, were analyzed using linear regression and analysis of covariance for a period of 1 year after the intervention. RESULTS: Patients were monitored on average for 302 (SD 135) days, although some continued beyond 12 months. The overall adherence rate for all measurements was 64.1{\%} (range 59.4{\%} to 68.8{\%}). The adherence rates of patients monitored in hospital settings relative to those monitored in community settings were significantly higher for spirometry (69.3{\%}, range 60.4{\%} to 78.2{\%}, versus 41.0{\%}, range 33.1{\%} to 49.0{\%}, P{\textless}.001), body weight (64.5{\%}, range 55.7{\%} to 73.2{\%}, versus 40.5{\%}, range 32.3{\%} to 48.7{\%}, P{\textless}.001), and body temperature (66.8{\%}, range 59.7{\%} to 73.9{\%}, versus 55.2{\%}, range 48.4{\%} to 61.9{\%}, P=.03). Adherence with blood glucose measurements (58.1{\%}, range 46.7{\%} to 69.5{\%}, versus 50.2{\%}, range 42.8{\%} to 57.6{\%}, P=.24) was not significantly different overall. Adherence rates for blood pressure (68.5{\%}, range 62.7{\%} to 74.2{\%}, versus 59.7{\%}, range 52.1{\%} to 67.3{\%}, P=.04), ECG (65.6{\%}, range 59.7{\%} to 71.5{\%}, versus 56.5{\%}, range 48.7{\%} to 64.4{\%}, P=.047), and pulse oximetry (67.0{\%}, range 61.4{\%} to 72.7{\%}, versus 56.4{\%}, range 48.6{\%} to 64.1{\%}, P=.02) were significantly higher in males relative to female subjects. No statistical differences were observed between rates of adherence for the younger patient group (70 years and younger) and older patient group (older than 70 years). CONCLUSIONS: Patients with chronic conditions enrolled in the home telemonitoring trial were able to record their vital signs at home at least once every 2 days over prolonged periods of time. Male patients maintained a higher adherence than female patients over time, and patients supervised by hospital-based care coordinators reported higher levels of adherence with their measurement schedule relative to patients supervised in community settings. This was most noticeable for spirometry. TRIAL REGISTRATION: Australian New Zealand Clinical Trials Registry ACTRN12613000635763; https://www.anzctr.org.au/Trial/Registration/TrialReview.aspx?id=364030{\&}isReview= true (Archived by WebCite at http://www.webcitation.org/6xPOU3DpR).},
author = {Celler, Branko and Argha, Ahmadreza and Varnfield, Marlien and Jayasena, Rajiv},
doi = {10.2196/medinform.9200},
isbn = {2291-9694 (Print)},
issn = {14388871},
journal = {J. Med. Internet Res.},
keywords = {Chronic disease,Clinical trial,Patient compliance,Telehealth,Telemonitoring,Vital signs},
month = {apr},
number = {4},
pages = {e15},
pmid = {29631991},
publisher = {JMIR Publications Inc.},
title = {{Patient adherence to scheduled vital sign measurements during home telemonitoring: Analysis of the intervention arm in a before and after trial}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/29631991 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5913569},
volume = {20},
year = {2018}
}
@article{Valenta2016,
abstract = {Since the inception of the Clinical and Translational Science Award (CTSA) program in 2006, leaders in education across CTSA sites have been developing and updating core competencies for Clinical and Translational Science (CTS) trainees. By 2009, 14 competency domains, including biomedical informatics, had been identified and published. Since that time, the evolution of the CTSA program, changes in the practice of CTS, the rapid adoption of electronic health records (EHRs), the growth of biomedical informatics, the explosion of big data, and the realization that some of the competencies had proven to be difficult to apply in practice have made it clear that the competencies should be updated. This paper describes the process undertaken and puts forth a new set of competencies that has been recently endorsed by the Clinical Research Informatics Workgroup of AMIA. In addition to providing context and background for the current version of the competencies, we hope this will serve as a model for revision of competencies over time.},
author = {Valenta, Annette L. and Meagher, Emma A. and Tachinardi, Umberto and Starren, Justin},
doi = {10.1093/jamia/ocw047},
issn = {1527974X},
journal = {J. Am. Med. Informatics Assoc.},
keywords = {Education,Graduate,Informatics,Professional competence,Translational medical research},
number = {4},
pages = {835--839},
title = {{Core informatics competencies for clinical and translational scientists: What do our customers and collaborators need to know?}},
volume = {23},
year = {2016}
}
@article{Bendz2010,
abstract = {We sought to establish the prevalence of lithium-induced end-stage renal disease in two regions of Sweden with 2.7 million inhabitants corresponding to about 30{\%} of the Swedish population. Eighteen patients with lithium-induced end-stage renal disease were identified among the 3369 patients in the general lithium-treated population, representing a sixfold increase in prevalence compared with the general population for renal replacement therapy. All lithium-treated patients were older than 46 years at end-stage renal disease with a mean lithium treatment time of 23 years with ten patients having discontinued lithium treatment an average of 10 years before the start of renal replacement therapy. The prevalence of chronic kidney disease (defined as plasma creatinine over 150 $\mu$mol/l) in the general lithium-treated population was about 1.2{\%} (excluding patients on renal replacement therapy). Compared with lithium-treated patients without renal failure, those with chronic kidney disease were older and most were men but, as groups, their mean serum lithium levels and psychiatric diagnoses did not differ. We found that end-stage renal disease is an uncommon but not rare consequence of long-term lithium treatment and is more prevalent than previously thought. Time on lithium was the only identified risk factor in this study, suggesting that regular monitoring of renal function in these patients is mandatory.},
author = {Bendz, Hans and Sch{\"{o}}n, Staffan and Attman, Per Ola and Aurell, Mattias},
doi = {10.1038/ki.2009.433},
issn = {00852538},
journal = {Kidney Int.},
keywords = {Adverse effects,Affective disorders,Lithium,Renal failure},
month = {feb},
number = {3},
pages = {219--224},
pmid = {19940841},
title = {{Renal failure occurs in chronic lithium treatment but is uncommon}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19940841 https://linkinghub.elsevier.com/retrieve/pii/S0085253815542233},
volume = {77},
year = {2010}
}
@misc{Khandaker2015,
abstract = {Complex interactions between the immune system and the brain might have important aetiological and therapeutic implications for neuropsychiatric brain disorders. A possible association between schizophrenia and the immune system was postulated over a century ago, and is supported by epidemiological and genetic studies pointing to links with infection and inflammation. Contrary to the traditional view that the brain is an immunologically privileged site shielded behind the blood-brain barrier, studies in the past 20 years have noted complex interactions between the immune system, systemic inflammation, and the brain, which can lead to changes in mood, cognition, and behaviour. In this Review, we describe some of the important areas of research regarding innate and adaptive immune response in schizophrenia and related psychotic disorders that, we think, will be of interest to psychiatric clinicians and researchers. We discuss potential mechanisms and therapeutic implications of these findings, including studies of anti-inflammatory drugs in schizophrenia, describe areas for development, and offer testable hypotheses for future investigations.},
author = {Khandaker, Golam M and Cousins, Lesley and Deakin, Julia and Lennox, Belinda R and Yolken, Robert and Jones, Peter B},
booktitle = {The Lancet Psychiatry},
doi = {10.1016/S2215-0366(14)00122-9},
issn = {22150374},
month = {mar},
number = {3},
pages = {258--270},
publisher = {Elsevier},
title = {{Inflammation and immunity in schizophrenia: Implications for pathophysiology and treatment}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S2215036614001229},
volume = {2},
year = {2015}
}
@misc{online_learning_management_software,
title = {{MyOpenMath}},
url = {https://www.myopenmath.com/course/public.php?cid=11774},
urldate = {2019-09-13}
}
@article{Chami2018,
abstract = {In low-income countries, complex comorbidities and weak health systems confound disease diagnosis and treatment. Yet, data-driven approaches have not been applied to develop better diagnostic strategies or to tailor treatment delivery for individuals within rural poor communities. We observed symptoms/diseases reported within three months by 16 357 individuals aged 1+ years in 17 villages of Mayuge District, Uganda. Symptoms were mapped to the Human Phenotype Ontology. Comorbidity networks were constructed. An edge between two symptoms/diseases was generated if the relative risk greater than 1, ϕ correlation greater than 0, and local false discovery rate less than 0.05. We studied how network structure and flagship symptom profiles varied against biosocial factors. 88.05{\%} of individuals (14 402/16 357) reported at least one symptom/disease. Young children and individuals in worse-off households—low socioeconomic status, poor water, sanitation, and hygiene, and poor medical care—had dense network structures with the highest comorbidity burden and/or were conducive to the onset of new comorbidities from existing flagship symptoms, such as fever. Flagship symptom profiles for fever revealed self-misdiagnoses of fever as malaria and sexually transmitted infections as a potentially missed cause of fever in individuals of reproductive age. Network analysis may inform the development of new diagnostic and treatment strategies for flagship symptoms used to characterize syndromes/diseases of global concern.},
author = {Chami, Goylette F. and Kabatereine, Narcis B. and Tukahebwa, Edridah M. and Dunne, David W.},
doi = {10.1098/rsif.2018.0248},
issn = {1742-5689},
journal = {J. R. Soc. Interface},
month = {oct},
number = {147},
pages = {20180248},
publisher = {The Royal Society},
title = {{Precision global health and comorbidity: a population-based study of 16 357 people in rural Uganda}},
url = {http://rsif.royalsocietypublishing.org/lookup/doi/10.1098/rsif.2018.0248},
volume = {15},
year = {2018}
}
@misc{bayesian_regression_youtube,
author = {Miller, Jeff},
title = {{Posterior for linear regression - YouTube}},
url = {https://www.youtube.com/watch?v=qz2U8coNwV4},
urldate = {2019-01-18}
}
@article{Textor2016,
abstract = {{\textcopyright} The Author 2017. Directed acyclic graphs (DAGs), which offer systematic representations of causal relationships, have become an established framework for the analysis of causal inference in epidemiology, often being used to determine covariate adjustment sets for minimizing confounding bias. DAGitty is a popular web application for drawing and analysing DAGs. Here we introduce the R package 'dagitty', which provides access to all of the capabilities of the DAGitty web application within the R platform for statistical computing, and also offers several new functions. We describe how the R package 'dagitty' can be used to: evaluate whether a DAG is consistent with the dataset it is intended to represent; enumerate 'statistically equivalent' but causally different DAGs; and identify exposureoutcome adjustment sets that are valid for causally different but statistically equivalent DAGs. This functionality enables epidemiologists to detect causal misspecifications in DAGs and make robust inferences that remain valid for a range of different DAGs. The R package 'dagitty' is available through the comprehensive R archive network (CRAN) at [https://cran.r-project.org/web/packages/dagitty/]. The source code is available on github at [https://github.com/jtextor/dagitty] . The web application 'DAGitty' is free software, licensed under the GNU general public licence (GPL) version 2 and is available at [http:// dagitty.net/].},
author = {Textor, Johannes and van der Zander, Benito and Gilthorpe, Mark S. and Li{\'{s}}kiewicz, Maciej and Ellison, George T.H.},
doi = {10.1093/ije/dyw341},
issn = {14643685},
journal = {Int. J. Epidemiol.},
month = {jan},
number = {6},
pages = {1887--1894},
title = {{Robust causal inference using directed acyclic graphs: The R package 'dagitty'}},
url = {https://academic.oup.com/ije/article-lookup/doi/10.1093/ije/dyw341},
volume = {45},
year = {2016}
}
@article{Moghaddass2018,
abstract = {Doctors often rely on their past experience in order to diagnose patients. For a doctor with enough experience, almost every patient would have similarities to key cases seen in the past, and each new patient could be viewed as a mixture of these key past cases. Because doctors often tend to reason this way, an efficient computationally aided diagnostic tool that thinks in the same way might be helpful in locating key past cases of interest that could assist with diagnosis. This article develops a novel mathematical model to mimic the type of logical thinking that physicians use when considering past cases. The proposed model can also provide physicians with explanations that would be similar to the way they would naturally reason about cases. The proposed method is designed to yield predictive accuracy, computational efficiency, and insight into medical data; the key element is the insight into medical data, in some sense we are automating a complicated process that physicians might perform manually. We finally implemented the result of this work on two publicly available healthcare datasets, for heart disease prediction and breast cancer prediction.},
archivePrefix = {arXiv},
arxivId = {1809.03541},
author = {Moghaddass, Ramin and Rudin, Cynthia},
eprint = {1809.03541},
issn = {08648921},
month = {sep},
title = {{Bayesian Patchworks: An Approach to Case-Based Reasoning}},
url = {http://arxiv.org/abs/1809.03541},
year = {2018}
}
@misc{Albers2019,
abstract = {In research studies, the need for additional samples to obtain sufficient statistical power has often to be balanced with the experimental costs. One approach to this end is to sequentially collect data until you have sufficient measurements, e.g., when the p-value drops below 0.05. I outline that this approach is common, yet that unadjusted sequential sampling leads to severe statistical issues, such as an inflated rate of false positive findings. As a consequence, the results of such studies are untrustworthy. I identify the statistical methods that can be implemented in order to account for sequential sampling.},
author = {Albers, Casper},
booktitle = {Nat. Commun.},
doi = {10.1038/s41467-019-09941-0},
issn = {20411723},
keywords = {Research data,Statistics},
month = {dec},
number = {1},
pages = {1921},
publisher = {Nature Publishing Group},
title = {{The problem with unadjusted multiple and sequential statistical testing}},
url = {http://www.nature.com/articles/s41467-019-09941-0},
volume = {10},
year = {2019}
}
@article{Humbatova2019,
abstract = {The growing application of deep neural networks in safety-critical domains makes the analysis of faults that occur in such systems of enormous importance. In this paper we introduce a large taxonomy of faults in deep learning (DL) systems. We have manually analysed 1059 artefacts gathered from GitHub commits and issues of projects that use the most popular DL frameworks (TensorFlow, Keras and PyTorch) and from related Stack Overflow posts. Structured interviews with 20 researchers and practitioners describing the problems they have encountered in their experience have enriched our taxonomy with a variety of additional faults that did not emerge from the other two sources. Our final taxonomy was validated with a survey involving an additional set of 21 developers, confirming that almost all fault categories (13/15) were experienced by at least 50{\%} of the survey participants.},
archivePrefix = {arXiv},
arxivId = {1910.11015},
author = {Jahangirova, Gunel and Humbatova, Nargiz and Bavota, Gabriele and Riccio, Vincenzo and Stocco, Andrea and Tonella, Paolo},
eprint = {1910.11015},
month = {oct},
title = {{Taxonomy of Real Faults in Deep Learning Systems}},
url = {http://arxiv.org/abs/1910.11015},
year = {2019}
}
@article{Oostwal2019,
abstract = {We study layered neural networks of rectified linear units (ReLU) in a modelling framework for stochastic training processes. The comparison with sigmoidal activation functions is in the center of interest. We compute typical learning curves for shallow networks with K hidden units in matching student teacher scenarios. The systems exhibit sudden changes of the generalization performance via the process of hidden unit specialization at critical sizes of the training set. Surprisingly, our results show that the training behavior of ReLU networks is qualitatively different from that of networks with sigmoidal activations. In networks with K {\textgreater}= 3 sigmoidal hidden units, the transition is discontinuous: Specialized network configurations co-exist and compete with states of poor performance even for very large training sets. On the contrary, the use of ReLU activations results in continuous transitions for all K: For large enough training sets, two competing, differently specialized states display similar generalization abilities, which coincide exactly for large networks in the limit K to infinity.},
archivePrefix = {arXiv},
arxivId = {1910.07476},
author = {Oostwal, Elisa and Straat, Michiel and Biehl, Michael},
eprint = {1910.07476},
month = {oct},
title = {{Hidden Unit Specialization in Layered Neural Networks: ReLU vs. Sigmoidal Activation}},
url = {http://arxiv.org/abs/1910.07476},
year = {2019}
}
@inproceedings{Bhattacharya2018c,
address = {New York, New York, USA},
author = {Bhattacharya, Moumita and Jurkovitz, Claudine and Shatkay, Hagit},
booktitle = {Proc. 2018 ACM Int. Conf. Bioinformatics, Comput. Biol. Heal. Informatics - BCB '18},
doi = {10.1145/3233547.3233723},
isbn = {9781450357944},
keywords = {co-occurring medical conditions,electronic health records,gibbs sampling,jensen-shannon divergence,latent dirichlet allocation,topic modeling},
pages = {570--570},
publisher = {ACM Press},
title = {{Co-occurrence of Medical Conditions}},
url = {http://dl.acm.org/citation.cfm?doid=3233547.3233723},
year = {2018}
}
@article{Anderson2017,
author = {Anderson, David P.},
doi = {10.1145/3108926},
issn = {00010782},
journal = {Commun. ACM},
month = {jul},
number = {8},
pages = {22--25},
title = {{Prophets, seers, and pioneers}},
url = {http://dl.acm.org/citation.cfm?doid=3127343.3108926},
volume = {60},
year = {2017}
}
@article{Carter2017,
abstract = {By creating user interfaces which let us work with the representations inside machine learning models, we can give people new tools for reasoning.},
author = {Carter, Shan and Nielsen, Michael},
doi = {10.23915/distill.00009},
issn = {2476-0757},
journal = {Distill},
month = {dec},
number = {12},
pages = {e9},
title = {{Using Artificial Intelligence to Augment Human Intelligence}},
url = {https://distill.pub/2017/aia},
volume = {2},
year = {2017}
}
@inproceedings{Forbus2010,
abstract = {We believe that the flexibility and robustness of common sense reasoning comes from analogical reasoning, learning, and generalization operating over massive amounts of experience. Million-fact knowledge bases are a good starting point, but are likely to be orders of magnitude smaller, in terms of ground facts, than will be needed to achieve human-like common sense reasoning. This paper describes the FIRE reasoning engine which we have built to experiment with this approach. We discuss its knowledge base organization, including coarse-coding via mentions and a persistent TMS to achieve efficient retrieval while respecting the logical environment formed by contexts and their relationships in the KB. We describe its stratified reasoning organization, which supports both reflexive reasoning (Ask, Query) and deliberative reasoning (Solve, HTN planner). Analogical reasoning, learning, and generalization are supported as part of reflexive reasoning. To show the utility of these ideas, we describe how they are used in the Companion cognitive architecture, which has been used in a variety of reasoning and learning experiments. Copyright {\textcopyright} 2010, Association for the Advancement of Artificial Intelligence. All rights reserved.},
author = {Forbus, Kenneth D. and Hinrichs, Tom and {De Kleer}, Johan and Usher, Jeffrey},
booktitle = {AAAI Fall Symp. - Tech. Rep.},
isbn = {9781577354840},
keywords = {analogy,common sense,knowledge representation,reasoning},
month = {nov},
pages = {21--26},
title = {{FIRE: Infrastructure for experience-based systems with common sense}},
url = {https://www.aaai.org/ocs/index.php/FSS/FSS10/paper/viewPaper/2244},
volume = {FS-10-02},
year = {2010}
}
@phdthesis{Koton1988,
abstract = {... This thesis describes a system, CASEY, that uses case- based reasoning to recall and remember problems it has seen before, and uses a causal model of its domain to justify re- using previous solutions and to solve unfamiliar problems. ...},
author = {Koton, Phyllis},
keywords = {Electrical Engineering and Computer Science.,Thesis},
publisher = {Massachusetts Institute of Technology},
school = {Massachussetts institute of Technology},
title = {{Using Experience in Learning and Problem Solving}},
url = {https://dspace.mit.edu/handle/1721.1/14779 http://groups.csail.mit.edu/medg/ftp/koton/using experience in learning and problem solving.pdf},
year = {1988}
}
@article{Kingma2014,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy},
eprint = {1412.6980},
month = {dec},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2014}
}
@article{Banakh2017,
abstract = {Atorvastatin and ticagrelor combination is a widely accepted therapy for secondary prevention of ischaemic heart disease. However, rhabdomyolysis is a well-known rare side effect of statins which should be considered when treatments are combined with cytochrome P450 3A4 enzyme inhibitors. We report a case of atorvastatin and ticagrelor associated severe rhabdomyolysis that progressed to multiorgan failure requiring renal replacement therapy, inotropes, intubation, and mechanical ventilation. Despite withdrawal of the precipitating cause and the supportive measures including renal replacement therapy, creatinine kinase increased due to ongoing rhabdomyolysis rapidly progressing to upper and lower limbs weakness. A muscle biopsy was performed to exclude myositis which confirmed extensive myonecrosis, consistent with statin associated rhabdomyolysis. After a prolonged ventilatory course in the intensive care unit, patient's condition improved with recovery from renal and liver dysfunction. The patient slowly regained her upper and lower limb function; she was successfully weaned off the ventilator and was discharged for rehabilitation. To our knowledge, this is a second case of statin associated rhabdomyolysis due to interaction between atorvastatin and ticagrelor. However, our case differed in that the patient was also on amlodipine, which is considered to be a weak cytochrome P450 3A4 inhibitor and may have further potentiated myotoxicity.},
author = {Banakh, Iouri and Kung, Ross and Tiruvoipati, Ravindranath and Gupta, Sachin and Haji, Kavi},
doi = {10.1155/2017/3801819},
issn = {2090-6420},
journal = {Case Reports Crit. Care},
pages = {1--4},
pmid = {28630772},
title = {{Severe Rhabdomyolysis due to Presumed Drug Interactions between Atorvastatin with Amlodipine and Ticagrelor}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28630772 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5463112 https://www.hindawi.com/journals/cricc/2017/3801819/},
volume = {2017},
year = {2017}
}
@incollection{Walsham1997a,
abstract = {An increasing interest is being shown, not least by IS researchers, in the socio-technical approach known as actor-network theory. The purpose of this paper is to assess the current and potential future contribution of the theory to IS research. A brief review of key concepts of the theory is given, some IS literature which uses the theory is described, and significant criticisms of the theory are examined in some detail. Finally, implications are drawn on the potential value of the theory for IS research in the future, with the broad conclusion being that it has much to offer in both theoretical and methodological terms.},
address = {Boston, MA},
author = {Walsham, G.},
booktitle = {Inf. Syst. Qual. Res.},
doi = {10.1007/978-0-387-35309-8_23},
pages = {466--480},
publisher = {Springer US},
title = {{Actor-Network Theory and IS Research: Current Status and Future Prospects}},
url = {http://link.springer.com/10.1007/978-0-387-35309-8{\_}23},
year = {1997}
}
@article{Khandaker2019,
abstract = {While comorbidity between coronary heart disease (CHD) and depression is evident, it is unclear whether the two diseases have shared underlying mechanisms. We performed a range of analyses in 367,703 unrelated middle-aged participants of European ancestry from UK Biobank, a population based cohort study, to assess whether comorbidity is primarily due to genetic or environmental factors, and to test whether cardiovascular risk factors and CHD are likely to be causally related to depression using Mendelian randomization. We showed family history of heart disease was associated with a 20{\%} increase in depression risk (95{\%} confidence interval [CI] 16{\%} to 24{\%}, p{\textless}0.0001), but a genetic risk score that is strongly associated with CHD risk was not associated with depression. An increase of one standard deviation in the CHD genetic risk score was associated with 71{\%} higher CHD risk, but 1{\%} higher depression risk (95{\%} CI 0{\%} to 3{\%}; p=0.11). Mendelian randomization analyses suggested that triglycerides, interleukin-6 (IL-6), and C-reactive protein (CRP) are likely causal risk factors for depression. The odds ratio for depression per standard deviation increase in genetically-predicted triglycerides was 1.18 (95{\%} CI 1.09 to 1.27; p=2× 10-5); per unit increase in genetically-predicted log-transformed IL-6 was 0.74 (95{\%} CI 0.62 to 0.89; p=0.0012); and per unit increase in genetically-predicted log-transformed CRP was 1.18 (95{\%} CI 1.07 to 1.29; p=0.0009). Our analyses suggest that comorbidity between depression and CHD arises largely from shared environmental factors. IL-6, CRP and triglycerides, are likely to be causally linked with depression, so could be targets for treatment and prevention of depression.},
author = {Khandaker, Golam M. and Zuber, Verena and Rees, Jessica M. B. and Carvalho, Livia and Mason, Amy M. and Foley, Christopher N. and Gkatzionis, Apostolos and Jones, Peter B. and Burgess, Stephen},
doi = {10.1038/s41380-019-0395-3},
issn = {1359-4184},
journal = {Mol. Psychiatry},
keywords = {Depression,Diagnostic markers,Genetics},
month = {mar},
pages = {1},
publisher = {Nature Publishing Group},
title = {{Shared mechanisms between coronary heart disease and depression: findings from a large UK general population-based cohort}},
url = {http://www.nature.com/articles/s41380-019-0395-3},
year = {2019}
}
@article{Naing2000,
abstract = {In direct age-adjustment, a common age-structured population is used as standard. This population may actually exist (e.g., United States population, 1999) or may be fictitious (e.g., two populations may be combined to create a standard). In indirect age-adjustment, a common set of age-specific rates is applied to the populations whose rates are to be standardized. The simplest and most useful form of indirect adjustment is the standardized mortality ratio (SMR) (5).},
author = {Naing, N N},
issn = {1394-195X},
journal = {Malays. J. Med. Sci.},
keywords = {direct,easy way,indirect,standardization methods},
month = {jan},
number = {1},
pages = {10--5},
pmid = {22844209},
publisher = {School of Medical Sciences, Universiti Sains Malaysia},
title = {{Easy way to learn standardization : direct and indirect methods.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22844209 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3406211 http://www.ncbi.nlm.nih.gov/pubmed/22844209{\%}0Ahttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3406211},
volume = {7},
year = {2000}
}
@article{Bijl-Brouwer2019,
abstract = {Public and social sector organizations are increasingly turning to innovation as a way to address the complex problems that society is facing. Design practice has already contributed significantly to public and social innovation, but to be effective at the public and social systems level, these practices must be adapted. This study investigates how five public and social innovation agencies adapted and used the core design practice of problem framing to address complex problems in society. The frames evolved according to nonlinear patterns through the co-evolution of problem and solution spaces. Practitioners adapted their framing practices to suit the complex social contexts by applying systemic design principles, pursuing multiple solutions and problem frames, and operationalizing wider research and thinking methods that align with the complex nature of each specific challenge. I argue that such practices require high-level expertise, and that capability building in public and social innovation should consider these emerging practices and levels of expertise.},
author = {van der Bijl-Brouwer, Mieke},
doi = {10.1016/j.sheji.2019.01.003},
issn = {24058718},
journal = {She Ji},
keywords = {Complexity,Design expertise,Design practice,Framing,Social innovation},
month = {mar},
number = {1},
pages = {29--43},
publisher = {Elsevier},
title = {{Problem Framing Expertise in Public and Social Innovation}},
url = {https://www.sciencedirect.com/science/article/pii/S2405872618301114},
volume = {5},
year = {2019}
}
@article{McNamara2017,
abstract = {Data wrangling is a critical foundation of data science, and wrangling of categorical data is an important component of this process. However, categorical data can introduce unique issues in data wrangling, particularly in real-world settings with collaborators and periodically-updated dynamic data. This paper discusses common problems arising from categorical variable transformations in R, demonstrates the use of factors, and suggests approaches to address data wrangling challenges. For each problem, we present at least two strategies for management, one in base R and the other from the ‘tidyverse.' We consider several motivating examples, suggest defensive coding strategies, and outline principles for data wrangling to help ensure data quality and sound analysis.},
author = {McNamara, Amelia and Horton, Nicholas J},
doi = {10.1080/00031305.2017.1356375},
issn = {0003-1305},
journal = {Am. Stat.},
keywords = {data derivation,data management,data science,statistical computing},
month = {jan},
pages = {0--0},
publisher = {PeerJ Inc.},
title = {{Wrangling categorical data in R}},
url = {https://peerj.com/preprints/3163/ https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1356375},
year = {2017}
}
@techreport{data_science_skills_report,
title = {{Dynamics of data science skills | Royal Society}},
url = {https://royalsociety.org/topics-policy/projects/dynamics-of-data-science/?gclid=EAIaIQobChMI0pvNzvSf4gIVT7ftCh0dgAAkEAAYASAAEgLZ7fD{\_}BwE}
}
@article{Wu2012,
abstract = {Background Mental disorders are widely recognized to be associated with increased risk of all-cause mortality. However, the extent to which highest-risk groups for mortality overlap with those viewed with highest concern by mental health services is less clear. The aim of the study was to investigate clinical risk assessment ratings for suicide, violence and self-neglect in relation to all-cause mortality among people receiving secondary mental healthcare. Method A total of 9234 subjects over the age of 15 years were identified from the South London and Maudsley Biomedical Research Centre Case Register who had received a second tier structured risk assessment in the course of their clinical care. A cohort analysis was carried out. Total scores for three risk assessment clusters (suicide, violence and self-neglect) were calculated and Cox regression models used to assess survival from first assessment. Results A total of 234 deaths had occurred over an average 9.4-month follow-up period. Mortality was relatively high for the cohort overall in relation to national norms [standardized mortality ratio 3.23, 95{\%} confidence interval (CI) 2.83-3.67] but not in relation to other mental health service users with similar diagnoses. Only the score for the self-neglect cluster predicted mortality [hazard ratio (HR) per unit increase 1.14, 95{\%} CI 1.04-1.24] with null findings for assessed risk of suicide or violence (HRs per unit increase 1.00 and 1.06 respectively). Conclusions Level of clinician-appraised risk of self-neglect, but not of suicide or violence, predicted all-cause mortality among people receiving specific assessment of risk in a secondary mental health service. {\textcopyright} 2011 Cambridge University Press.},
author = {Wu, C. Y. and Chang, C. K. and Hayes, R. D. and Broadbent, M. and Hotopf, M. and Stewart, R.},
doi = {10.1017/S0033291711002698},
issn = {00332917},
journal = {Psychol. Med.},
keywords = {Mortality,risk assessment,self-neglect,suicide,violence},
month = {aug},
number = {8},
pages = {1581--1590},
pmid = {22153124},
title = {{Clinical risk assessment rating and all-cause mortality in secondary mental healthcare: The South London and Maudsley NHS Foundation Trust Biomedical Research Centre (SLAM BRC) Case Register}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22153124 https://www.cambridge.org/core/product/identifier/S0033291711002698/type/journal{\_}article},
volume = {42},
year = {2012}
}
@article{Fey2019,
abstract = {We propose a dynamic neighborhood aggregation (DNA) procedure guided by (multi-head) attention for representation learning on graphs. In contrast to current graph neural networks which follow a simple neighborhood aggregation scheme, our DNA procedure allows for a selective and node-adaptive aggregation of neighboring embeddings of potentially differing locality. In order to avoid overfitting, we propose to control the channel-wise connections between input and output by making use of grouped linear projections. In a number of transductive node-classification experiments, we demonstrate the effectiveness of our approach.},
archivePrefix = {arXiv},
arxivId = {1904.04849},
author = {Fey, Matthias},
eprint = {1904.04849},
month = {apr},
title = {{Just Jump: Dynamic Neighborhood Aggregation in Graph Neural Networks}},
url = {http://arxiv.org/abs/1904.04849},
year = {2019}
}
@article{Howick2019,
abstract = {Malaria parasites adopt a remarkable variety of morphological life stages as they transition through multiple mammalian host and mosquito vector environments.We profiled the single-cell transcriptomes of thousands of individual parasites, deriving the first highresolution transcriptional atlas of the entire Plasmodium berghei life cycle. We then used our atlas to precisely define developmental stages of single cells from three different human malaria parasite species, including parasites isolated directly from infected individuals. The Malaria Cell Atlas provides both a comprehensive view of gene usage in a eukaryotic parasite and an open-access reference dataset for the study of malaria parasites.},
author = {Howick, Virginia M. and Russell, Andrew J.C. and Andrews, Tallulah and Heaton, Haynes and Reid, Adam J. and Natarajan, Kedar and Butungi, Hellen and Metcalf, Tom and Verzier, Lisa H. and Rayner, Julian C. and Berriman, Matthew and Herren, Jeremy K. and Billker, Oliver and Hemberg, Martin and Talman, Arthur M. and Lawniczak, Mara K.N.},
doi = {10.1126/science.aaw2619},
issn = {10959203},
journal = {Science (80-. ).},
month = {aug},
number = {6455},
pmid = {31439762},
publisher = {American Association for the Advancement of Science},
title = {{The malaria cell atlas: Single parasite transcriptomes across the complete Plasmodium life cycle}},
volume = {365},
year = {2019}
}
@article{Sajovic2016,
abstract = {In this paper we develop operational calculus on programming spaces that generalizes existing approaches to automatic differentiation of computer programs and provides a rigorous framework for program analysis through calculus. We present an abstract computing machine that models automatically differentiable computer programs. Computer programs are viewed as maps on a finite dimensional vector space called virtual memory space, which we extend by the tensor algebra of its dual to accommodate derivatives. The extended virtual memory is by itself an algebra of programs, a data structure one can calculate with, and its elements give the expansion of the original program as an infinite tensor series at program's input values. We define the operator of differentiation on programming spaces and implement a generalized shift operator in terms of its powers. Our approach offers a powerful tool for program analysis and approximation, and provides deep learning with a formal calculus. Such a calculus connects general programs with deep learning through operators that map both formulations to the same space. This equivalence enables a generalization of existing methods for neural analysis to any computer program, and vice versa. Several applications are presented, most notably a meaningful way of neural network initialization that leads to a process of program boosting.},
archivePrefix = {arXiv},
arxivId = {1610.07690},
author = {Sajovic, {\v{Z}}iga and Vuk, Martin},
eprint = {1610.07690},
month = {oct},
title = {{Operational calculus on programming spaces}},
url = {http://arxiv.org/abs/1610.07690},
year = {2016}
}
@article{Root-Bernstein2019,
abstract = {Tool use has been reported in a wide range of vertebrates, but so far not in Suidae (the pigs). Suidae are widely considered to be “intelligent” and have many traits associated with tool use, so this is surprising. Here, we report the first structured observations of umprompted instrumental object manipulation in a pig, the Visayan warty pig Sus cebifrons, which we argue qualifies as tool use. Three individuals were observed using bark or sticks to dig with. Two individuals, adult females, used the sticks or bark, using a rowing motion, during the final stage of nest building. The third individual, an adult male, attempted to use a stick to dig with. Stick and branch manipulation was observed in other contexts, but not for digging. Our observations suggest the hypothesis that the observed use of stick to dig with could have been socially learned through vertical transmission (mother-daughter) as well as horizontal transmission (female-male). When used by the females, it altered their digging affordance, and had a specific placement in the nest-building sequence. In addition to its context-specificity and its role in a functional sequence, the observed tool use is distinguished by an ambiguous function or effectiveness as a digging behaviour, and the participation of the male in a female action pattern. Observations of unprompted tool use represented for the first time in a phylogenetic family are rare. These open new possibilities for research on tool use and social learning in Suidae.},
author = {Root-Bernstein, Meredith and Narayan, Trupthi and Cornier, Lucile and Bourgeois, Aude},
doi = {10.1016/j.mambio.2019.08.003},
issn = {16165047},
journal = {Mamm. Biol.},
month = {sep},
pages = {102--110},
publisher = {Urban {\&} Fischer},
title = {{Context-specific tool use by Sus cebifrons}},
url = {https://www.sciencedirect.com/science/article/abs/pii/S1616504719300333{\#}abs0005},
volume = {98},
year = {2019}
}
@article{Mirowski2018,
abstract = {Navigating through unstructured environments is a basic capability of intelligent creatures, and thus is of fundamental interest in the study and development of artificial intelligence. Long-range navigation is a complex cognitive task that relies on developing an internal representation of space, grounded by recognisable landmarks and robust visual processing, that can simultaneously support continuous self-localisation ("I am here") and a representation of the goal ("I am going there"). Building upon recent research that applies deep reinforcement learning to maze navigation problems, we present an end-to-end deep reinforcement learning approach that can be applied on a city scale. Recognising that successful navigation relies on integration of general policies with locale-specific knowledge, we propose a dual pathway architecture that allows locale-specific features to be encapsulated, while still enabling transfer to multiple cities. We present an interactive navigation environment that uses Google StreetView for its photographic content and worldwide coverage, and demonstrate that our learning method allows agents to learn to navigate multiple cities and to traverse to target destinations that may be kilometres away. The project webpage http://streetlearn.cc contains a video summarising our research and showing the trained agent in diverse city environments and on the transfer task, the form to request the StreetLearn dataset and links to further resources. The StreetLearn environment code is available at https://github.com/deepmind/streetlearn},
archivePrefix = {arXiv},
arxivId = {1804.00168},
author = {Mirowski, Piotr and Grimes, Matthew Koichi and Malinowski, Mateusz and Hermann, Karl Moritz and Anderson, Keith and Teplyashin, Denis and Simonyan, Karen and Kavukcuoglu, Koray and Zisserman, Andrew and Hadsell, Raia},
eprint = {1804.00168},
month = {mar},
title = {{Learning to Navigate in Cities Without a Map}},
url = {http://arxiv.org/abs/1804.00168},
year = {2018}
}
@article{Shi2020,
abstract = {Artificial intelligence for social good (AI4SG) is a research theme that aims to use and advance artificial intelligence to address societal issues and improve the well-being of the world. AI4SG has received lots of attention from the research community in the past decade with several successful applications. Building on the most comprehensive collection of the AI4SG literature to date with over 1000 contributed papers, we provide a detailed account and analysis of the work under the theme in the following ways. (1) We quantitatively analyze the distribution and trend of the AI4SG literature in terms of application domains and AI techniques used. (2) We propose three conceptual methods to systematically group the existing literature and analyze the eight AI4SG application domains in a unified framework. (3) We distill five research topics that represent the common challenges in AI4SG across various application domains. (4) We discuss five issues that, we hope, can shed light on the future development of the AI4SG research.},
archivePrefix = {arXiv},
arxivId = {2001.01818},
author = {Shi, Zheyuan Ryan and Wang, Claire and Fang, Fei},
eprint = {2001.01818},
month = {jan},
title = {{Artificial Intelligence for Social Good: A Survey}},
url = {http://arxiv.org/abs/2001.01818},
year = {2020}
}
@article{Marcus2020,
abstract = {Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.},
archivePrefix = {arXiv},
arxivId = {2002.06177},
author = {Marcus, Gary},
eprint = {2002.06177},
month = {feb},
title = {{The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence}},
url = {http://arxiv.org/abs/2002.06177},
year = {2020}
}
@book{Irizarry2016,
abstract = {Coursera statistics genomic data analysis},
author = {Irizarry, Rafael A. and Love, Michael I.},
booktitle = {Data Anal. Life Sci. with R},
doi = {10.1201/9781315367002},
title = {{Data Analysis for the Life Sciences with R}},
url = {https://leanpub.com/dataanalysisforthelifesciences},
year = {2016}
}
@article{Rolfe2016,
abstract = {Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data, and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.},
archivePrefix = {arXiv},
arxivId = {1609.02200},
author = {Rolfe, Jason Tyler},
eprint = {1609.02200},
month = {sep},
title = {{Discrete Variational Autoencoders}},
url = {http://arxiv.org/abs/1609.02200},
year = {2016}
}
@article{Cangea2017,
abstract = {We propose two multimodal deep learning architectures that allow for cross-modal dataflow (XFlow) between the feature extractors, thereby extracting more interpretable features and obtaining a better representation than through unimodal learning, for the same amount of training data. These models can usefully exploit correlations between audio and visual data, which have a different dimensionality and are therefore nontrivially exchangeable. Our work improves on existing multimodal deep learning metholodogies in two essential ways: (1) it presents a novel method for performing cross-modality (before features are learned from individual modalities) and (2) extends the previously proposed cross-connections, which only transfer information between streams that process compatible data. Both cross-modal architectures outperformed their baselines (by up to 7.5{\%}) when evaluated on the AVletters dataset.},
archivePrefix = {arXiv},
arxivId = {1709.00572},
author = {Cangea, Cătălina and Veli{\v{c}}kovi{\'{c}}, Petar and Li{\`{o}}, Pietro},
eprint = {1709.00572},
month = {sep},
title = {{XFlow: 1D-2D Cross-modal Deep Neural Networks for Audiovisual Classification}},
url = {http://arxiv.org/abs/1709.00572},
year = {2017}
}
@article{Hamaker2015,
abstract = {The cross-lagged panel model (CLPM) is believed by many to overcome the problems associated with the use of cross-lagged correlations as a way to study causal influences in longitudinal panel data. The current article, however, shows that if stability of constructs is to some extent of a trait-like, timeinvariant nature, the autoregressive relationships of the CLPM fail to adequately account for this. As a result, the lagged parameters that are obtained with the CLPM do not represent the actual within-person relationships over time, and this may lead to erroneous conclusions regarding the presence, predominance, and sign of causal influences. In this article we present an alternative model that separates the within-person process from stable between-person differences through the inclusion of random intercepts, and we discuss how this model is related to existing structural equation models that include cross-lagged relationships. We derive the analytical relationship between the cross-lagged parameters from the CLPM and the alternative model, and use simulations to demonstrate the spurious results that may arise when using the CLPM to analyze data that include stable, trait-like individual differences. We also present a modeling strategy to avoid this pitfall and illustrate this using an empirical data set. The implications for both existing and future cross-lagged panel research are discussed.},
author = {Hamaker, Ellen L. and Kuiper, Rebecca M. and Grasman, Raoul P.P.P.},
doi = {10.1037/a0038889},
issn = {1082989X},
journal = {Psychol. Methods},
keywords = {Cross-lagged panel,Longitudinal model,Reciprocal effects,Trait-state models,Within-person dynamics},
month = {mar},
number = {1},
pages = {102--116},
pmid = {25822208},
title = {{A critique of the cross-lagged panel model}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25822208 http://doi.apa.org/getdoi.cfm?doi=10.1037/a0038889},
volume = {20},
year = {2015}
}
@article{Wu2018,
abstract = {Objective: Unlocking the data contained within both structured and unstructured components of electronic health records (EHRs) has the potential to provide a step change in data available for secondary research use, generation of actionable medical insights, hospital management, and trial recruitment. To achieve this, we implemented SemEHR, an open source semantic search and analytics tool for EHRs. Methods: SemEHR implements a generic information extraction (IE) and retrieval infrastructure by identifying contextualized mentions of a wide range of biomedical concepts within EHRs. Natural language processing annotations are further assembled at the patient level and extended with EHR-specific knowledge to generate a timeline for each patient. The semantic data are serviced via ontology-based search and analytics interfaces. Results: SemEHR has been deployed at a number of UK hospitals, including the Clinical Record Interactive Search, an anonymized replica of the EHR of the UK South London and Maudsley National Health Service Foun-dation Trust, one of Europe's largest providers of mental health services. In 2 Clinical Record Interactive Search–based studies, SemEHR achieved 93{\%} (hepatitis C) and 99{\%} (HIV) F-measure results in identifying true positive patients. At King's College Hospital in London, as part of the CogStack program (github.com/cogstack), SemEHR is being used to recruit patients into the UK Department of Health 100 000 Genomes Project (genomic-sengland.co.uk). The validation study suggests that the tool can validate previously recruited cases and is very fast at searching phenotypes; time for recruitment criteria checking was reduced from days to minutes. Vali-dated on open intensive care EHR data, Medical Information Mart for Intensive Care III, the vital signs extracted by SemEHR can achieve around 97{\%} accuracy. Conclusion: Results from the multiple case studies demonstrate SemEHR's efficiency: weeks or months of work can be done within hours or minutes in some cases. SemEHR provides a more comprehensive view of patients, bringing in more and unexpected insight compared to study-oriented bespoke IE systems. SemEHR is open source, available at https://github.com/CogStack/SemEHR.},
author = {Wu, Honghan and Toti, Giulia and Morley, Katherine I and Ibrahim, Zina M and Folarin, Amos and Jackson, Richard and Kartoglu, Ismail and Agrawal, Asha and Stringer, Clive and Gale, Darren and Gorrell, Genevieve and Roberts, Angus and Broadbent, Matthew and Stewart, Robert and Dobson, Richard JB},
doi = {10.1093/JAMIA/OCX160},
isbn = {1527974X (Electronic)},
issn = {1527974X},
journal = {J. Am. Med. Informatics Assoc.},
keywords = {FHIR,Information extraction,NLP,Ontology,Patient recruitment,Secondary use of EHR,Semantic search},
month = {may},
number = {5},
pages = {530--537},
pmid = {29361077},
publisher = {Oxford University Press},
title = {{SemEHR: A general-purpose semantic search system to surface semantic data from clinical notes for tailored care, trial recruitment, and clinical research}},
url = {https://academic.oup.com/jamia/article/25/5/530/4817428},
volume = {25},
year = {2018}
}
@article{Lenzen1944,
author = {Lenzen, V. F. and Craik, K. J. W.},
doi = {10.2307/2181361},
issn = {00318108},
journal = {Philos. Rev.},
month = {sep},
number = {5},
pages = {503},
title = {{The Nature of Explanation.}},
url = {https://www.jstor.org/stable/2181361?origin=crossref},
volume = {53},
year = {1944}
}
@article{Cakir2017,
abstract = {Lithium is a cornerstone in treatment of bipolar disorder. Findings are conflicting as to whether acquired unresponsiveness occurs following the discontinuation. Retrospective life chart data were evaluated to investigate the incidence of loss of response. Sixty-five patients chosen from a larger cohort, followed with prospective life charts, who discontinued lithium and had a second lithium treatment. Patients who had at least 2 mood episodes when they were drug na{\"{i}}ve to describe the natural frequency of illness and 3 mood episodes before the discontinuation were included. The type of response was defined as excellent, partial, or poor according to mirror design method. Eighteen of 65 patients (27.6{\%}) had a decreased response to lithium following its discontinuation. Nine of these patients (13.8{\%}) were unresponsive and nine patients (13.8{\%}) had attenuated response to second lithium treatment. The mean time of discontinuation was longer in the patients who show decreased response (245.8+268.2 vs. 117.9+149.8 days, p=.01). Those who had episode recurrences during the discontinuation were more likely to show reduced responsiveness upon re-treatment. After discontinuation of lithium treatment, more than a quarter of the patients showed an attenuated response or unresponsiveness, and initial partial responders more likely to show unresponsiveness than excellent responders.},
author = {Cakir, Sibel and Yazıcı, Olcay and Post, Robert M.},
doi = {10.1016/j.psychres.2016.11.046},
issn = {18727123},
journal = {Psychiatry Res.},
keywords = {Bipolar disorders,Discontinuation,Lithium,Long term treatment,Response,Treatment resistance,Unresponsiviness},
month = {jan},
pages = {305--309},
pmid = {27974284},
title = {{Decreased responsiveness following lithium discontinuation in bipolar disorder: A naturalistic observation study}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27974284 https://linkinghub.elsevier.com/retrieve/pii/S016517811630628X},
volume = {247},
year = {2017}
}
@inproceedings{Yao2018,
abstract = {Estimating individual treatment effect (ITE) is a challenging problem in causal inference, due to the missing counterfactuals and the selection bias. Existing ITE estimation methods mainly focus on balancing the distributions of control and treated groups, but ignore the local similarity information that provides meaningful constraints on the ITE estimation. In this paper, we propose a local similarity preserved individual treatment effect (SITE) estimation method based on deep representation learning. SITE preserves local similarity and balances data distributions simultaneously, by focusing on several hard samples in each mini-batch. Experimental results on synthetic and three real-world datasets demonstrate the advantages of the proposed SITE method, compared with the state-of-the-art ITE estimation methods.},
author = {Yao, Liuyi and Li, Sheng and Li, Yaliang and Huai, Mengdi and Gao, Jing and Zhang, Aidong},
booktitle = {Neural Inf. Process. Syst.},
number = {Nips},
pages = {2638--2648},
title = {{Representation Learning for Treatment Effect Estimation from Observational Data}},
url = {https://papers.nips.cc/paper/7529-representation-learning-for-treatment-effect-estimation-from-observational-data},
year = {2018}
}
@article{TIBSHIRANI1997,
abstract = {I propose a new method for variable selection and shrinkage in Cox's proportional hazards model. My proposal minimizes the log partial likelihood subject to the sum of the absolute values of the parameters being bounded by a constant. Because of the nature of this constraint, it shrinks coefficients and produces some coefficients that are exactly zero. As a result it reduces the estimation variance while providing an interpretable final model. The method is a variation of the 'lasso' proposal of Tibshirani, designed for the linear regression context. Simulations indicate that the lasso can be more accurate than stepwise selection in this setting.},
author = {Tibshirani, Robert},
doi = {10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3},
isbn = {0277-6715 (Print)},
issn = {02776715},
journal = {Stat. Med.},
month = {feb},
number = {4},
pages = {385--395},
pmid = {9044528},
publisher = {John Wiley {\&} Sons, Ltd.},
title = {{The lasso method for variable selection in the cox model}},
url = {http://doi.wiley.com/10.1002/{\%}28SICI{\%}291097-0258{\%}2819970228{\%}2916{\%}3A4{\%}3C385{\%}3A{\%}3AAID-SIM380{\%}3E3.0.CO{\%}3B2-3},
volume = {16},
year = {1997}
}
@article{Han2019,
abstract = {Innovation, creativity, and competition are some of the fundamental underlying forces driving the advances in Artificial Intelligence (AI). This race for technological supremacy creates a complex ecology of choices that may lead to negative consequences, in particular, when ethical and safety procedures are underestimated or even ignored. Here we resort to a novel game theoretical framework to describe the ongoing AI bidding war, also allowing for the identification of procedures on how to influence this race to achieve desirable outcomes. By exploring the similarities between the ongoing competition in AI and evolutionary systems, we show that the timelines in which AI supremacy can be achieved play a crucial role for the evolution of safety prone behaviour and whether influencing procedures are required. When this supremacy can be achieved in a short term (near AI), the significant advantage gained from winning a race leads to the dominance of those who completely ignore the safety precautions to gain extra speed, rendering of the presence of reciprocal behavior irrelevant. On the other hand, when such a supremacy is a distant future, reciprocating on others' safety behaviour provides in itself an efficient solution, even when monitoring of unsafe development is hard. Our results suggest under what conditions AI safety behaviour requires additional supporting procedures and provide a basic framework to model them.},
archivePrefix = {arXiv},
arxivId = {1907.12393},
author = {Han, The Anh and Pereira, Luis Moniz and Santos, Francisco C. and Lenaerts, Tom},
eprint = {1907.12393},
month = {jul},
title = {{Modelling the Safety and Surveillance of the AI Race}},
url = {http://arxiv.org/abs/1907.12393},
year = {2019}
}
@article{Hastie2015,
abstract = {The matrix-completion problem has attracted a lot of attention, largely as a result of the celebrated Netflix competition. Two popular approaches for solving the problem are nuclear-norm- regularized matrix approximation (Candes and Tao, 2009; Mazumder et al., 2010), and maximum-margin matrix factorization (Srebro et al., 2005). These two procedures are in some cases solving equivalent problems, but with quite different algorithms. In this article we bring the two approaches together, leading to an efficient algorithm for large matrix factorization and completion that outperforms both of these. We develop a software package softImpute in R for implementing our approaches, and a distributed version for very large matrices using the Spark cluster programming environment},
archivePrefix = {arXiv},
arxivId = {1410.2596},
author = {Hastie, Trevor and Mazumder, Rahul and Lee, Jason D. and Zadeh, Reza},
eprint = {1410.2596},
issn = {15337928},
journal = {J. Mach. Learn. Res.},
keywords = {alternating least squares,matrix completion,nuclear norm,svd},
number = {1},
pages = {3367--3402},
publisher = {MIT Press},
title = {{Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares}},
url = {http://dl.acm.org/citation.cfm?id=2789272.2912106 http://jmlr.org/papers/v16/hastie15a.html},
volume = {16},
year = {2015}
}
@article{Peters2014,
abstract = {We consider the problem of learning causal directed acyclic graphs from an observational joint distribution. One can use these graphs to predict the outcome of interventional experiments, from which data are often not available. We show that if the observational distribution follows a structural equation model with an additive noise structure, the directed acyclic graph becomes identifiable from the distribution under mild conditions. This constitutes an interesting alternative to traditional methods that assume faithfulness and identify only the Markov equivalence class of the graph, thus leaving some edges undirected. We provide practical algorithms for finitely many samples, RESIT (regression with subsequent independence test) and two methods based on an independence score. We prove that RESIT is correct in the population setting and provide an empirical evaluation. {\textcopyright} 2014 Jonas Peters, Joris M. Mooij, Dominik Janzing and Bernhard Sch{\"{o}}lkopf.},
archivePrefix = {arXiv},
arxivId = {1309.6779},
author = {Peters, Jonas and Mooij, Joris M. and Janzing, Dominik and Sch{\"{o}}lkopf, Bernhard},
eprint = {1309.6779},
issn = {15337928},
journal = {J. Mach. Learn. Res.},
keywords = {Additive noise,Bayesian networks,Causal inference,Causal minimality,Identifiability,Structural equation models},
pages = {2009--2053},
title = {{Causal discovery with continuous additive noise models}},
url = {https://papers.nips.cc/paper/3548-nonlinear-causal-discovery-with-additive-noise-models},
volume = {15},
year = {2014}
}
@article{Gordon2018,
abstract = {This paper introduces a new framework for data efficient and versatile learning. Specifically: 1) We develop ML-PIP, a general framework for Meta-Learning approximate Probabilistic Inference for Prediction. ML-PIP extends existing probabilistic interpretations of meta-learning to cover a broad class of methods. 2) We introduce $\backslash$Versa{\{}{\}}, an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass. $\backslash$Versa{\{}{\}} substitutes optimization at test time with forward passes through inference networks, amortizing the cost of inference and relieving the need for second derivatives during training. 3) We evaluate $\backslash$Versa{\{}{\}} on benchmark datasets where the method sets new state-of-the-art results, handles arbitrary numbers of shots, and for classification, arbitrary numbers of classes at train and test time. The power of the approach is then demonstrated through a challenging few-shot ShapeNet view reconstruction task.},
archivePrefix = {arXiv},
arxivId = {1805.09921},
author = {Gordon, Jonathan and Bronskill, John and Bauer, Matthias and Nowozin, Sebastian and Turner, Richard E.},
eprint = {1805.09921},
month = {may},
title = {{Meta-Learning Probabilistic Inference For Prediction}},
url = {http://arxiv.org/abs/1805.09921},
year = {2018}
}
@book{Clinton2016,
author = {Clinton, Ed Jr.},
isbn = {9781515412007},
publisher = {Wilder Publications, Inc.},
title = {{The small world of m-75}},
year = {2016}
}
@article{Dumoulin2018,
abstract = {Many real-world problems require integrating multiple sources of information. Sometimes these problems involve multiple, distinct modalities of information — vision, language, audio, etc. — as is required to understand a scene in a movie or answer a question about an image. Other times, these problems involve multiple sources of the same kind of input, i.e. when summarizing several documents or drawing one image in the style of another. When approaching such problems, it often makes sense to process one source of information in the context of another; for instance, in the right example above, one can extract meaning from the image in the context of the question. In machine learning, we often refer to this context-based processing as conditioning: the computation carried out by a model is conditioned or modulated by information extracted from an auxiliary input. Finding an effective way to condition on or fuse sources of information is an open research problem, and in this article, we concentrate on a specific family of approaches we call feature-wise transformations. We will examine the use of feature-wise transformations in many neural network architectures to solve a surprisingly large and diverse set of problems; their success, we will argue, is due to being flexible enough to learn an effective representation of the conditioning input in varied settings. In the language of multi-task learning, where the conditioning signal is taken to be a task description, feature-wise transformations learn a task representation which allows them to capture and leverage the relationship between multiple sources of information, even in remarkably different problem settings.},
author = {Dumoulin, Vincent and Perez, Ethan and Schucher, Nathan and Strub, Florian and Vries, Harm and Courville, Aaron and Bengio, Yoshua},
doi = {10.23915/distill.00011},
issn = {2476-0757},
journal = {Distill},
month = {jul},
number = {7},
pages = {e11},
title = {{Feature-wise transformations}},
url = {https://distill.pub/2018/feature-wise-transformations},
volume = {3},
year = {2018}
}
@inproceedings{Wu2018a,
abstract = {Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems --- BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6{\%} lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.},
archivePrefix = {arXiv},
arxivId = {1803.08494},
author = {Wu, Yuxin and He, Kaiming},
booktitle = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
doi = {10.1007/978-3-030-01261-8_1},
eprint = {1803.08494},
isbn = {9783030012601},
issn = {16113349},
month = {mar},
pages = {3--19},
title = {{Group normalization}},
url = {http://arxiv.org/abs/1803.08494},
volume = {11217 LNCS},
year = {2018}
}
@article{Rockova2018,
abstract = {Despite the wide adoption of spike-and-slab methodology for Bayesian variable selection, its potential for penalized likelihood estimation has largely been overlooked. In this paper, we bridge this gap by cross-fertilizing these two paradigms with the Spike-and-Slab LASSO procedure for variable selection and parameter estimation in linear regression. We introduce a new class of self-adaptive penalty functions that arise from a fully Bayes spike-and-slab formulation, ultimately moving beyond the separable penalty framework. A virtue of these non-separable penalties is their ability to borrow strength across coordinates, adapt to en- semble sparsity information and exert multiplicity adjustment. The Spike-and-Slab LASSO procedure harvests efficient Bayesian EM and coordinate-wise implementations with a path- following scheme for dynamic posterior exploration. We show on simulated data that the fully Bayes penalty mimics oracle performance, providing a viable alternative to cross-validation. We develop theory for the separable and non-separable variants of the penalty, showing rate- optimality of the global mode as well as optimal posterior concentration when p {\textgreater} n. Thus, the modal estimates can be supplemented with meaningful uncertainty assessments.},
author = {Ro{\v{c}}kov{\'{a}}, Veronika and George, Edward I.},
doi = {10.1080/01621459.2016.1260469},
issn = {1537274X},
journal = {J. Am. Stat. Assoc.},
keywords = {High-dimensional regression,LASSO,Penalized likelihood,Posterior concentration,Spike-and-Slab,Variable selection},
month = {jan},
number = {521},
pages = {431--444},
pmid = {54},
title = {{The Spike-and-Slab LASSO}},
url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2016.1260469},
volume = {113},
year = {2018}
}
@article{Qiu2017a,
abstract = {Single-cell trajectories can unveil how gene regulation governs cell fate decisions. However, learning the structure of complex trajectories with multiple branches remains a challenging computational problem. We present Monocle 2, an algorithm that uses reversed graph embedding to describe multiple fate decisions in a fully unsupervised manner. We applied Monocle 2 to two studies of blood development and found that mutations in the genes encoding key lineage transcription factors divert cells to alternative fates.},
author = {Qiu, Xiaojie and Mao, Qi and Tang, Ying and Wang, Li and Chawla, Raghav and Pliner, Hannah A. and Trapnell, Cole},
doi = {10.1038/nmeth.4402},
issn = {15487105},
journal = {Nat. Methods},
month = {oct},
number = {10},
pages = {979--982},
publisher = {Nature Publishing Group},
title = {{Reversed graph embedding resolves complex single-cell trajectories}},
volume = {14},
year = {2017}
}
@article{Wayne2018,
abstract = {Animals execute goal-directed behaviours despite the limited range and scope of their sensors. To cope, they explore environments and store memories maintaining estimates of important information that is not presently available. Recently, progress has been made with artificial intelligence (AI) agents that learn to perform tasks from sensory input, even at a human level, by merging reinforcement learning (RL) algorithms with deep neural networks, and the excitement surrounding these results has led to the pursuit of related ideas as explanations of non-human animal learning. However, we demonstrate that contemporary RL algorithms struggle to solve simple tasks when enough information is concealed from the sensors of the agent, a property called "partial observability". An obvious requirement for handling partially observed tasks is access to extensive memory, but we show memory is not enough; it is critical that the right information be stored in the right format. We develop a model, the Memory, RL, and Inference Network (MERLIN), in which memory formation is guided by a process of predictive modeling. MERLIN facilitates the solution of tasks in 3D virtual reality environments for which partial observability is severe and memories must be maintained over long durations. Our model demonstrates a single learning agent architecture that can solve canonical behavioural tasks in psychology and neurobiology without strong simplifying assumptions about the dimensionality of sensory input or the duration of experiences.},
archivePrefix = {arXiv},
arxivId = {1803.10760},
author = {Wayne, Greg and Hung, Chia-Chun and Amos, David and Mirza, Mehdi and Ahuja, Arun and Grabska-Barwinska, Agnieszka and Rae, Jack and Mirowski, Piotr and Leibo, Joel Z. and Santoro, Adam and Gemici, Mevlana and Reynolds, Malcolm and Harley, Tim and Abramson, Josh and Mohamed, Shakir and Rezende, Danilo and Saxton, David and Cain, Adam and Hillier, Chloe and Silver, David and Kavukcuoglu, Koray and Botvinick, Matt and Hassabis, Demis and Lillicrap, Timothy},
eprint = {1803.10760},
month = {mar},
title = {{Unsupervised Predictive Memory in a Goal-Directed Agent}},
url = {http://arxiv.org/abs/1803.10760},
year = {2018}
}
@misc{R_shiny,
author = {Chang, Winston and Cheng, Joe and Allaire, JJ and Xie, Yihui and McPherson, Jonathan},
title = {{shiny: Web Application Framework for R}},
url = {https://cran.r-project.org/package=shiny},
year = {2017}
}
@techreport{Tran2018,
abstract = {We describe Bayesian Layers, a module designed for fast experimentation with neural network uncertainty. It extends neural network libraries with layers capturing uncertainty over weights (Bayesian neural nets), pre-activation units (dropout), activations ("stochastic output layers"), and the function itself (Gaussian processes). With reversible layers, one can also propagate uncertainty from input to output such as for flow-based distributions and constant-memory backpropa-gation. Bayesian Layers are a drop-in replacement for other layers, maintaining core features that one typically desires for experimentation. As demonstration, we fit a 10-billion parameter "Bayesian Transformer" on 512 TPUv2 cores, which replaces attention layers with their Bayesian counterpart. 1 batch{\_}size = 256 features, labels = load{\_}dataset(batch{\_}size) lstm = layers.LSTMCellReparameterization(512) output{\_}layer = tf.keras.layers.Dense(labels.shape[-1]) state = lstm.get{\_}initial{\_}state(features) nll = 0. for t in range(features.shape[1]): net, state = lstm(features[:, t], state) logits = output{\_}layer(net) nll += tf.losses.softmax{\_}cross{\_}entropy(onehot{\_}labels=labels[:, t], logits=logits) kl = sum(lstm.losses) loss = nll + kl optimizer = tf.train.AdamOptimizer() train{\_}op = optimizer.minimize(loss) Figure 1: Bayesian RNN (Fortunato et al., 2017). Bayesian Layers integrate easily into existing work-flows. x t b h W x W h W y b y h t {\textperiodcentered} {\textperiodcentered} {\textperiodcentered} {\textperiodcentered} {\textperiodcentered} {\textperiodcentered} y t Figure 2: Graphical model depiction. Default arguments specify learnable distributions over the LSTM's weights and biases; we apply a deterministic output layer.},
archivePrefix = {arXiv},
arxivId = {1812.03973v1},
author = {Tran, Dustin and Dusenberry, Mike and {Van Der Wilk}, Mark and Hafner, Danijar and Brain, Google},
doi = {arXiv:1812.03973v1},
eprint = {1812.03973v1},
month = {dec},
title = {{Bayesian Layers: A Module for Neural Network Uncertainty}},
url = {http://arxiv.org/abs/1812.03973 http://bit.ly/2PA38Vk.},
year = {2018}
}
@article{Winschiers-Theophilus2013,
abstract = {Current human-computer interaction (HCI) paradigms are deeply rooted in a Western epistomology that attest its partiality and bias of its embedded assumptions, values, definitions, techiques and derived frameworks an models. Thus tensions created between local cultures and HCI principles require researchers to pursue a more critical research agenda within an indigenous epistomology. In this article an Afro-cetric paradigm is presented, as promoted by African scholars, as an alternative perspective to guide interaction design in a situated context in Africa and promote the refraiming of HCI. A practical realization of this paradigm shift within out own community-driven design in Southern Africa is illustrated.},
author = {Winschiers-Theophilus, Heike and Bidwell, Nicola J.},
doi = {10.1080/10447318.2013.765763},
issn = {10447318},
journal = {Int. J. Hum. Comput. Interact.},
month = {mar},
number = {4},
pages = {243--255},
publisher = {Taylor {\&} Francis Group},
title = {{Toward an Afro-Centric Indigenous HCI Paradigm}},
url = {http://www.tandfonline.com/doi/abs/10.1080/10447318.2013.765763},
volume = {29},
year = {2013}
}
@article{Aminikhanghahi2017,
abstract = {Change points are abrupt variations in time series data. Such abrupt changes may represent transitions that occur between states. Detection of change points is useful in modelling and prediction of time series and is found in application areas such as medical condition monitoring, climate change detection, speech and image analysis, and human activity analysis. This survey article enumerates, categorizes, and compares many of the methods that have been proposed to detect change points in time series. The methods examined include both supervised and unsupervised algorithms that have been introduced and evaluated. We introduce several criteria to compare the algorithms. Finally, we present some grand challenges for the community to consider.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Aminikhanghahi, Samaneh and Cook, Diane J},
doi = {10.1007/s10115-016-0987-z},
eprint = {15334406},
isbn = {9780128000977},
issn = {02193116},
journal = {Knowl. Inf. Syst.},
keywords = {Change point detection,Data mining,Machine learning,Segmentation,Time series data},
month = {may},
number = {2},
pages = {339--367},
pmid = {24655651},
publisher = {NIH Public Access},
title = {{A survey of methods for time series change point detection}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28603327 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5464762},
volume = {51},
year = {2017}
}
@inproceedings{Sokol2018,
address = {California},
author = {Sokol, Kacper and Flach, Peter},
booktitle = {Proc. Twenty-Seventh Int. Jt. Conf. Artif. Intell.},
doi = {10.24963/ijcai.2018/836},
isbn = {9780999241127},
month = {jul},
pages = {5785--5786},
publisher = {International Joint Conferences on Artificial Intelligence Organization},
title = {{Conversational Explanations of Machine Learning Predictions Through Class-contrastive Counterfactual Statements}},
url = {https://www.ijcai.org/proceedings/2018/836},
year = {2018}
}
@misc{elastic_net_matlab,
title = {{Lasso and Elastic Net - MATLAB MathWorks}},
url = {https://uk.mathworks.com/help/stats/lasso-and-elastic-net.html},
urldate = {2017-11-21}
}
@misc{tensorflow_recommender_old,
title = {{Implementing a recommendation system on Tensorflow – Felipe Salvatore – Medium}},
url = {https://medium.com/@felsal/implementing-a-recommendation-system-on-tensorflow-e41b80793165},
urldate = {2019-04-02}
}
@article{Delaney2019,
abstract = {Single-cell transcriptomic studies are identifying novel cell populations with exciting functional roles in various in vivo contexts, but identification of succinct gene marker panels for such populations remains a challenge. In this work, we introduce COMET, a computational framework for the identification of candidate marker panels consisting of one or more genes for cell populations of interest identified with single-cell RNA-seq data. We show that COMET outperforms other methods for the identification of single-gene panels and enables, for the first time, prediction of multi-gene marker panels ranked by relevance. Staining by flow cytometry assay confirmed the accuracy of COMET's predictions in identifying marker panels for cellular subtypes, at both the single- and multi-gene levels, validating COMET's applicability and accuracy in predicting favorable marker panels from transcriptomic input. COMET is a general non-parametric statistical framework and can be used as-is on various high-throughput datasets in addition to single-cell RNA-sequencing data. COMET is available for use via a web interface (http://www.cometsc.com/) or a stand-alone software package (https://github.com/MSingerLab/COMETSC).},
author = {Delaney, Conor and Schnell, Alexandra and Cammarata, Louis V and Yao-Smith, Aaron and Regev, Aviv and Kuchroo, Vijay K and Singer, Meromit},
doi = {10.15252/msb.20199005},
issn = {1744-4292},
journal = {Mol. Syst. Biol.},
keywords = {cell types,computational biology,data analysis,marker panel,single-cell RNA-seq},
month = {oct},
number = {10},
pages = {e9005},
title = {{Combinatorial prediction of marker panels from single-cell transcriptomic data.}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.15252/msb.20199005},
volume = {15},
year = {2019}
}
@article{Blei2017,
abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
archivePrefix = {arXiv},
arxivId = {1601.00670},
author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
doi = {10.1080/01621459.2017.1285773},
eprint = {1601.00670},
isbn = {1601.00670},
issn = {1537274X},
journal = {J. Am. Stat. Assoc.},
keywords = {Algorithms,Computationally intensive methods,Statistical computing},
month = {apr},
number = {518},
pages = {859--877},
pmid = {303902},
title = {{Variational Inference: A Review for Statisticians}},
url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1285773},
volume = {112},
year = {2017}
}
@article{Ding2018,
abstract = {Single-cell RNA-sequencing has great potential to discover cell types, identify cell states, trace development lineages, and reconstruct the spatial organization of cells. However, dimension reduction to interpret structure in single-cell sequencing data remains a challenge. Existing algorithms are either not able to uncover the clustering structures in the data or lose global information such as groups of clusters that are close to each other. We present a robust statistical model, scvis, to capture and visualize the low-dimensional structures in single-cell gene expression data. Simulation results demonstrate that low-dimensional representations learned by scvis preserve both the local and global neighbor structures in the data. In addition, scvis is robust to the number of data points and learns a probabilistic parametric mapping function to add new data points to an existing embedding. We then use scvis to analyze four single-cell RNA-sequencing datasets, exemplifying interpretable two-dimensional representations of the high-dimensional single-cell RNA-sequencing data.},
author = {Ding, Jiarui and Condon, Anne and Shah, Sohrab P.},
doi = {10.1038/s41467-018-04368-5},
issn = {20411723},
journal = {Nat. Commun.},
keywords = {Bioinformatics,Computational models,Machine learning},
month = {dec},
number = {1},
pages = {1--13},
publisher = {Nature Publishing Group},
title = {{Interpretable dimensionality reduction of single cell transcriptome data with deep generative models}},
volume = {9},
year = {2018}
}
@article{Wiegel2004,
abstract = {Using recent progress in biological scaling, we explore the way in which the immune system of an animal scales with its mass (M). It is shown that the number of cells in a single clone of B cells should scale as M and that the B-cell repertoire scales as ln (cM), where c is a constant. The time that a B cell needs to circulate once through the organism is shown to scale as M(1/4)ln (cM). It is suggested that the scaling of other cell populations in the immune system could be derived from these scaling relations for B cells.},
author = {Wiegel, Frederik W and Perelson, Alan S},
doi = {10.1046/j.0818-9641.2004.01229.x},
isbn = {0818-9641 (Print)},
issn = {08189641},
journal = {Immunol. Cell Biol.},
keywords = {Protectons,Scaling laws,Theoretical immunology},
number = {2},
pages = {127--131},
pmid = {15061763},
title = {{Some scaling principles for the immune system}},
volume = {82},
year = {2004}
}
@article{Angst2005,
abstract = {There is no data on the variation in the suicide risk over lifetime and on the suicide-preventive effect of the long-term treatment of mood-disorder patients with antidepressants and neuroleptics. Our research focused on 186 unipolar (D), 60 bipolar II (Dm), 130 nuclear bipolar I (MD), and 30 preponderantly manic patients (M/Md); that were followed-up from 1963 to 2003. By 2003, 45 (11.1{\%}) of the 406 patients had committed suicide. Suicide rates were highest among D patients (Standardized Mortality Ratio, SMR = 26.4), MD (SMR = 13.6), Dm (SMR = 10.6) and lowest among M/Md patients (SMR -4.7). Prospectively, the suicide rate decreased over the 44 years' follow-up; Lithium, neuroleptics and antidepressants reduced suicides significantly. Long-term treatment also reduced overall mortality, and combined treatments proved more effective than mono-therapy. Copyright {\textcopyright} International Academy for Suicide Research.},
author = {Angst, Jules and Angst, Felix and Gerber-Werder, Regina and Gamma, Alex},
doi = {10.1080/13811110590929488},
issn = {13811118},
journal = {Arch. Suicide Res.},
keywords = {Antidepressants,Bipolar disorders,Lithium,Major depressive disorder,Neuroleptics,Suicide},
month = {jul},
number = {3},
pages = {279--300},
pmid = {16020171},
title = {{Suicide in 406 mood-disorder patients with and without long-term medication: A 40 to 44 years' follow-up}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16020171 http://www.tandfonline.com/doi/abs/10.1080/13811110590929488},
volume = {9},
year = {2005}
}
@article{Bhattacharya2018a,
abstract = {Chronic Kidney Disease (CKD) is one of several conditions that affect a growing percentage of the US population; the disease is accompanied by multiple co-morbidities, and is hard to diagnose in-and-of itself. In its advanced forms it carries severe outcomes and can lead to death. It is thus important to detect the disease as early as possible, which can help devise effective intervention and treatment plan. Here we investigate ways to utilize information available in electronic health records (EHRs) from regular office visits of more than 13,000 patients, in order to distinguish among several stages of the disease. While clinical data stored in EHRs provide valuable information for risk-stratification, one of the major challenges in using them arises from data imbalance. That is, records associated with a more severe condition are typically under-represented compared to those associated with a milder manifestation of the disease. To address imbalance, we propose and develop a sampling-based ensemble approach, hierarchical meta-classification, aiming to stratify CKD patients into severity stages, using simple quantitative non-text features gathered from standard office visit records. The proposed hierarchical meta-classification method frames the multiclass classification task as a hierarchy of two subtasks. The first is binary classification, separating records associated with the majority class from those associated with all minority classes combined, using meta-classification. The second subtask separates the records assigned to the combined minority classes into the individual constituent classes. The proposed method identifies a significant proportion of patients suffering from the more advanced stages of the condition, while also correctly identifying most of the less severe cases, maintaining high sensitivity, specificity and F-measure (≥ 93{\%}). Our results show that the high level of performance attained by our method is preserved even when the size of the training set is significantly reduced, demonstrating the stability and generalizability of our approach. We present a new approach to perform classification while addressing data imbalance, which is inherent in the biomedical domain. Our model effectively identifies severity stages of CKD patients, using information readily available in office visit records within the realistic context of high data imbalance.},
author = {Bhattacharya, Moumita and Jurkovitz, Claudine and Shatkay, Hagit},
doi = {10.1186/s12911-018-0675-x},
issn = {14726947},
journal = {BMC Med. Inform. Decis. Mak.},
keywords = {Electronic health records,Hierarchical classification,Imbalanced data,Kidney disease,Meta-classification},
month = {dec},
number = {S4},
pages = {125},
publisher = {BioMed Central},
title = {{Chronic Kidney Disease stratification using office visit records: Handling data imbalance via hierarchical meta-classification}},
url = {https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-018-0675-x},
volume = {18},
year = {2018}
}
@misc{mimic_website,
title = {{MIMIC}},
url = {https://mimic.physionet.org/},
year = {2018}
}
@article{McInnes2018,
abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
archivePrefix = {arXiv},
arxivId = {1802.03426},
author = {McInnes, Leland and Healy, John and Melville, James},
eprint = {1802.03426},
month = {feb},
title = {{UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction}},
url = {http://arxiv.org/abs/1802.03426},
year = {2018}
}
@article{Aminikhanghahi2017a,
abstract = {Change points are abrupt variations in time series data. Such abrupt changes may represent transitions that occur between states. Detection of change points is useful in modelling and prediction of time series and is found in application areas such as medical condition monitoring, climate change detection, speech and image analysis, and human activity analysis. This survey article enumerates, categorizes, and compares many of the methods that have been proposed to detect change points in time series. The methods examined include both supervised and unsupervised algorithms that have been introduced and evaluated. We introduce several criteria to compare the algorithms. Finally, we present some grand challenges for the community to consider.},
author = {Aminikhanghahi, Samaneh and Cook, Diane J},
doi = {10.1007/s10115-016-0987-z},
issn = {02193116},
journal = {Knowl. Inf. Syst.},
keywords = {Change point detection,Data mining,Machine learning,Segmentation,Time series data},
month = {may},
number = {2},
pages = {339--367},
pmid = {28603327},
publisher = {NIH Public Access},
title = {{A survey of methods for time series change point detection}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28603327 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5464762},
volume = {51},
year = {2017}
}
@article{Maj2019,
abstract = {The genetic component of many common traits is associated with the gene expression and several variants act as expression quantitative loci, regulating the gene expression in a tissue- specific manner. In this work, we applied tissue-specific cis-eQTL gene expression prediction models on the genotype of 808 samples including controls, patients with mild cognitive impairment, and subjects with Alzheimer Disease. We then dissected the imputed transcriptomic profiles by means of different unsupervised and supervised machine learning approaches to identify potential biological associations (all code is available at https://github.com/imerelli/DeepNeuro). Our analysis suggests that unsupervised and supervised methods can provide complementary information, which can be integrated for a better characterization of the underlying biological system. In particular, a variational autoencoder representation of the transcriptomic profiles, followed by a support vector machine classification, has been used for tissue-specific gene prioritizations. Interestingly, the achieved gene prioritization can be efficiently integrated as a feature selection step for improving the accuracy of deep learning classifier networks. The identified gene-tissue information suggests a potential role for inflammatory and regulatory processes in gut-brain axis related tissues. In line with the expected low heritability that can be apportioned to eQTL variants, we were able to achieve only relatively low prediction capability with deep learning classification models. However, our analysis revealed that the classification power strongly depends on the network structure, with recurrent neural networks being the best performing network class. Interestingly, cross-tissue analysis suggests a potentially greater role of models trained in brain tissues also by considering dementia-related endophenotypes. Overall, the present analysis suggests that the combination of supervised and unsupervised machine learning techniques can be used for the evaluation of high dimensional omics data.},
author = {Maj, Carlo and Azevedo, Tiago and Giansanti, Valentina and Borisov, Oleg and Dimitri, Giovanna Maria and Spasov, Simeon and Lio', Pietro and Merelli, Ivan},
doi = {10.3389/FGENE.2019.00726},
issn = {1664-8021},
journal = {Front. Genet.},
keywords = {Alzheheimer's disease,GTEx,Support vector machine,Variational autoencoder,deep learning,eQTL,gene expression imputation,recurrent neural network},
pages = {726},
publisher = {Frontiers},
title = {{Integration of machine learning methods to dissect genetically imputed transcriptomic profiles in Alzheimer's Disease.}},
url = {https://www.frontiersin.org/articles/10.3389/fgene.2019.00726/abstract},
volume = {10},
year = {2019}
}
@article{Kren2017,
abstract = {Manual creation of machine learning ensembles is a hard and tedious task which requires an expert and a lot of time. In this work we describe a new version of the GP-ML algorithm which uses genetic programming to create machine learning workows (combinations of preprocessing, classification, and ensembles) automatically, using strongly typed genetic programming and asynchronous evolution. The current version improves the way in which the individuals in the genetic programming are created and allows for much larger workows. Additionally, we added new machine learning methods. The algorithm is compared to the grid search of the base methods and to its previous versions on a set of problems from the UCI machine learning repository.},
author = {Křen, Tom{\'{a}}{\v{s}} and Pil{\'{a}}t, Martin and Neruda, Roman},
doi = {10.1142/S021821301760020X},
issn = {0218-2130},
journal = {Int. J. Artif. Intell. Tools},
keywords = {Genetic programming,asynchronous evolutionary algorithm,machine learning workows},
month = {oct},
number = {05},
pages = {1760020},
publisher = {World Scientific Publishing Company},
title = {{Automatic Creation of Machine Learning Workflows with Strongly Typed Genetic Programming}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S021821301760020X},
volume = {26},
year = {2017}
}
@misc{caret_r_package,
title = {{The caret Package}},
url = {http://topepo.github.io/caret/index.html},
urldate = {2018-04-11}
}
@misc{software_xppauto,
title = {{Bifurcation Calculations with XPP AUTO}},
url = {http://www.math.pitt.edu/{~}bard/bardware/tut/xppauto.html},
urldate = {2019-12-23}
}
@article{Gitlin2016a,
abstract = {Despite its virtually universal acceptance as the gold standard in treating bipolar disorder, prescription rates for lithium have been decreasing recently. Although this observation is multifactorial, one obvious potential contributor is the side effect and toxicity burden associated with lithium. Additionally, side effect concerns assuredly play some role in lithium nonadherence. This paper summarizes the knowledge base on side effects and toxicity and suggests optimal management of these problems. Thirst and excessive urination, nausea and diarrhea and tremor are rather common side effects that are typically no more than annoying even though they are rather prevalent. A simple set of management strategies that involve the timing of the lithium dose, minimizing lithium levels within the therapeutic range and, in some situations, the prescription of side effect antidotes will minimize the side effect burden for patients. In contrast, weight gain and cognitive impairment from lithium tend to be more distressing to patients, more difficult to manage and more likely to be associated with lithium nonadherence. Lithium has adverse effects on the kidneys, thyroid gland and parathyroid glands, necessitating monitoring of these organ functions through periodic blood tests. In most cases, lithium-associated renal effects are relatively mild. A small but measurable percentage of lithium-treated patients will show progressive renal impairment. Infrequently, lithium will need to be discontinued because of the progressive renal insufficiency. Lithium-induced hypothyroidism is relatively common but easily diagnosed and treated. Hyperparathyroidism from lithium is a relatively more recently recognized phenomenon.},
author = {Gitlin, Michael},
doi = {10.1186/s40345-016-0068-y},
issn = {21947511},
journal = {Int. J. Bipolar Disord.},
keywords = {Lithium,Nephrotoxicity,Renal function,Side effects,Thyroid},
month = {dec},
number = {1},
pages = {27},
pmid = {27900734},
publisher = {Springer},
title = {{Lithium side effects and toxicity: prevalence and management strategies}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27900734 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5164879},
volume = {4},
year = {2016}
}
@book{interpretable_ml_book_git,
author = {Molnar, Christoph},
title = {{Interpretable Machine Learning}},
url = {https://christophm.github.io/interpretable-ml-book/{\%}7D},
year = {2018}
}
@article{Wetherall2019,
abstract = {BACKGROUND Depressive disorders can be debilitating, as well as a risk factor for self-harm and suicide. Social rank theory (SRT) suggests depression stems from feelings of defeat and entrapment that ensue from experiencing oneself to be of lower rank than others. This study aims to review the literature investigating the relationship between self-perceptions of social rank and depressive symptoms or suicidal ideation/behaviour. METHODS A keyword search of three psychological and medical databases was completed (Psychinfo, Medline, Web of Knowledge). Studies were quality assessed using established criteria. RESULTS An initial 1290 records were identified. After application of inclusion and exclusion criteria, 70 remained measuring depressive symptoms (n=68), self-harm (n=3) and suicidal ideation (n=3). The main measures assessing social rank were the social comparison scale (SCS; n = 32) and subjective social status (SSS, n = 32), with six additional papers including another measure of social rank. In univariate analyses, as perceptions of social rank decreased, depressive symptoms (and suicidal ideation/ self-harm) increased. Multivariate analyses indicated that social rank may act as a psychosocial mechanism to explain the relationship between social factors (in particular socio-economic status) and depressive symptoms. Additionally, psychological variables, such as rumination or self-esteem, may mediate or moderate the relationship between social rank and depressive or suicidal symptoms. LIMITATIONS Study quality was variable and 89{\%} of studies were cross-sectional. CONCLUSIONS Although more prospective research is required, this review highlights the importance of understanding an individual's perception of their social position compared to others as it may lead to an enhanced understanding of the aetiology of depressive disorders.},
author = {Wetherall, Karen and Robb, Kathryn A and O'Connor, Rory C},
doi = {10.1016/J.JAD.2018.12.045},
issn = {0165-0327},
journal = {J. Affect. Disord.},
month = {mar},
pages = {300--319},
publisher = {Elsevier},
title = {{Social rank theory of depression: A systematic review of self-perceptions of social rank and their relationship with depressive symptoms and suicide risk}},
url = {https://www.sciencedirect.com/science/article/pii/S0165032718310280 https://www.sciencedirect.com/science/article/pii/S0165032718310280?dgcid=raven{\_}sd{\_}aip{\_}email},
volume = {246},
year = {2018}
}
@inproceedings{Lucas2012,
abstract = {A successful theory of causal reasoning should be able to account for inferences about counterfactual scenarios. Pearl (2000) has developed a formal account of causal reasoning that has been highly influential but that suffers from at least two limitations as an account of counterfactual reasoning: it does not distinguish between counterfactual observations and counterfactual interventions, and it does not accommodate backtracking counterfactuals. We present an extension of Pearl's account that overcomes both limitations. Our model provides a unified treatment of counterfactual interventions and back- tracking counterfactuals, and we show that it accounts for data collected by Sloman and Lagnado (2005) and Rips (2010).},
author = {Lucas, Christopher G and Kemp, Charles},
booktitle = {34th Annu. Meet. Cogn. Sci. Soc.},
pages = {707--12},
title = {{A unified theory of counterfactual reasoning}},
year = {2012}
}
@article{Aschenbrenner2019,
abstract = {Dysregulated intestinal immune responses are the cause of inflammatory bowel diseases (IBD). Using single-cell and bulk transcriptomic approaches we investigate the responses of monocytes and peripheral blood mononuclear cells to multiple stimuli and relate those to transcriptional responses in the inflamed intestine. We identify auto- and paracrine sensing of IL-1$\alpha$/$\beta$ and IL-10 regulation as key signals that control the development of inflammatory IL-23-producing monocytes. Uptake of whole bacteria induces IL-10 resistance and favours IL-23 secretion. IL-1$\alpha$/$\beta$+CD14+ monocyte signatures are enriched in patients with ulcerating intestinal inflammation and resistance to anti-TNF therapy. In contrast, IL-23 and tumour necrosis factor expression in the absence of this inflammatory monocyte signature was associated with homeostatic lymphocyte differentiation explaining why IL-23 and TNF expression alone are poor predictors for IBD activity. Gene co-expression analysis assists the identification of IBD patient subgroups that might benefit from IL-23p19 and/or IL-1$\alpha$/IL-1$\beta$-targeting therapies.},
author = {Aschenbrenner, Dominik and Quaranta, Maria and Banerjee, Soumya and Ilott, Nicholas and Jansen, Joanneke and Steere, Boyd A. and Chen, Yin-Huai and Ho, Stephen and Cox, Karen and Investigators, Oxford IBD Cohort and Arancibia-Carcamo, Carolina V. and Coles, Mark and Gaffney, Eamonn and Travis, Simon and Denson, Lee A. and Kugathasan, Subra and Schmitz, Jochen and Powrie, Fiona and Sansom, Stephen and Uhlig, Holm H.},
doi = {10.1101/719492},
journal = {bioRxiv},
month = {jul},
pages = {719492},
publisher = {Cold Spring Harbor Laboratory},
title = {{Systems-level analysis of monocyte responses in inflammatory bowel disease identifies IL-10 and IL-1 cytokine networks that regulate IL-23}},
url = {https://www.biorxiv.org/content/10.1101/719492v1},
year = {2019}
}
@article{Moore2019a,
abstract = {Hype surrounds the promotions, aspirations, and notions of ‘artificial intelligence (AI) for social good' and its related permutations. These terms, as used in data science and particularly in public discourse, are vague. Far from being irrelevant to data scientists or practitioners of AI, the terms create the public notion of the systems built. Through a critical reflection, I explore how notions of AI for social good are vague, offer insufficient criteria for judgement, and elide the externalities and structural interdependence of AI systems. Instead, the field known as ‘AI for social good' is best understood and referred to as ‘AI for not bad.'},
author = {Moore, Jared},
doi = {10.3389/fdata.2019.00032},
issn = {2624-909X},
journal = {Front. Big Data},
month = {sep},
title = {{AI for Not Bad}},
url = {https://www.frontiersin.org/article/10.3389/fdata.2019.00032/full},
volume = {2},
year = {2019}
}
@article{Miller2019a,
abstract = {Machine learning (ML) and its parent technology trend, artificial intelligence (AI), are deriving novel insights from ever larger and more complex datasets. Efficient and accurate AI analytics require fastidious data science—the careful curating of knowledge representations in databases, decomposition of data matrices to reduce dimensionality, and preprocessing of datasets to mitigate the confounding effects of messy (i.e., missing, redundant, and outlier) data. Messier, bigger and more dynamic medical datasets create the potential for ML computing systems querying databases to draw erroneous data inferences, portending real-world human health consequences. High-dimensional medical datasets can be static or dynamic. For example, principal component analysis (PCA) used within R computing packages can speed {\&} scale disease association analytics for deriving polygenic risk scores from static gene-expression microarrays. Robust PCA of k-dimensional subspace data accelerates image acquisition and reconstruction of dynamic 4-D magnetic resonance imaging studies, enhancing tracking of organ physiology, tissue relaxation parameters, and contrast agent effects. Unlike other data-dense business and scientific sectors, medical AI users must be aware that input data quality limitations can have health implications, potentially reducing analytic model accuracy for predicting clinical disease risks and patient outcomes. As AI technologies find more health applications, physicians should contribute their health domain expertize to rules-/ML-based computer system development, inform input data provenance and recognize the importance of data preprocessing quality assurance before interpreting the clinical implications of intelligent machine outputs to patients.},
author = {Miller, D. Douglas},
doi = {10.1038/s41746-019-0138-5},
issn = {2398-6352},
journal = {npj Digit. Med.},
keywords = {Health occupations,Medical ethics},
month = {dec},
number = {1},
pages = {62},
publisher = {Nature Publishing Group},
title = {{The medical AI insurgency: what physicians must know about data to practice with intelligent machines}},
url = {http://www.nature.com/articles/s41746-019-0138-5},
volume = {2},
year = {2019}
}
@article{Malhi2011,
abstract = {Background: Lithium has long been recognised for its mood-stabilizing effects in the management of bipolar disorder (BD) but in practice its use has been limited because of real and 'imagined' concerns. This article addresses the need for lithium to be measured with respect to its clinical and functional effects. It introduces a visual scale, termed lithiumeter, which captures the optimal lithium plasma levels for the treatment of BD. Methods: Key words pertaining to lithium's administration, dosing, and side effects as well as its efficacy in acute and long-term treatment of BD were used to conduct an electronic search of the literature. Relevant articles were identified by the authors and reviewed. Results: This paper outlines the considerations necessary prior to initiating lithium therapy and provides a guide to monitoring lithium plasma levels. Current recommendations for optimal plasma lithium levels in the management of BD are then discussed with respect to indications for use in the acute phases of the illness and maintenance therapy. The risks associated with lithium treatment are also discussed. Conclusions: The lithiumeter provides a practical guide of optimal lithium levels for the clinical management of BD. {\textcopyright} 2011 John Wiley and Sons A/S.},
author = {Malhi, Gin S and Tanious, Michelle and Gershon, Samuel},
doi = {10.1111/j.1399-5618.2011.00918.x},
issn = {13985647},
journal = {Bipolar Disord.},
keywords = {Bipolar disorder,Dosing schedule,Lithium,Maintenance,Side effects,Toxicity,Treatment regimen},
month = {may},
number = {3},
pages = {219--226},
pmid = {21676125},
title = {{The lithiumeter: A measured approach}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21676125 http://doi.wiley.com/10.1111/j.1399-5618.2011.00918.x},
volume = {13},
year = {2011}
}
@article{LeeRodgers2014,
abstract = {Many data structures, particularly time series data, are naturally$\backslash$nseasonal, cyclical, or otherwise circular. Past graphical methods for$\backslash$ntime series have focused on linear plots. In this article, we move$\backslash$ngraphical analysis onto the circle. We focus on 2 particular methods,$\backslash$none old and one new. Rose diagrams are circular histograms and can be$\backslash$nproduced in several different forms using the RRose software system. In$\backslash$naddition, we propose, develop, illustrate, and provide software support$\backslash$nfor a new circular graphical method, called Wrap-Around Time Series$\backslash$nPlots (WATS Plots), which is a graphical method useful to support time$\backslash$nseries analyses in general but in particular in relation to interrupted$\backslash$ntime series designs. We illustrate the use of WATS Plots with an$\backslash$ninterrupted time series design evaluating the effect of the Oklahoma$\backslash$nCity bombing on birthrates in Oklahoma County during the 10years$\backslash$nsurrounding the bombing of the Murrah Building in Oklahoma City. We$\backslash$ncompare WATS Plots with linear time series representations and overlay$\backslash$nthem with smoothing and error bands. Each method is shown to have$\backslash$nadvantages in relation to the other; in our example, the WATS Plots more$\backslash$nclearly show the existence and effect size of the fertility$\backslash$ndifferential.},
author = {{Lee Rodgers}, Joseph and Beasley, William Howard and Schuelke, Matthew},
doi = {10.1080/00273171.2014.946589},
issn = {00273171},
journal = {Multivariate Behav. Res.},
month = {nov},
number = {6},
pages = {571--580},
pmid = {26735359},
publisher = {Psychology Press},
title = {{Graphical Data Analysis on the Circle: Wrap-Around Time Series Plots for (Interrupted) Time Series Designs}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00273171.2014.946589},
volume = {49},
year = {2014}
}
@article{Banerjee2010e,
author = {Banerjee, Soumya and Moses, Melanie},
doi = {10.1007/s11721-010-0048-2},
isbn = {1505277914},
issn = {1935-3812},
journal = {Swarm Intell.},
keywords = {artificial immune system,distributed search,immune system scaling,lymph node,multi robot control,nile virus,west},
month = {oct},
number = {4},
pages = {301--318},
title = {{Scale invariance of immune system response rates and times: perspectives on immune system architecture and implications for artificial immune systems}},
url = {http://link.springer.com/10.1007/s11721-010-0048-2},
volume = {4},
year = {2010}
}
@misc{transmart,
abstract = {http://transmartfoundation.org/},
title = {{i2b2 tranSMART Foundation}},
url = {http://transmartfoundation.org/},
urldate = {2017-11-10}
}
@article{Tan2020,
author = {Tan, Audrey and Durbin, Mark and Chung, Frank R. and Rubin, Ada L. and Cuthel, Allison M. and McQuilkin, Jordan A. and Modrek, Aram S. and Jamin, Catherine and Gavin, Nicholas and Mann, Devin and Swartz, Jordan L. and Austrian, Jonathan S. and Testa, Paul A. and Hill, Jacob D. and Grudzen, Corita R.},
doi = {10.1186/s12911-020-1021-7},
issn = {1472-6947},
journal = {BMC Med. Inform. Decis. Mak.},
month = {dec},
number = {1},
pages = {13},
title = {{Design and implementation of a clinical decision support tool for primary palliative Care for Emergency Medicine (PRIM-ER)}},
url = {https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1021-7},
volume = {20},
year = {2020}
}
@misc{software_mirador,
title = {{Mirador: a tool for visual exploration of complex datasets}},
url = {https://fathom.info/mirador/},
year = {2019}
}
@article{Lipton2015,
abstract = {Clinical medical data, especially in the intensive care unit (ICU), consist of multivariate time series of observations. For each patient visit (or episode), sensor data and lab test results are recorded in the patient's Electronic Health Record (EHR). While potentially containing a wealth of insights, the data is difficult to mine effectively, owing to varying length, irregular sampling and missing data. Recurrent Neural Networks (RNNs), particularly those using Long Short-Term Memory (LSTM) hidden units, are powerful and increasingly popular models for learning from sequence data. They effectively model varying length sequences and capture long range dependencies. We present the first study to empirically evaluate the ability of LSTMs to recognize patterns in multivariate time series of clinical measurements. Specifically, we consider multilabel classification of diagnoses, training a model to classify 128 diagnoses given 13 frequently but irregularly sampled clinical measurements. First, we establish the effectiveness of a simple LSTM network for modeling clinical data. Then we demonstrate a straightforward and effective training strategy in which we replicate targets at each sequence step. Trained only on raw time series, our models outperform several strong baselines, including a multilayer perceptron trained on hand-engineered features.},
archivePrefix = {arXiv},
arxivId = {1511.03677},
author = {Lipton, Zachary C. and Kale, David C. and Elkan, Charles and Wetzel, Randall},
doi = {10.14722/ndss.2015.23268},
eprint = {1511.03677},
isbn = {9782875870148},
issn = {16130073},
month = {nov},
title = {{Learning to Diagnose with LSTM Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1511.03677},
year = {2015}
}
@article{JonKleinberg2016,
abstract = {It's Sunday night. You're the deputy mayor of a big city. You sit down to watch a movie and ask Netflix for help. (" Will I like Birdemic? Ishtar? Zoolander 2? ") The Netflix recommendation algorithm predicts what movie you'd like by mining data on millions of previous movie-watchers using sophisticated machine learning tools. And then the next day you go to work and every one of your agencies will make hiring decisions with little idea of which candidates would be good workers; community college students will be largely left to their own devices to decide which courses are too hard or too easy for them; and your social service system will implement a reactive rather than preventive approach to homelessness because they don't believe it's possible to forecast which families will wind up on the streets.},
author = {{Jon Kleinberg} and {Jens Ludwig} and {Sendhil Mullainathan}},
journal = {Harv. Bus. Rev.},
pages = {1--10},
title = {{A Guide to Solving Social Problems with Machine Learning}},
url = {https://hbr.org/2016/12/a-guide-to-solving-social-problems-with-machine-learning},
year = {2016}
}
@article{Ghosh2020,
abstract = {Objective Ageing is accompanied by deterioration of multiple bodily functions and inflammation, which collectively contribute to frailty. We and others have shown that frailty co-varies with alterations in the gut microbiota in a manner accelerated by consumption of a restricted diversity diet. The Mediterranean diet (MedDiet) is associated with health. In the NU-AGE project, we investigated if a 1-year MedDiet intervention could alter the gut microbiota and reduce frailty. Design We profiled the gut microbiota in 612 non-frail or pre-frail subjects across five European countries (UK, France, Netherlands, Italy and Poland) before and after the administration of a 12-month long MedDiet intervention tailored to elderly subjects (NU-AGE diet). Results Adherence to the diet was associated with specific microbiome alterations. Taxa enriched by adherence to the diet were positively associated with several markers of lower frailty and improved cognitive function, and negatively associated with inflammatory markers including C-reactive protein and interleukin-17. Analysis of the inferred microbial metabolite profiles indicated that the diet-modulated microbiome change was associated with an increase in short/branch chained fatty acid production and lower production of secondary bile acids, p-cresols, ethanol and carbon dioxide. Microbiome ecosystem network analysis showed that the bacterial taxa that responded positively to the MedDiet intervention occupy keystone interaction positions, whereas frailty-associated taxa are peripheral in the networks. Conclusion Collectively, our findings support the feasibility of improving the habitual diet to modulate the gut microbiota which in turn has the potential to promote healthier ageing.},
author = {Ghosh, Tarini Shankar and Rampelli, Simone and Jeffery, Ian B and Santoro, Aurelia and Neto, Marta and Capri, Miriam and Giampieri, Enrico and Jennings, Amy and Candela, Marco and Turroni, Silvia and Zoetendal, Erwin G and Hermes, Gerben D A and Elodie, Caumon and Meunier, Nathalie and Brugere, Corinne Malpuech and Pujos-Guillot, Estelle and Berendsen, Agnes M and {De Groot}, Lisette C P G M and Feskins, Edith J M and Kaluza, Joanna and Pietruszka, Barbara and Bielak, Marta Jeruszka and Comte, Blandine and Maijo-Ferre, Monica and Nicoletti, Claudio and {De Vos}, Willem M and Fairweather-Tait, Susan and Cassidy, Aedin and Brigidi, Patrizia and Franceschi, Claudio and O'Toole, Paul W},
doi = {10.1136/gutjnl-2019-319654},
issn = {0017-5749},
journal = {Gut},
month = {feb},
pages = {gutjnl--2019--319654},
publisher = {BMJ Publishing Group},
title = {{Mediterranean diet intervention alters the gut microbiome in older people reducing frailty and improving health status: the NU-AGE 1-year dietary intervention across five European countries}},
url = {http://gut.bmj.com/lookup/doi/10.1136/gutjnl-2019-319654},
year = {2020}
}
@article{Abdollahi2016,
abstract = {Most of the existing approaches to collab- orative ltering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical mod- els, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present ecient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Net ix data set, containing over 100 mil- lion user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of mul- tiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6{\%} better than the score of Net ix's own system},
archivePrefix = {arXiv},
arxivId = {1606.07129},
author = {Salakhutdinov, Ruslan and Mnih, Andriy and Hinton, Geoffrey},
doi = {10.1145/1273496.1273596},
eprint = {1606.07129},
isbn = {9781595937933},
journal = {Proc. 24th Int. Conf. Mach. Learn. - ICML '07},
month = {jun},
pages = {791--798},
title = {{Restricted Boltzmann machines for collaborative filtering}},
url = {http://arxiv.org/abs/1606.07129 http://portal.acm.org/citation.cfm?doid=1273496.1273596},
year = {2007}
}
@article{Eichstaedt2018,
abstract = {Depression, the most prevalent mental illness, is underdiagnosed and undertreated, highlighting the need to extend the scope of current screening methods. Here, we use language from Facebook posts of consenting individuals to predict depression recorded in electronic medical records. We accessed the history of Facebook statuses posted by 683 patients visiting a large urban academic emergency department, 114 of whom had a diagnosis of depression in their medical records. Using only the language preceding their first documentation of a diagnosis of depression, we could identify depressed patients with fair accuracy [area under the curve (AUC) = 0.69], approximately matching the accuracy of screening surveys benchmarked against medical records. Restricting Facebook data to only the 6 months immediately preceding the first documented diagnosis of depression yielded a higher prediction accuracy (AUC = 0.72) for those users who had sufficient Facebook data. Significant prediction of future depression status was possible as far as 3 months before its first documentation. We found that language predictors of depression include emotional (sadness), interpersonal (loneliness, hostility), and cognitive (preoccupation with the self, rumination) processes. Unobtrusive depression assessment through social media of consenting individuals may become feasible as a scalable complement to existing screening and monitoring procedures.},
author = {Smith, Robert J and Schwartz, H Andrew and Preoţiuc-Pietro, Daniel and Eichstaedt, Johannes C and Asch, David A and Ungar, Lyle H and Merchant, Raina M and Crutchley, Patrick},
doi = {10.1073/pnas.1802331115},
isbn = {1802331115},
issn = {0027-8424},
journal = {Proc. Natl. Acad. Sci.},
keywords = {Facebook,big data,depression,screening,social media},
month = {oct},
number = {44},
pages = {11203--11208},
pmid = {30322910},
publisher = {National Academy of Sciences},
title = {{Facebook language predicts depression in medical records}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/30322910 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC6217418},
volume = {115},
year = {2018}
}
@article{Barrett2016,
abstract = {How is value created in an online community (OC) over time? We explored this question through a longitudinal field study of an OC in the healthcare arena. We found that multiple kinds of value were produced and changed over time as different participants engaged with the OC and its evolving technology in various ways. To explain our findings, we theorize OC value as performed through the ongoing sociomaterial configuring of strategies, digital platforms, and stakeholder engagement. We develop a process perspective to explain these dynamics and identify multiple different kinds of value being created by an OC over time: financial, epistemic, ethical, service, reputational, and platform. Our research points to the importance of expanding the notion of OC users to encompass a broader understanding of stakeholders. It further suggests that creating OC value increasingly requires going beyond a dyadic relationship between the OC and the firm to encompassing a more complex relationship involving a wider ecosystem of stakeholders.},
author = {Barrett, Michael and Oborn, Eivor and Orlikowski, Wanda},
doi = {10.1287/isre.2016.0648},
issn = {15265536},
journal = {Inf. Syst. Res.},
keywords = {Computer-mediated communication and collaboration,Digital,Health IT,Online communities,Value},
month = {dec},
number = {4},
pages = {704--723},
publisher = {INFORMS},
title = {{Creating value in online communities: The sociomaterial configuring of strategy, platform, and stakeholder engagement}},
url = {http://pubsonline.informs.org/doi/10.1287/isre.2016.0648},
volume = {27},
year = {2016}
}
@article{Ferrell2001,
abstract = {Xenopus oocyte maturation is an example of an all-or-none, irreversible cell fate induction process. In response to a submaximal concentration of the steroid hormone progesterone, a given oocyte may either mature or not mature, but it can exist in intermediate states only transiently. Moreover, once an oocyte has matured, it will remain arrested in the mature state even after the progesterone is removed. It has been hypothesized that the all-or-none character of oocyte maturation, and some aspects of the irreversibility of maturation, arise out of the bistability of the signal transduction system that triggers maturation. The bistability, in turn, is hypothesized to arise from the way the signal transducers are organized into a signaling circuit that includes positive feedback (which makes it so that the system cannot rest in intermediate states) and ultrasensitivity (which filters small stimuli out of the feedback loop, allowing the system to have a stable off-state). Here we review two simple graphical methods that are commonly used to analyze bistable systems, discuss the experimental evidence for bistability in oocyte maturation, and suggest that bistability may be a common means of producing all-or-none responses and a type of biochemical memory. (c) 2001 American Institute of Physics.},
author = {Ferrell, James E. and Xiong, Wen},
doi = {10.1063/1.1349894},
issn = {10541500},
journal = {Chaos},
keywords = {biochemistry,cellular biophysics,feedback,nonlinear dynamical systems,proteins},
month = {mar},
number = {1},
pages = {227--236},
publisher = {American Institute of Physics},
title = {{Bistability in cell signaling: How to make continuous processes discontinuous, and reversible processes irreversible}},
url = {http://scitation.aip.org/content/aip/journal/chaos/11/1/10.1063/1.1349894},
volume = {11},
year = {2001}
}
@article{Gortler2019,
abstract = {How to turn a collection of small building blocks into a versatile tool for solving regression problems.},
author = {G{\"{o}}rtler, Jochen and Kehlbeck, Rebecca and Deussen, Oliver},
doi = {10.23915/distill.00017},
issn = {2476-0757},
journal = {Distill},
month = {apr},
number = {4},
title = {{A Visual Exploration of Gaussian Processes}},
url = {https://distill.pub/2019/visual-exploration-gaussian-processes},
volume = {4},
year = {2019}
}
@misc{disability_employment,
title = {{Work, health and disability green paper: improving lives - GOV.UK}},
url = {https://www.gov.uk/government/consultations/work-health-and-disability-improving-lives/work-health-and-disability-green-paper-improving-lives},
urldate = {2019-10-18}
}
@article{Barr2013,
abstract = {Linear mixed-effects models (LMEMs) have become increasingly prominent in psycholinguistics and related areas. However, many researchers do not seem to appreciate how random effects structures affect the generalizability of an analysis. Here, we argue that researchers using LMEMs for confirmatory hypothesis testing should minimally adhere to the standards that have been in place for many decades. Through theoretical arguments and Monte Carlo simulation, we show that LMEMs generalize best when they include the maximal random effects structure justified by the design. The generalization performance of LMEMs including data-driven random effects structures strongly depends upon modeling criteria and sample size, yielding reasonable results on moderately-sized samples when conservative criteria are used, but with little or no power advantage over maximal models. Finally, random-intercepts-only LMEMs used on within-subjects and/or within-items data from populations where subjects and/or items vary in their sensitivity to experimental manipulations always generalize worse than separate F1 and F2 tests, and in many cases, even worse than F1 alone. Maximal LMEMs should be the 'gold standard' for confirmatory hypothesis testing in psycholinguistics and beyond. {\textcopyright} 2012 Elsevier Inc.},
author = {Barr, Dale J and Levy, Roger and Scheepers, Christoph and Tily, Harry J},
doi = {10.1016/j.jml.2012.11.001},
issn = {0749596X},
journal = {J. Mem. Lang.},
keywords = {Generalization,Linear mixed-effects models,Monte Carlo simulation,Statistics},
month = {apr},
number = {3},
pages = {255--278},
pmid = {24403724},
publisher = {NIH Public Access},
title = {{Random effects structure for confirmatory hypothesis testing: Keep it maximal}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24403724 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3881361},
volume = {68},
year = {2013}
}
@article{Proust-Lima2015,
abstract = {The R package lcmm provides a series of functions to estimate statistical models based on linear mixed model theory. It includes the estimation of mixed models and latent class mixed models for Gaussian longitudinal outcomes (hlme), curvilinear and ordinal univariate longitudinal outcomes (lcmm) and curvilinear multivariate outcomes (multlcmm), as well as joint latent class mixed models (Jointlcmm) for a (Gaussian or curvilinear) longitudinal outcome and a time-to-event that can be possibly left-truncated right-censored and defined in a competing setting. Maximum likelihood esimators are obtained using a modified Marquardt algorithm with strict convergence criteria based on the parameters and likelihood stability, and on the negativity of the second derivatives. The package also provides various post-fit functions including goodness-of-fit analyses, classification, plots, predicted trajectories, individual dynamic prediction of the event and predictive accuracy assessment. This paper constitutes a companion paper to the package by introducing each family of models, the estimation technique, some implementation details and giving examples through a dataset on cognitive aging.},
archivePrefix = {arXiv},
arxivId = {1503.00890},
author = {Proust-Lima, C{\'{e}}cile and Philipps, Viviane and Liquet, Benoit},
doi = {10.18637/jss.v078.i02},
eprint = {1503.00890},
issn = {1548-7660},
journal = {J. Stat. Softw.},
keywords = {Fortran 90,R,curvilinearity,dynamic prediction,growth mixture model,joint model,psychometric tests},
month = {jun},
number = {2},
pages = {1--56},
title = {{Estimation of extended mixed models using latent classes and latent processes: the R package lcmm}},
url = {http://www.jstatsoft.org/v78/i02/ http://arxiv.org/abs/1503.00890{\%}0Ahttp://dx.doi.org/10.18637/jss.v078.i02},
volume = {78},
year = {2015}
}
@misc{stanford_network_data,
title = {{Stanford Large Network Dataset Collection}},
url = {http://snap.stanford.edu/data/index.html},
urldate = {2019-03-24}
}
@article{Hooker2019,
abstract = {Neural network pruning techniques have demonstrated it is possible to remove the majority of weights in a network with surprisingly little degradation to test set accuracy. However, this measure of performance conceals significant differences in how different classes and images are impacted by pruning. We find that certain examples, which we term pruning identified exemplars (PIEs), and classes are systematically more impacted by the introduction of sparsity. Removing PIE images from the test-set greatly improves top-1 accuracy for both pruned and non-pruned models. These hard-to-generalize-to images tend to be mislabelled, of lower image quality, depict multiple objects or require fine-grained classification. These findings shed light on previously unknown trade-offs, and suggest that a high degree of caution should be exercised before pruning is used in sensitive domains.},
archivePrefix = {arXiv},
arxivId = {1911.05248},
author = {Hooker, Sara and Courville, Aaron and Dauphin, Yann and Frome, Andrea},
eprint = {1911.05248},
month = {nov},
title = {{Selective Brain Damage: Measuring the Disparate Impact of Model Pruning}},
url = {http://arxiv.org/abs/1911.05248},
year = {2019}
}
@article{Edlow2019,
abstract = {We present an ultra-high resolution MRI dataset of an ex vivo human brain specimen. The brain specimen was donated by a 58-year-old woman who had no history of neurological disease and died of non-neurological causes. After fixation in 10{\%} formalin, the specimen was imaged on a 7 Tesla MRI scanner at 100 µ m isotropic resolution using a custom-built 31-channel receive array coil. Single-echo multi-flip Fast Low-Angle SHot (FLASH) data were acquired over 100 hours of scan time (25 hours per flip angle), allowing derivation of a T1 parameter map and synthesized FLASH volumes. This dataset provides an unprecedented view of the three-dimensional neuroanatomy of the human brain. To optimize the utility of this resource, we warped the dataset into standard stereotactic space. We now distribute the dataset in both native space and stereotactic space to the academic community via multiple platforms. We envision that this dataset will have a broad range of investigational, educational, and clinical applications that will advance understanding of human brain anatomy in health and disease. View this table:},
author = {Edlow, Brian L. and Mareyam, Azma and Horn, Andreas and Polimeni, Jonathan R. and Dylan, M and Augustinack, Jean and Stockmann, Jason P. and Diamond, Bram R. and Tirrell, Lee S. and Folkerth, Rebecca D. and Wald, Lawrence L. and Fischl, Bruce and van der Kouwe, Andre and Hospital, Massachusetts General and General, Massachusetts and Section, Neuromodulation},
doi = {10.1101/649822},
journal = {bioRxiv},
month = {may},
pages = {649822},
publisher = {Cold Spring Harbor Laboratory},
title = {{7 Tesla MRI of the}},
url = {https://www.biorxiv.org/content/10.1101/649822v1},
year = {2019}
}
@article{Gill2017,
author = {Gill, Karamjit S.},
doi = {10.1007/s00146-017-0755-y},
issn = {14355655},
journal = {AI Soc.},
month = {nov},
number = {4},
pages = {475--482},
publisher = {Springer London},
title = {{Uncommon voices of AI}},
url = {http://link.springer.com/10.1007/s00146-017-0755-y},
volume = {32},
year = {2017}
}
@article{Roitmann2014,
abstract = {PURPOSE: New pharmacovigilance methods are needed as a consequence of the morbidity caused by drugs. We exploit fine-grained drug related adverse event information extracted by text mining from electronic medical records (EMRs) to stratify patients based on their adverse events and to determine adverse event co-occurrences.$\backslash$n$\backslash$nMETHODS: We analyzed the similarity of adverse event profiles of 2347 patients extracted from EMRs from a mental health center in Denmark. The patients were clustered based on their adverse event profiles and the similarities were presented as a network. The set of adverse events in each main patient cluster was evaluated. Co-occurrences of adverse events in patients (p-value {\textless} 0.01) were identified and presented as well.$\backslash$n$\backslash$nRESULTS: We found that each cluster of patients typically had a most distinguishing adverse event. Examination of the co-occurrences of adverse events in patients led to the identification of potentially interesting adverse event correlations that may be further investigated as well as provide further patient stratification opportunities.$\backslash$n$\backslash$nCONCLUSIONS: We have demonstrated the feasibility of a novel approach in pharmacovigilance to stratify patients based on fine-grained adverse event profiles, which also makes it possible to identify adverse event correlations. Used on larger data sets, this data-driven method has the potential to reveal unknown patterns concerning adverse event occurrences.},
author = {Roitmann, Eva and Eriksson, Robert and Brunak, S{\o}ren},
doi = {10.3389/fphys.2014.00332},
issn = {1664042X},
journal = {Front. Physiol.},
keywords = {Adverse drugs reactions,Adverse events,Data mining,Electronic medical records,Network analysis,Patient stratification},
month = {sep},
number = {SEP},
pages = {332},
publisher = {Frontiers},
title = {{Patient stratification and identification of adverse event correlations in the space of 1190 drug related adverse events}},
url = {http://journal.frontiersin.org/article/10.3389/fphys.2014.00332/abstract},
volume = {5},
year = {2014}
}
@article{Cranmer2019b,
abstract = {We introduce an approach for imposing physically motivated inductive biases on graph networks to learn interpretable representations and improved zero-shot generalization. Our experiments show that our graph network models, which implement this inductive bias, can learn message representations equivalent to the true force vector when trained on n-body gravitational and spring-like simulations. We use symbolic regression to fit explicit algebraic equations to our trained model's message function and recover the symbolic form of Newton's law of gravitation without prior knowledge. We also show that our model generalizes better at inference time to systems with more bodies than had been experienced during training. Our approach is extensible, in principle, to any unknown interaction law learned by a graph network, and offers a valuable technique for interpreting and inferring explicit causal theories about the world from implicit knowledge captured by deep learning.},
archivePrefix = {arXiv},
arxivId = {1909.05862},
author = {Cranmer, Miles D. and Xu, Rui and Battaglia, Peter and Ho, Shirley},
eprint = {1909.05862},
month = {sep},
title = {{Learning Symbolic Physics with Graph Networks}},
url = {http://arxiv.org/abs/1909.05862},
year = {2019}
}
@article{Bar-Joseph2001,
abstract = {We present the first practical algorithm for the optimal linear leaf ordering of trees that are generated by hierarchical clustering. Hierarchical clustering has been extensively used to analyze gene expression data, and we show how optimal leaf ordering can reveal biological structure that is not observed with an existing heuristic ordering method. For a tree with n leaves, there are 2n-1 linear orderings consistent with the structure of the tree. Our optimal leaf ordering algorithm runs in time O(n4), and we present further improvements that make the running time of our algorithm practical. Contact: {\{}zivbj,gifford{\}}@mit.edu; tommi@ai.mit.edu},
author = {Bar-Joseph, Ziv and Gifford, David K. and Jaakkola, Tommi S.},
doi = {10.1093/bioinformatics/17.suppl_1.S22},
isbn = {1367-4803 (Print)},
issn = {13674803},
journal = {Bioinformatics},
month = {jun},
number = {SUPPL. 1},
pages = {S22--S29},
pmid = {11472989},
publisher = {Oxford University Press},
title = {{Fast optimal leaf ordering for hierarchical clustering}},
url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/17.suppl{\_}1.S22},
volume = {17},
year = {2001}
}
@article{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
eprint = {1502.03167},
month = {feb},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@article{Graff2012,
abstract = {In this paper we present an algorithm for rapid Bayesian analysis that combines the benefits of nested sampling and artificial neural networks. The blind accelerated multimodal Bayesian inference (BAMBI) algorithm implements the MultiNest package for nested sampling as well as the training of an artificial neural network (NN) to learn the likelihood function. In the case of computationally expensive likelihoods, this allows the substitution of a much more rapid approximation in order to increase significantly the speed of the analysis. We begin by demonstrating, with a few toy examples, the ability of a NN to learn complicated likelihood surfaces. BAMBI's ability to decrease running time for Bayesian inference is then demonstrated in the context of estimating cosmological parameters from Wilkinson Microwave Anisotropy Probe and other observations. We show that valuable speed increases are achieved in addition to obtaining NNs trained on the likelihood functions for the different model and data combinations. These NNs can then be used for an even faster follow-up analysis using the same likelihood and different priors. This is a fully general algorithm that can be applied, without any pre-processing, to other problems with computationally expensive likelihood functions.},
archivePrefix = {arXiv},
arxivId = {1110.2997},
author = {Graff, Philip and Feroz, Farhan and Hobson, Michael P. and Lasenby, Anthony},
doi = {10.1111/j.1365-2966.2011.20288.x},
eprint = {1110.2997},
issn = {00358711},
journal = {Mon. Not. R. Astron. Soc.},
month = {jan},
pages = {no--no},
title = {{BAMBI: blind accelerated multimodal Bayesian inference}},
url = {http://arxiv.org/abs/1110.2997 http://dx.doi.org/10.1111/j.1365-2966.2011.20288.x https://academic.oup.com/mnras/article-lookup/doi/10.1111/j.1365-2966.2011.20288.x},
year = {2012}
}
@article{Islam2019,
abstract = {Non-communicable diseases (NCDs) pose major challenges for health systems in low-and-middle income countries (LMICs). Social media may be a low-cost, powerful tool to support NCDs prevention and management in LMICs through its ability to reach a large population. However, data on the role of social media for NCD prevention and management in LMICs is scares. This commentary paper explores the role of social media for prevention and management of NCDs and discusses how these may particularly have a role in supporting people in LMICs. We conducted a literature search using PubMed and Google Scholar to identify peer-reviewed articles using social media for NCDs in LMICs. Technology based interventions are increasingly being examined as a means to address healthcare gaps, especially in LMICs. The potential role of social media in NCD prevention and management includes patient health education and information sharing, psychological support, self-management, public health campaigns and health professional's capacity building. Nevertheless, there is little direct data on utilizing social media for NCD prevention and management in LMICs and thus a systematic review was not possible. However, social media may also have risks and challenges, such as conveying incorrect information, lack of data confidentiality, monitoring, and regulation, commercial interests, equity of access, and lack of standards. Regulatory guidelines and standards need to be developed and adhered to help avoid adverse consequences. Further research on effectiveness of social media for NCDs using robust methodologies in different population groups for short/long term impacts in LMICs is recommended.},
author = {Islam, Sheikh Mohammad Shariful and Tabassum, Reshman and Liu, Yong and Chen, Shiqun and Redfern, Julie and Kim, Sun Young and Ball, Kylie and Maddison, Ralph and Chow, Clara K},
doi = {10.1016/j.hlpt.2019.01.001},
issn = {22118845},
journal = {Heal. Policy Technol.},
keywords = {Cardiovascular disease,Facebook,Information and communication technologies,Self-management,Social media,Twitter},
month = {mar},
number = {1},
pages = {96--101},
publisher = {Elsevier},
title = {{The role of social media in preventing and managing non-communicable diseases in low-and-middle income countries: Hope or hype?}},
url = {https://www.sciencedirect.com/science/article/pii/S2211883718302326},
volume = {8},
year = {2019}
}
@article{Arijs2009,
abstract = {Infliximab is an effective treatment for ulcerative colitis with over 60{\%} of patients responding to treatment and up to 30{\%} reaching remission. The mechanism of resistance to anti-tumour necrosis factor alpha (anti-TNFalpha) is unknown. This study used colonic mucosal gene expression to provide a predictive response signature for infliximab treatment in ulcerative colitis.$\backslash$nTwo cohorts of patients who received their first treatment with infliximab for refractory ulcerative colitis were studied. Response to infliximab was defined as endoscopic and histological healing. Total RNA from pre-treatment colonic mucosal biopsies was analysed with Affymetrix Human Genome U133 Plus 2.0 Arrays. Quantitative RT-PCR was used to confirm microarray data.$\backslash$nFor predicting response to infliximab treatment, pre-treatment colonic mucosal expression profiles were compared for responders and non-responders. Comparative analysis identified 179 differentially expressed probe sets in cohort A and 361 in cohort B with an overlap of 74 probe sets, representing 53 known genes, between both analyses. Comparative analysis of both cohorts combined, yielded 212 differentially expressed probe sets. The top five differentially expressed genes in a combined analysis of both cohorts were osteoprotegerin, stanniocalcin-1, prostaglandin-endoperoxide synthase 2, interleukin 13 receptor alpha 2 and interleukin 11. All proteins encoded by these genes are involved in the adaptive immune response. These markers separated responders from non-responders with 95{\%} sensitivity and 85{\%} specificity.$\backslash$nGene array studies of ulcerative colitis mucosal biopsies identified predictive panels of genes for (non-)response to infliximab. Further study of the pathways involved should allow a better understanding of the mechanisms of resistance to infliximab therapy in ulcerative colitis. ClinicalTrials.gov number, NCT00639821.},
author = {Arijs, I and Li, K and Toedter, G and Quintens, R and {Van Lommel}, L and {Van Steen}, K and Leemans, P and {De Hertogh}, G and Lemaire, K and Ferrante, M and Schnitzler, F and Thorrez, L and Ma, K and Song, X-Y Y.R. and Marano, C and {Van Assche}, G and Vermeire, S and Geboes, K and Schuit, F and Baribaud, F and Rutgeerts, P},
doi = {10.1136/gut.2009.178665},
isbn = {1468-3288 (Electronic)$\backslash$r0017-5749 (Linking)},
issn = {00175749},
journal = {Gut},
month = {dec},
number = {12},
pages = {1612--1619},
pmid = {19700435},
publisher = {BMJ Publishing Group},
title = {{Mucosal gene signatures to predict response to infliximab in patients with ulcerative colitis}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19700435},
volume = {58},
year = {2009}
}
@misc{NICE2018,
author = {NICE},
language = {eng},
title = {{CG185 Bipolar disorder: assessment and management (2014, updated 2018)}},
url = {https://www.nice.org.uk/guidance/cg185},
year = {2019}
}
@misc{gapminder_teaching,
title = {{Open-source data and tools related to global health and socio-economics}},
url = {https://www.gapminder.org/for-teachers/},
urldate = {2019-10-07}
}
@article{Cipriani2018,
abstract = {Background: Major depressive disorder is one of the most common, burdensome, and costly psychiatric disorders worldwide in adults. Pharmacological and non-pharmacological treatments are available; however, because of inadequate resources, antidepressants are used more frequently than psychological interventions. Prescription of these agents should be informed by the best available evidence. Therefore, we aimed to update and expand our previous work to compare and rank antidepressants for the acute treatment of adults with unipolar major depressive disorder. Methods: We did a systematic review and network meta-analysis. We searched Cochrane Central Register of Controlled Trials, CINAHL, Embase, LILACS database, MEDLINE, MEDLINE In-Process, PsycINFO, the websites of regulatory agencies, and international registers for published and unpublished, double-blind, randomised controlled trials from their inception to Jan 8, 2016. We included placebo-controlled and head-to-head trials of 21 antidepressants used for the acute treatment of adults (≥18 years old and of both sexes) with major depressive disorder diagnosed according to standard operationalised criteria. We excluded quasi-randomised trials and trials that were incomplete or included 20{\%} or more of participants with bipolar disorder, psychotic depression, or treatment-resistant depression; or patients with a serious concomitant medical illness. We extracted data following a predefined hierarchy. In network meta-analysis, we used group-level data. We assessed the studies' risk of bias in accordance to the Cochrane Handbook for Systematic Reviews of Interventions, and certainty of evidence using the Grading of Recommendations Assessment, Development and Evaluation framework. Primary outcomes were efficacy (response rate) and acceptability (treatment discontinuations due to any cause). We estimated summary odds ratios (ORs) using pairwise and network meta-analysis with random effects. This study is registered with PROSPERO, number CRD42012002291. Findings: We identified 28 552 citations and of these included 522 trials comprising 116 477 participants. In terms of efficacy, all antidepressants were more effective than placebo, with ORs ranging between 2{\textperiodcentered}13 (95{\%} credible interval [CrI] 1{\textperiodcentered}89–2{\textperiodcentered}41) for amitriptyline and 1{\textperiodcentered}37 (1{\textperiodcentered}16–1{\textperiodcentered}63) for reboxetine. For acceptability, only agomelatine (OR 0{\textperiodcentered}84, 95{\%} CrI 0{\textperiodcentered}72–0{\textperiodcentered}97) and fluoxetine (0{\textperiodcentered}88, 0{\textperiodcentered}80–0{\textperiodcentered}96) were associated with fewer dropouts than placebo, whereas clomipramine was worse than placebo (1{\textperiodcentered}30, 1{\textperiodcentered}01–1{\textperiodcentered}68). When all trials were considered, differences in ORs between antidepressants ranged from 1{\textperiodcentered}15 to 1{\textperiodcentered}55 for efficacy and from 0{\textperiodcentered}64 to 0{\textperiodcentered}83 for acceptability, with wide CrIs on most of the comparative analyses. In head-to-head studies, agomelatine, amitriptyline, escitalopram, mirtazapine, paroxetine, venlafaxine, and vortioxetine were more effective than other antidepressants (range of ORs 1{\textperiodcentered}19–1{\textperiodcentered}96), whereas fluoxetine, fluvoxamine, reboxetine, and trazodone were the least efficacious drugs (0{\textperiodcentered}51–0{\textperiodcentered}84). For acceptability, agomelatine, citalopram, escitalopram, fluoxetine, sertraline, and vortioxetine were more tolerable than other antidepressants (range of ORs 0{\textperiodcentered}43–0{\textperiodcentered}77), whereas amitriptyline, clomipramine, duloxetine, fluvoxamine, reboxetine, trazodone, and venlafaxine had the highest dropout rates (1{\textperiodcentered}30–2{\textperiodcentered}32). 46 (9{\%}) of 522 trials were rated as high risk of bias, 380 (73{\%}) trials as moderate, and 96 (18{\%}) as low; and the certainty of evidence was moderate to very low. Interpretation: All antidepressants were more efficacious than placebo in adults with major depressive disorder. Smaller differences between active drugs were found when placebo-controlled trials were included in the analysis, whereas there was more variability in efficacy and acceptability in head-to-head trials. These results should serve evidence-based practice and inform patients, physicians, guideline developers, and policy makers on the relative merits of the different antidepressants. Funding: National Institute for Health Research Oxford Health Biomedical Research Centre and the Japan Society for the Promotion of Science.},
archivePrefix = {arXiv},
arxivId = {arXiv:hep-th/9912205},
author = {Cipriani, Andrea and Furukawa, Toshi A and Salanti, Georgia and Chaimani, Anna and Atkinson, Lauren Z and Ogawa, Yusuke and Leucht, Stefan and Ruhe, Henricus G and Turner, Erick H and Higgins, Julian P.T. and Egger, Matthias and Takeshima, Nozomi and Hayasaka, Yu and Imai, Hissei and Shinohara, Kiyomi and Tajika, Aran and Ioannidis, John P.A. and Geddes, John R},
doi = {10.1016/S0140-6736(17)32802-7},
eprint = {9912205},
isbn = {0140-6736},
issn = {1474547X},
journal = {Lancet},
month = {apr},
number = {10128},
pages = {1357--1366},
pmid = {29477251},
primaryClass = {arXiv:hep-th},
publisher = {Elsevier},
title = {{Comparative efficacy and acceptability of 21 antidepressant drugs for the acute treatment of adults with major depressive disorder: a systematic review and network meta-analysis}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/29477251 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5889788},
volume = {391},
year = {2018}
}
@article{Chang2010a,
abstract = {Background: Higher mortality has been found for people with serious mental illness (SMI, including schizophrenia, schizoaffective disorders, and bipolar affective disorder) at all age groups. Our aim was to characterize vulnerable groups for excess mortality among people with SMI, substance use disorders, depressive episode, and recurrent depressive disorder.Methods: A case register was developed at the South London and Maudsley National Health Services Foundation Trust (NHS SLAM), accessing full electronic clinical records on over 150,000 mental health service users as a well-defined cohort since 2006. The Case Register Interactive Search (CRIS) system enabled searching and retrieval of anonymised information since 2008. Deaths were identified by regular national tracing returns after 2006. Standardized mortality ratios (SMRs) were calculated for the period 2007 to 2009 using SLAM records for this period and the expected number of deaths from age-specific mortality statistics for the England and Wales population in 2008. Data were stratified by gender, ethnicity, and specific mental disorders.Results: A total of 31,719 cases, aged 15 years old or more, active between 2007-2009 and with mental disorders of interest prior to 2009 were detected in the SLAM case register. SMRs were 2.15 (95{\%} CI: 1.95-2.36) for all SMI with genders combined, 1.89 (1.64-2.17) for women and 2.47 (2.17-2.80) for men. In addition, highest mortality risk was found for substance use disorders (SMR = 4.17; 95{\%} CI: 3.75-4.64). Age- and gender-standardised mortality ratios by ethnic group revealed huge fluctuations, and SMRs for all disorders diminished in strength with age. The main limitation was the setting of secondary mental health care provider in SLAM.Conclusions: Substantially higher mortality persists in people with serious mental illness, substance use disorders and depressive disorders. Furthermore, mortality risk differs substantially with age, diagnosis, gender and ethnicity. Further research into specific risk groups is required. {\textcopyright} 2010 Chang et al; licensee BioMed Central Ltd.},
author = {Chang, Chin Kuo and Hayes, Richard D and Broadbent, Matthew and Fernandes, Andrea C and Lee, William and Hotopf, Matthew and Stewart, Robert},
doi = {10.1186/1471-244X-10-77},
isbn = {1471-244X (Electronic)$\backslash$r1471-244X (Linking)},
issn = {1471244X},
journal = {BMC Psychiatry},
month = {dec},
number = {1},
pages = {77},
pmid = {20920287},
title = {{All-cause mortality among people with serious mental illness (SMI), substance use disorders, and depressive disorders in southeast London: A cohort study}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20920287 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2958993 http://bmcpsychiatry.biomedcentral.com/articles/10.1186/1471-244X-10-77},
volume = {10},
year = {2010}
}
@article{Haque2017,
abstract = {RNA sequencing (RNA-seq) is a genomic approach for the detection and quantitative analysis of messenger RNA molecules in a biological sample and is useful for studying cellular responses. RNA-seq has fueled much discovery and innovation in medicine over recent years. For practical reasons, the technique is usually conducted on samples comprising thousands to millions of cells. However, this has hindered direct assessment of the fundamental unit of biology-the cell. Since the first single-cell RNA-sequencing (scRNA-seq) study was published in 2009, many more have been conducted, mostly by specialist laboratories with unique skills in wet-lab single-cell genomics, bioinformatics, and computation. However, with the increasing commercial availability of scRNA-seq platforms, and the rapid ongoing maturation of bioinformatics approaches, a point has been reached where any biomedical researcher or clinician can use scRNA-seq to make exciting discoveries. In this review, we present a practical guide to help researchers design their first scRNA-seq studies, including introductory information on experimental hardware, protocol choice, quality control, data analysis and biological interpretation.},
author = {Haque, Ashraful and Engel, Jessica and Teichmann, Sarah A. and L{\"{o}}nnberg, Tapio},
doi = {10.1186/s13073-017-0467-4},
issn = {1756994X},
journal = {Genome Med.},
month = {dec},
number = {1},
pages = {75},
pmid = {28821273},
title = {{A practical guide to single-cell RNA-sequencing for biomedical research and clinical applications}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28821273 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5561556 http://genomemedicine.biomedcentral.com/articles/10.1186/s13073-017-0467-4},
volume = {9},
year = {2017}
}
@article{Dean2019,
abstract = {Post-traumatic stress disorder (PTSD) impacts many veterans and active duty soldiers, but diagnosis can be problematic due to biases in self-disclosure of symptoms, stigma within military populations, and limitations identifying those at risk. Prior studies suggest that PTSD may be a systemic illness, affecting not just the brain, but the entire body. Therefore, disease signals likely span multiple biological domains, including genes, proteins, cells, tissues, and organism-level physiological changes. Identification of these signals could aid in diagnostics, treatment decision-making, and risk evaluation. In the search for PTSD diagnostic biomarkers, we ascertained over one million molecular, cellular, physiological, and clinical features from three cohorts of male veterans. In a discovery cohort of 83 warzone-related PTSD cases and 82 warzone-exposed controls, we identified a set of 343 candidate biomarkers. These candidate biomarkers were selected from an integrated approach using (1) data-driven methods, including Support Vector Machine with Recursive Feature Elimination and other standard or published methodologies, and (2) hypothesis-driven approaches, using previous genetic studies for polygenic risk, or other PTSD-related literature. After reassessment of {\~{}}30{\%} of these participants, we refined this set of markers from 343 to 28, based on their performance and ability to track changes in phenotype over time. The final diagnostic panel of 28 features was validated in an independent cohort (26 cases, 26 controls) with good performance (AUC = 0.80, 81{\%} accuracy, 85{\%} sensitivity, and 77{\%} specificity). The identification and validation of this diverse diagnostic panel represents a powerful and novel approach to improve accuracy and reduce bias in diagnosing combat-related PTSD.},
author = {Dean, Kelsey R. and Hammamieh, Rasha and Mellon, Synthia H. and Abu-Amara, Duna and Flory, Janine D. and Guffanti, Guia and Wang, Kai and Daigle, Bernie J. and Gautam, Aarti and Lee, Inyoul and Yang, Ruoting and Almli, Lynn M. and Bersani, F. Saverio and Chakraborty, Nabarun and Donohue, Duncan and Kerley, Kimberly and Kim, Taek-Kyun and Laska, Eugene and {Young Lee}, Min and Lindqvist, Daniel and Lori, Adriana and Lu, Liangqun and Misganaw, Burook and Muhie, Seid and Newman, Jennifer and Price, Nathan D. and Qin, Shizhen and Reus, Victor I. and Siegel, Carole and Somvanshi, Pramod R. and Thakur, Gunjan S. and Zhou, Yong and Hood, Leroy and Ressler, Kerry J. and Wolkowitz, Owen M. and Yehuda, Rachel and Jett, Marti and Doyle, Francis J. and Marmar, Charles},
doi = {10.1038/s41380-019-0496-z},
issn = {1359-4184},
journal = {Mol. Psychiatry},
keywords = {Diagnostic markers,Psychiatric disorders},
month = {sep},
pages = {1--13},
publisher = {Nature Publishing Group},
title = {{Multi-omic biomarker identification and validation for diagnosing warzone-related post-traumatic stress disorder}},
url = {http://www.nature.com/articles/s41380-019-0496-z},
year = {2019}
}
@article{Choi2019,
abstract = {Effective modeling of electronic health records (EHR) is rapidly becoming an important topic in both academia and industry. A recent study showed that utilizing the graphical structure underlying EHR data (e.g. relationship between diagnoses and treatments) improves the performance of prediction tasks such as heart failure diagnosis prediction. However, EHR data do not always contain complete structure information. Moreover, when it comes to claims data, structure information is completely unavailable to begin with. Under such circumstances, can we still do better than just treating EHR data as a flat-structured bag-of-features? In this paper, we study the possibility of utilizing the implicit structure of EHR by using the Transformer for prediction tasks on EHR data. Specifically, we argue that the Transformer is a suitable model to learn the hidden EHR structure, and propose the Graph Convolutional Transformer, which uses data statistics to guide the structure learning process. Our model empirically demonstrated superior prediction performance to previous approaches on both synthetic data and publicly available EHR data on encounter-based prediction tasks such as graph reconstruction and readmission prediction, indicating that it can serve as an effective general-purpose representation learning algorithm for EHR data.},
archivePrefix = {arXiv},
arxivId = {1906.04716},
author = {Choi, Edward and Xu, Zhen and Li, Yujia and Dusenberry, Michael W. and Flores, Gerardo and Xue, Yuan and Dai, Andrew M.},
eprint = {1906.04716},
month = {jun},
title = {{Graph Convolutional Transformer: Learning the Graphical Structure of Electronic Health Records}},
url = {http://arxiv.org/abs/1906.04716},
year = {2019}
}
@article{VanderBijl-Brouwer2017,
abstract = {Service design is one of the keys to improving how we target today's complex societal problems. The predominant view of service systems is mechanistic and linear. A service infrastructure—which includes solutions like service blueprints, scripts, and protocols—is, in some ways, designed to control the behavior of service professionals at the service interface. This view undermines the intrinsic motivation, expertise, and creativity of service professionals. This article presents a different perspective on service design. Using theories of social systems and complex responsive processes, I define service organizations as ongoing iterated patterns of relationships between people, and identify them as complex social service systems. I go on to show how the human-centeredness of design practices contributes to designing for such service systems. In particular, I show how a deep understanding of the needs and aspirations of service professionals through phenomenological themes contributes to designing for social infrastructures that support continuous improvement and adaptation of the practices executed by service professionals at the service interface.},
author = {van der Bijl-Brouwer, Mieke},
doi = {10.1016/j.sheji.2017.11.002},
issn = {24058718},
journal = {She Ji},
keywords = {Complex responsive processes,Human-centered design,Phenomenological themes,Public sector innovation,Service design,Social systems},
month = {sep},
number = {3},
pages = {183--197},
publisher = {Elsevier},
title = {{Designing for Social Infrastructures in Complex Service Systems: A Human-Centered and Social Systems Perspective on Service Design}},
url = {https://www.sciencedirect.com/science/article/pii/S2405872617300692},
volume = {3},
year = {2017}
}
@misc{egfr_website,
title = {{Estimated Glomerular Filtration Rate (eGFR) | National Kidney Foundation}},
url = {https://www.kidney.org/atoz/content/gfr},
urldate = {2019-01-18}
}
@inproceedings{Tanevski2017,
abstract = {{\textcopyright} 2017, Springer International Publishing AG. Process-based modeling is an approach to constructing explanatory models of dynamical systems from knowledge and data. The knowledge encodes information about potential processes that explain the relationships between the observed system entities. The resulting process-based models provide both an explanatory overview of the system components and closed-form equations that allow for simulating the system behavior. In this paper, we present three recent improvements of the process-based approach: (i) improving predictive performance of process-based models using ensembles, (ii) extending the scope of process-based models towards handling uncertainty and (iii) addressing the task of automated process-based design.},
author = {Tanevski, Jovan and Simidjievski, Nikola and Todorovski, Ljup{\v{c}}o and D{\v{z}}eroski, Sa{\v{s}}o},
booktitle = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
doi = {10.1007/978-3-319-71273-4_35},
isbn = {9783319712727},
issn = {16113349},
pages = {378--382},
publisher = {Springer, Cham},
title = {{Process-Based Modeling and Design of Dynamical Systems}},
url = {http://link.springer.com/10.1007/978-3-319-71273-4{\_}35},
volume = {10536 LNAI},
year = {2017}
}
@article{Hirdes2003,
abstract = {OBJECTIVES: To develop a scale predicting mortality and other adverse outcomes associated with frailty. DESIGN: Observational study based on Minimum Data Set (MDS) 2.0 and mortality data. SETTING: Ontario chronic hospitals. PARTICIPANTS: All chronic hospital patients (N = 28,495) assessed with the MDS 2.0 after mandatory implementation in July 1996 followed until May 1999. MEASUREMENTS: MDS 2.0 assessments done as part of normal practice mainly by registered nurses or multidisciplinary teams in a chronic hospital. Mortality data are available from the accompanying discharge tracking form. RESULTS: The MDS-Changes in Health, End-stage disease and Symptoms and Signs (CHESS) score is a composite measure addressing changes in health, end-stage disease, and symptoms and signs of medical problems. It is a strong predictor of mortality (P {\textless} .0001) independent of the effects of age, sex, activities of daily living impairment, cognition, and do-not-resuscitate orders. It is also strongly associated with physician activity, complex medical procedures, and pain (P {\textless} .001 for each dependent variable). CONCLUSIONS: The CHESS score provides a useful new MDS-based test to predict mortality and to measure instability in health as a clinical outcome.},
author = {Hirdes, John P and Frijters, Dinnus H and Teare, Gary F},
doi = {10.1034/j.1601-5215.2002.51017.x},
isbn = {0002-8614},
issn = {00028614},
journal = {J. Am. Geriatr. Soc.},
keywords = {Frailty,Minimum Data Set,Mortality},
month = {jan},
number = {1},
pages = {96--100},
pmid = {12534853},
title = {{The MDS-CHESS scale: A new measure to predict mortality in institutionalized older people}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12534853},
volume = {51},
year = {2003}
}
@article{Strawbridge2019,
abstract = {Esterase isozyme polymorphism was documented for digestive juice and haemolymph of the tropical multivoltine silkworm, Bombyx mori L., breed CB5 (GP) and its syngenic lines (CB5Lme-1, CB5Lm-2 and CB5Lm-5) using $\alpha$- and $\beta$-naphthylacetate separately as nonspecific substrates (Ogita, Z., Kasai, T., 1965. Genetico-biochemical analysis of specific esterases in Musca domestica. Jpn. J. Genet. 40, 173-184). Polymorphism existed in the isozyme pattern of $\alpha$-esterase with two or three bands in digestive juice and three to five bands in haemolymph. No polymorphism was observed in $\beta$-esterase isozyme pattern having four bands in digestive juice and two bands in haemolymph. During the course of esterase isozyme studies, the presence of some specific $\alpha$-esterase bands (Est-1, 4 and 5) in haemolymph and $\beta$-esterase bands (Est-1, 2 and 3) in digestive juice were observed. But both $\alpha$- and $\beta$-esterase bands Est-3 and 4 in digestive juice and Est-2 and 3 in haemolymph were found to be nonspecific. Nonspecific $\beta$-esterase band (Est-3) in haemolymph of CB5 (GP) and its syngenic lines withstood a temperature up to 80±1°C for 10 min. No thermostable band was observed in the isozyme zymogram of $\alpha$-esterase in digestive juice and haemolymph or $\beta$-esterase in digestive juice. Overall, this study discusses the presence of esterase heterogeneity in the CB5 (GP) genepool, syngenic lines development, occurrence of specific $\alpha$- and $\beta$-esterase bands in digestive juice and haemolymph and thermostable $\beta$-esterase band Est-3 in haemolymph in tropical silkworm Bombyx mori L. {\textcopyright} 2001 Elsevier Science Ltd. All rights reserved.},
author = {Strawbridge, Rebecca and Carter, Ben and Marwood, Lindsey and Bandelow, Borwin and Tsapekos, Dimosthenis and Nikolova, Viktoriya L. and Taylor, Rachael and Mantingh, Tim and de Angel, Valeria and Patrick, Fiona and Cleare, Anthony J. and Young, Allan H.},
doi = {10.1016/S0965-1748(01)00065-0},
issn = {09651748},
journal = {Br. J. Psychiatry},
language = {eng},
month = {jan},
number = {1},
pages = {1--10},
pmid = {30457075},
title = {{Augmentation therapies for treatment-resistant depression: systematic review and meta-analysis}},
url = {https://www.cambridge.org/core/product/identifier/S0007125018002337/type/journal{\_}article},
volume = {214},
year = {2018}
}
@misc{gapminder_tools,
annote = {http://www.gapminder.org/data/},
title = {{Open-source data and tools related to global health and socio-economics}},
url = {http://www.gapminder.org/data/},
urldate = {2014-08-20}
}
@article{Rogers2014,
abstract = {This paper introduces a special issue of Cognitive Science initiated on the 25th anniversary of the publication of Parallel Distributed Processing (PDP), a two-volume work that introduced the use of neural network models as vehicles for understanding cognition. The collection surveys the core commitments of the PDP framework, the key issues the framework has addressed, and the debates the framework has spawned, and presents viewpoints on the current status of these issues. The articles focus on both historical roots and contemporary developments in learning, optimality theory, perception, memory, language, conceptual knowledge, cognitive control, and consciousness. Here we consider the approach more generally, reviewing the original motivations, the resulting framework, and the central tenets of the underlying theory. We then evaluate the impact of PDP both on the field at large and within specific subdomains of cognitive science and consider the current role of PDP models within the broader landscape of contemporary theoretical frameworks in cognitive science. Looking to the future, we consider the implications for cognitive science of the recent success of machine learning systems called "deep networks"-systems that build on key ideas presented in the PDP volumes.},
author = {Rogers, Timothy T. and Mcclelland, James L.},
doi = {10.1111/cogs.12148},
isbn = {9780262291408},
issn = {03640213},
journal = {Cogn. Sci.},
keywords = {Cognition,Cognitive control,Connectionist models,Language,Learning,Memory,Neural networks,Perception},
number = {6},
pages = {1024--1077},
publisher = {MIT Press},
title = {{Parallel distributed processing at 25: Further explorations in the microstructure of cognition}},
url = {https://ieeexplore.ieee.org/document/6302931/authors},
volume = {38},
year = {2014}
}
@article{Khandaker2016,
author = {Khandaker, Golam M. and Dantzer, Robert},
doi = {10.1007/s00213-015-3975-1},
issn = {0033-3158},
journal = {Psychopharmacology (Berl).},
month = {may},
number = {9},
pages = {1559--1573},
publisher = {Springer Berlin Heidelberg},
title = {{Is there a role for immune-to-brain communication in schizophrenia?}},
url = {http://link.springer.com/10.1007/s00213-015-3975-1},
volume = {233},
year = {2016}
}
@article{Antoniou2018,
abstract = {Despite their impressive performance in many tasks, deep neural networks often struggle at relational reasoning. This has recently been remedied with the introduction of a plug-in relational module that considers relations between pairs of objects. Unfortunately, this is combinatorially expensive. In this extended abstract, we show that a DenseNet incorporating dilated convolutions excels at relational reasoning on the Sort-of-CLEVR dataset, allowing us to forgo this relational module and its associated expense.},
archivePrefix = {arXiv},
arxivId = {1811.00410},
author = {Antoniou, Antreas and S{\l}owik, Agnieszka and Crowley, Elliot J. and Storkey, Amos},
eprint = {1811.00410},
month = {nov},
title = {{Dilated DenseNets for Relational Reasoning}},
url = {http://arxiv.org/abs/1811.00410},
year = {2018}
}
@inproceedings{Bidwell2010,
abstract = {We reflect on activities to design a mobile application to enable rural people in South Africa's Eastern Cape to record and share their stories, which have implications for 'cross-cultural design,' and the wider use of stories in design. We based our initial concept for generating stories with audio and photos on cell-phones on a scenario informed by abstracting from digital storytelling projects globally and our personal experience. But insights from ethnography, and technology experiments involving storytelling, in a rural village led us to query our grounding assumptions and usability criteria. So, we implemented a method using cell-phones to localise storytelling, involve rural users and probe ways to incorporate visual and audio media. Products from this method helped us to generate design ideas for our current prototype which offers great flexibility. Thus we present a new way to depict stories digitally and a process for improving such software.},
address = {New York, New York, USA},
author = {Bidwell, Nicola J. and Reitmaier, Thomas and Marsden, Gary and Hansen, Susan},
booktitle = {Proc. 28th Int. Conf. Hum. factors Comput. Syst. - CHI '10},
doi = {10.1145/1753326.1753564},
isbn = {9781605589299},
keywords = {cross-cultural,dialogical approach to design,digital storytelling,ict4d,mobile devices,oral knowledge,rural},
pages = {1593},
publisher = {ACM Press},
title = {{Designing with mobile digital storytelling in rural Africa}},
url = {http://portal.acm.org/citation.cfm?doid=1753326.1753564},
year = {2010}
}
@misc{birmingham_,
title = {{Birmingham Freedom Project – Awareness and Empowerment Programmes}},
url = {https://birminghamfreedomproject.org/},
urldate = {2019-08-08}
}
@article{Ustun2016,
abstract = {Scoring systems are linear classification models that only require users to add, subtract and multiply a few small numbers in order to make a prediction. These models are in widespread use by the medical community, but are difficult to learn from data because they need to be accurate and sparse, have coprime integer coefficients, and satisfy multiple operational constraints. We present a new method for creating data-driven scoring systems called a Supersparse Linear Integer Model (SLIM). SLIM scoring systems are built by solving an integer program that directly encodes measures of accuracy (the 0-1 loss) and sparsity (the {\$}\backslashell{\_}0{\$}-seminorm) while restricting coefficients to coprime integers. SLIM can seamlessly incorporate a wide range of operational constraints related to accuracy and sparsity, and can produce highly tailored models without parameter tuning. We provide bounds on the testing and training accuracy of SLIM scoring systems, and present a new data reduction technique that can improve scalability by eliminating a portion of the training data beforehand. Our paper includes results from a collaboration with the Massachusetts General Hospital Sleep Laboratory, where SLIM was used to create a highly tailored scoring system for sleep apnea screening},
archivePrefix = {arXiv},
arxivId = {1502.04269},
author = {Ustun, Berk and Rudin, Cynthia},
doi = {10.1007/s10994-015-5528-6},
eprint = {1502.04269},
isbn = {9781577356288},
issn = {15730565},
journal = {Mach. Learn.},
keywords = {0–1 Loss,Discrete linear classification,Integer programming,Interpretability,Medical scoring systems,Sleep apnea,Sparsity,Supervised Classification},
month = {feb},
number = {3},
pages = {349--391},
title = {{Supersparse linear integer models for optimized medical scoring systems}},
url = {http://arxiv.org/abs/1502.04269 http://dx.doi.org/10.1007/s10994-015-5528-6},
volume = {102},
year = {2016}
}
@article{Bocchetta2015,
abstract = {Background: The effects of lithium treatment on renal function have been previously shown, albeit with discrepancies regarding their relevance. In this study, we examined glomerular filtration rate in patients treated with lithium for up to 33 years. Methods: All lithium patients registered from 1980 to 2012 at a Lithium Clinic were screened. Estimated glomerular filtration rate (eGFR) was calculated from serum creatinine concentration using the Modification of Diet in Renal Disease Study Group equation. A cross-sectional evaluation of the last available eGFR of 953 patients was carried out using multivariate regression analysis for gender, current age, and duration of lithium treatment. Survival analysis was subsequently applied to calculate the time on lithium needed to enter the eGFR ranges 45 to 59 mL/min/1.73 m 2 (G3a) or 30 to 44 mL/min/1.73 m 2 (G3b). Finally, 4-year follow-up of eGFR was examined in subgroups of patients who, after reduction to an eGFR lower than 45 mL/min/1.73 m 2 either i) continued lithium at the same therapeutic range or ii) discontinued lithium or continued at concentrations below the therapeutic range (0.5 mmol/L). Results: In the cross-sectional evaluation, eGFR was found to be lower in women (by 3.47 mL/min/1.73 m 2), in older patients (0.73 mL/min/1.73 m 2 per year of age), and in patients with longer lithium treatment (0.73 mL/min/1.73 m 2 per year). Half of the patients treated for longer than 20 years had an eGFR lower than 60 mL/min/1.73 m 2 . The median time on lithium taken to enter G3a or G3b was 25 years (95{\%} CI, 23.2–26.9) and 31 years (95{\%} CI, 26.6–35.4), respectively. Progression of renal failure throughout the 4-year follow-up after a reduction to an eGFR lower than 45 mL/min/1.73 m 2 did not differ between the subgroup who continued lithium as before and the subgroup who either discontinued lithium or continued at concentrations below the therapeutic range. Conclusions: Duration of lithium treatment is to be added to advancing age as a risk factor for reduced glomerular filtration rate. However, renal dysfunction tends to appear after decades of treatment and to progress slowly and irrespective of lithium continuation.},
author = {Bocchetta, Alberto and Ardau, Raffaella and Fanni, Tiziana and Sardu, Claudia and Piras, Doloretta and Pani, Antonello and {Del Zompo}, Maria},
doi = {10.1186/s12916-014-0249-4},
issn = {17417015},
journal = {BMC Med.},
keywords = {Chronic kidney disease,Glomerular filtration,Lithium treatment},
month = {dec},
number = {1},
pages = {12},
publisher = {BioMed Central},
title = {{Renal function during long-term lithium treatment: A cross-sectional and longitudinal study}},
url = {http://bmcmedicine.biomedcentral.com/articles/10.1186/s12916-014-0249-4},
volume = {13},
year = {2015}
}
@article{Raccuglia2016,
abstract = {Inorganic-organic hybrid materials such as organically templated metal oxides, metal-organic frameworks (MOFs) and organohalide perovskites have been studied for decades, and hydrothermal and (non-aqueous) solvothermal syntheses have produced thousands of new materials that collectively contain nearly all the metals in the periodic table. Nevertheless, the formation of these compounds is not fully understood, and development of new compounds relies primarily on exploratory syntheses. Simulation- and data-driven approaches (promoted by efforts such as the Materials Genome Initiative) provide an alternative to experimental trial-and-error. Three major strategies are: simulation-based predictions of physical properties (for example, charge mobility, photovoltaic properties, gas adsorption capacity or lithium-ion intercalation) to identify promising target candidates for synthetic efforts; determination of the structure-property relationship from large bodies of experimental data, enabled by integration with high-throughput synthesis and measurement tools; and clustering on the basis of similar crystallographic structure (for example, zeolite structure classification or gas adsorption properties). Here we demonstrate an alternative approach that uses machine-learning algorithms trained on reaction data to predict reaction outcomes for the crystallization of templated vanadium selenites. We used information on 'dark' reactions--failed or unsuccessful hydrothermal syntheses--collected from archived laboratory notebooks from our laboratory, and added physicochemical property descriptions to the raw notebook information using cheminformatics techniques. We used the resulting data to train a machine-learning model to predict reaction success. When carrying out hydrothermal synthesis experiments using previously untested, commercially available organic building blocks, our machine-learning model outperformed traditional human strategies, and successfully predicted conditions for new organically templated inorganic product formation with a success rate of 89 per cent. Inverting the machine-learning model reveals new hypotheses regarding the conditions for successful product formation.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Raccuglia, Paul and Elbert, Katherine C. and Adler, Philip D.F. and Falk, Casey and Wenny, Malia B. and Mollo, Aurelio and Zeller, Matthias and Friedler, Sorelle A. and Schrier, Joshua and Norquist, Alexander J.},
doi = {10.1038/nature17439},
eprint = {NIHMS150003},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {14764687},
journal = {Nature},
keywords = {Computer science,Solid,Theory and computation,state chemistry},
month = {may},
number = {7601},
pages = {73--76},
pmid = {27147027},
publisher = {Nature Publishing Group},
title = {{Machine-learning-assisted materials discovery using failed experiments}},
url = {http://www.nature.com/articles/nature17439},
volume = {533},
year = {2016}
}
@misc{data_clinical_open_repo,
title = {{Open or Easy Access Clinical Data Sources for Biomedical Research}},
url = {https://github.com/EpistasisLab/ClinicalDataSources},
urldate = {2019-03-01},
year = {2016}
}
@article{Holmes,
abstract = {MARSS is a package for fitting mul-tivariate autoregressive state-space models to time-series data. The MARSS package imple-ments state-space models in a maximum like-lihood framework. The core functionality of MARSS is based on likelihood maximization us-ing the Kalman filter/smoother, combined with an EM algorithm. To make comparisons with other packages available, parameter estimation is also permitted via direct search routines avail-able in 'optim'. The MARSS package allows data to contain missing values and allows a wide variety of model structures and constraints to be specified (such as fixed or shared parame-ters). In addition to model-fitting, the package provides bootstrap routines for simulating data and generating confidence intervals, and multi-ple options for calculating model selection crite-ria (such as AIC). The MARSS package (Holmes et al., 2012) is an R package for fitting linear multivariate autoregres-sive state-space (MARSS) models with Gaussian er-rors to time-series data. This class of model is ex-tremely important in the study of linear stochas-tic dynamical systems, and these models are used in many different fields, including economics, engi-neering, genetics, physics and ecology. The model class has different names in different fields; some common names are dynamic linear models (DLMs) and vector autoregressive (VAR) state-space mod-els. There are a number of existing R packages for fitting this class of models, including sspir (Deth-lefsen et al., 2009) for univariate data and dlm (Petris, 2010), dse (Gilbert, 2009), KFAS (Helske, 2011) and FKF (Luethi et al., 2012) for multivari-ate data. Additional packages are available on other platforms, such as SsfPack (Durbin and Koop-man, 2001), EViews (www.eviews.com) and Brodgar (www.brodgar.com). Except for Brodgar and sspir, these packages provide maximization of the like-lihood surface (for maximum-likelihood parameter estimation) via quasi-Newton or Nelder-Mead type algorithms. The MARSS package was developed to provide an alternative maximization algorithm, based instead on an Expectation-Maximization (EM) algorithm and to provide a standardized model-specification framework for fitting different model structures.},
author = {Holmes, Elizabeth E. and Ward, Eric J. and Wills, Kellie},
doi = {10.3389/fpsyg.2013.00624},
isbn = {2073-4859},
issn = {20734859},
journal = {R J.},
number = {June},
pages = {11--19},
title = {{MARSS: Multivariate autoregressive state-space models for analyzing time-series data}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.375.6030},
volume = {4},
year = {2012}
}
@article{Reininghaus2015,
abstract = {The excess mortality in people with psychotic disorders is a major public health concern, but little is known about the clinical and social risk factors which may predict this health inequality and help inform preventative strategies. We aimed to investigate mortality in a large epidemiologically characterized cohort of individuals with first-episode psychosis compared with the general population and to determine clinical and social risk factors for premature death. All 557 individuals with first-episode psychosis initially identified in 2 areas (Southeast London and Nottinghamshire, United Kingdom) were traced over a 10-year period in the ӔSOP-10 study. Compared with the general population, all-cause (standardized mortality ratio [SMR] 3.6, 95{\%} confidence interval [CI] 2.6–4.9), natural-cause (SMR 1.7, 95{\%} CI 1.0–2.7) and unnatural-cause (SMR 13.3, 95{\%} CI 8.7–20.4) mortality was very high. Illicit drug use was associated with an increased risk of all-cause mortality (adj. rate ratio [RR] 2.31, 95{\%} CI 1.06–5.03). Risk of natural-cause mortality increased with a longer time to first remission (adj. RR 6.61, 95{\%} CI 1.33–32.77). Family involvement at first contact strongly reduced risk of unnatural-cause mortality (adj. RR 0.09, 95{\%} CI 0.01–0.69). Our findings suggest that the mortality gap in people with psychotic disorders remains huge and may be wider for unnatural-cause mortality than previously reported. Efforts should now focus on further understanding and targeting these tractable clinical and social risk factors of excess mortality. Early intervention and dual diagnosis services may play a key role in achieving more rapid remission and carer involvement and addressing substance use problems to reduce excess mortality in psychosis.},
author = {Reininghaus, Ulrich and Dutta, Rina and Dazzan, Paola and Doody, Gillian A. and Fearon, Paul and Lappin, Julia and Heslin, Margaret and Onyejiaka, Adanna and Donoghue, Kim and Lomas, Ben and Kirkbride, James B. and Murray, Robin M. and Croudace, Tim and Morgan, Craig and Jones, Peter B.},
doi = {10.1093/schbul/sbu138},
issn = {17451701},
journal = {Schizophr. Bull.},
keywords = {Mortality,Psychosis,Risk factors,Schizophrenia},
month = {may},
number = {3},
pages = {664--673},
pmid = {25262443},
title = {{Mortality in schizophrenia and other psychoses: A 10-year follow-up of the {\AE}sOP first-episode cohort}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25262443 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4393685 https://academic.oup.com/schizophreniabulletin/article-lookup/doi/10.1093/schbul/sbu138},
volume = {41},
year = {2015}
}
@article{Kipf2016,
abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
archivePrefix = {arXiv},
arxivId = {1609.02907},
author = {Kipf, Thomas N. and Welling, Max},
doi = {10.1051/0004-6361/201527329},
eprint = {1609.02907},
isbn = {9781611970685},
issn = {0004-6361},
month = {sep},
pmid = {23459267},
title = {{Semi-Supervised Classification with Graph Convolutional Networks}},
url = {http://arxiv.org/abs/1609.02907},
year = {2016}
}
@misc{auto_sklearn,
abstract = {https://automl.github.io/auto-sklearn/stable/},
title = {{What is auto-sklearn? — AutoSklearn 0.2.0 documentation}},
url = {https://automl.github.io/auto-sklearn/stable/},
urldate = {2017-08-17}
}
@article{Choi2016,
abstract = {Learning efficient representations for concepts has been proven to be an important basis for many applications such as machine translation or document classification. Proper representations of medical concepts such as diagnosis, medication, procedure codes and visits will have broad applications in healthcare analytics. However, in Electronic Health Records (EHR) the visit sequences of patients include multiple concepts (diagnosis, procedure, and medication codes) per visit. This structure provides two types of relational information, namely sequential order of visits and co-occurrence of the codes within each visit. In this work, we propose Med2Vec, which not only learns distributed representations for both medical codes and visits from a large EHR dataset with over 3 million visits, but also allows us to interpret the learned representations confirmed positively by clinical experts. In the experiments, Med2Vec displays significant improvement in key medical applications compared to popular baselines such as Skip-gram, GloVe and stacked autoencoder, while providing clinically meaningful interpretation.},
archivePrefix = {arXiv},
arxivId = {1602.05568},
author = {Choi, Edward and Bahadori, Mohammad Taha and Searles, Elizabeth and Coffey, Catherine and Sun, Jimeng},
doi = {10.1248/cpb.58.1555},
eprint = {1602.05568},
isbn = {9781450342322},
issn = {0009-2363},
month = {feb},
pmid = {21139254},
title = {{Multi-layer Representation Learning for Medical Concepts}},
url = {http://arxiv.org/abs/1602.05568},
year = {2016}
}
@article{Kellogg2019,
abstract = {Organizational change is difficult. It's challenging to get people who are set in their ways to go about their jobs differently. So what types of interventions might actually change people's behaviors in ways that make change more palatable? To answer that question, a researcher conducted a two-year ethnographic study of the primary-care departments in two U.S. hospitals. Both had received grants to implement the same change throughout their hospitals, but one was dramatically more successful than the other. What made the difference? At the hospital that was the most successful, managers had enlisted the aid of medical assistants to help change the doctors' behaviors. They had a had a high capacity for influence because they had a lot of structural power; in other words, they were best poised to intervene in a doctor's workflow and critical tasks. The act of leveraging the structural power of low level workers to push change from the bottom up may have implications for other fields, too, including law, accounting, or consulting.},
author = {Kellogg, Katherine C.},
journal = {Harvard Buiness Rev.},
month = {feb},
pages = {1--6},
title = {{How to Orchestrate Change from the Bottom Up}},
url = {https://hbr.org/2019/02/how-to-orchestrate-change-from-the-bottom-up},
year = {2019}
}
@article{Flajnik2010,
abstract = {The adaptive immune system (AIS) in mammals, which is centred on lymphocytes bearing antigen receptors that are generated by somatic recombination, arose approximately 500 million years ago in jawed fish. This intricate defence system consists of many molecules, mechanisms and tissues that are not present in jawless vertebrates. Two macroevolutionary events are believed to have contributed to the genesis of the AIS: the emergence of the recombination-activating gene (RAG) transposon, and two rounds of whole-genome duplication. It has recently been discovered that a non-RAG-based AIS with similarities to the jawed vertebrate AIS including two lymphoid cell lineages arose in jawless fish by convergent evolution. We offer insights into the latest advances in this field and speculate on the selective pressures that led to the emergence and maintenance of the AIS. {\textcopyright} 2010 Macmillan Publishers Limited. All rights reserved.},
author = {Flajnik, Martin F. and Kasahara, Masanori},
doi = {10.1038/nrg2703},
issn = {14710056},
journal = {Nat. Rev. Genet.},
month = {jan},
number = {1},
pages = {47--59},
pmid = {19997068},
title = {{Origin and evolution of the adaptive immune system: Genetic events and selective pressures}},
volume = {11},
year = {2010}
}
@article{Gui2011,
abstract = {The widespread use of high-throughput methods of single nucleotide polymorphism (SNP) genotyping has created a number of computational and statistical challenges. The problem of identifying SNP-SNP interactions in case-control studies has been studied extensively, and a number of new techniques have been developed. Little progress has been made, however, in the analysis of SNP-SNP interactions in relation to time-to-event data, such as patient survival time or time to cancer relapse. We present an extension of the two class multifactor dimensionality reduction (MDR) algorithm that enables detection and characterization of epistatic SNP-SNP interactions in the context of survival analysis. The proposed Survival MDR (Surv-MDR) method handles survival data by modifying MDR's constructive induction algorithm to use the log-rank test. Surv-MDR replaces balanced accuracy with log-rank test statistics as the score to determine the best models. We simulated datasets with a survival outcome related to two loci in the absence of any marginal effects. We compared Surv-MDR with Cox-regression for their ability to identify the true predictive loci in these simulated data. We also used this simulation to construct the empirical distribution of Surv-MDR's testing score. We then applied Surv-MDR to genetic data from a population-based epidemiologic study to find prognostic markers of survival time following a bladder cancer diagnosis. We identified several two-loci SNP combinations that have strong associations with patients' survival outcome. Surv-MDR is capable of detecting interaction models with weak main effects. These epistatic models tend to be dropped by traditional Cox regression approaches to evaluating interactions. With improved efficiency to handle genome wide datasets, Surv-MDR will play an important role in a research strategy that embraces the complexity of the genotype-phenotype mapping relationship since epistatic interactions are an important component of the genetic basis of disease.},
author = {Gui, Jiang and Moore, Jason H. and Kelsey, Karl T. and Marsit, Carmen J. and Karagas, Margaret R. and Andrew, Angeline S.},
doi = {10.1007/s00439-010-0905-5},
isbn = {0043901009055},
issn = {03406717},
journal = {Hum. Genet.},
month = {jan},
number = {1},
pages = {101--110},
pmid = {20981448},
publisher = {Springer-Verlag},
title = {{A novel survival multifactor dimensionality reduction method for detecting gene-gene interactions with application to bladder cancer prognosis}},
url = {http://link.springer.com/10.1007/s00439-010-0905-5},
volume = {129},
year = {2011}
}
@article{WarrenLiao2005,
abstract = {Time series clustering has been shown effective in providing useful information in various domains. There seems to be an increased interest in time series clustering as part of the effort in temporal data mining research. To provide an overview, this paper surveys and summarizes previous works that investigated the clustering of time series data in various application domains. The basics of time series clustering are presented, including general-purpose clustering algorithms commonly used in time series clustering studies, the criteria for evaluating the performance of the clustering results, and the measures to determine the similarity/dissimilarity between two time series being compared, either in the forms of raw data, extracted features, or some model parameters. The past researchs are organized into three groups depending upon whether they work directly with the raw data either in the time or frequency domain, indirectly with features extracted from the raw data, or indirectly with models built from the raw data. The uniqueness and limitation of previous research are discussed and several possible topics for future research are identified. Moreover, the areas that time series clustering have been applied to are also summarized, including the sources of data used. It is hoped that this review will serve as the steppingstone for those interested in advancing this area of research. {\textcopyright} 2005 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.},
author = {{Warren Liao}, T.},
doi = {10.1016/j.patcog.2005.01.025},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognit.},
keywords = {Clustering,Data mining,Distance measure,Time series data},
month = {nov},
number = {11},
pages = {1857--1874},
pmid = {18557655},
publisher = {Pergamon},
title = {{Clustering of time series data - A survey}},
url = {https://www.sciencedirect.com/science/article/pii/S0031320305001305},
volume = {38},
year = {2005}
}
@article{Kingma2013,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P and Welling, Max},
eprint = {1312.6114},
month = {dec},
title = {{Auto-Encoding Variational Bayes}},
url = {http://arxiv.org/abs/1312.6114},
year = {2013}
}
@article{Deac2019,
abstract = {Complex or co-existing diseases are commonly treated using drug combinations, which can lead to higher risk of adverse side effects. The detection of polypharmacy side effects is usually done in Phase IV clinical trials, but there are still plenty which remain undiscovered when the drugs are put on the market. Such accidents have been affecting an increasing proportion of the population (15{\%} in the US now) and it is thus of high interest to be able to predict the potential side effects as early as possible. Systematic combinatorial screening of possible drug-drug interactions (DDI) is challenging and expensive. However, the recent significant increases in data availability from pharmaceutical research and development efforts offer a novel paradigm for recovering relevant insights for DDI prediction. Accordingly, several recent approaches focus on curating massive DDI datasets (with millions of examples) and training machine learning models on them. Here we propose a neural network architecture able to set state-of-the-art results on this task---using the type of the side-effect and the molecular structure of the drugs alone---by leveraging a co-attentional mechanism. In particular, we show the importance of integrating joint information from the drug pairs early on when learning each drug's representation.},
archivePrefix = {arXiv},
arxivId = {1905.00534},
author = {Deac, Andreea and Huang, Yu-Hsiang and Veli{\v{c}}kovi{\'{c}}, Petar and Li{\`{o}}, Pietro and Tang, Jian},
eprint = {1905.00534},
month = {may},
title = {{Drug-Drug Adverse Effect Prediction with Graph Co-Attention}},
url = {http://arxiv.org/abs/1905.00534},
year = {2019}
}
@article{Fraccaro2017,
abstract = {This paper takes a step towards temporal reasoning in a dynamically changing video, not in the pixel space that constitutes its frames, but in a latent space that describes the non-linear dynamics of the objects in its world. We introduce the Kalman variational auto-encoder, a framework for unsupervised learning of sequential data that disentangles two latent representations: an object's representation, coming from a recognition model, and a latent state describing its dynamics. As a result, the evolution of the world can be imagined and missing data imputed, both without the need to generate high dimensional frames at each time step. The model is trained end-to-end on videos of a variety of simulated physical systems, and outperforms competing methods in generative and missing data imputation tasks.},
archivePrefix = {arXiv},
arxivId = {1710.05741},
author = {Fraccaro, Marco and Kamronn, Simon and Paquet, Ulrich and Winther, Ole},
doi = {10.1007/s00167-017-4621-8},
eprint = {1710.05741},
isbn = {09422056 (ISSN)},
issn = {10495258},
month = {oct},
title = {{A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning}},
url = {http://arxiv.org/abs/1710.05741},
year = {2017}
}
@article{Murphy2017,
abstract = {We are fortunate to be living in an era of twin biomedical data surges: a burgeoning representation of human phenotypes in the medical records of our healthcare systems, and high-throughput sequencing making rapid technological advances. The difficulty representing genomic data and its annotations has almost by itself led to the recognition of a biomedical "Big Data" challenge, and the complexity of healthcare data only compounds the problem to the point that coherent representation of both systems on the same platform seems insuperably difficult. We investigated the capability for complex, integrative genomic and clinical queries to be supported in the Informatics for Integrating Biology and the Bedside (i2b2) translational software package. Three different data integration approaches were developed: The first is based on Sequence Ontology, the second is based on the tranSMART engine, and the third on CouchDB. These novel methods for representing and querying complex genomic and clinical data on the i2b2 platform are available today for advancing precision medicine.},
author = {Murphy, Shawn N. and Avillach, Paul and Bellazzi, Riccardo and Phillips, Lori and Gabetta, Matteo and Eran, Alal and McDuffie, Michael T. and Kohane, Isaac S.},
doi = {10.1371/journal.pone.0172187},
isbn = {19326203 (Electronic)},
issn = {19326203},
journal = {PLoS One},
number = {4},
pmid = {28388645},
title = {{Combining clinical and genomics queries using i2b2 - Three methods}},
volume = {12},
year = {2017}
}
@inproceedings{Dzeroski2001,
abstract = {Relational reinforcement learning is presented, a learning technique that combines reinforcement learning with relational learning or inductive logic programming. Due to the use of a more expressive representation language to represent states, actions and Q-functions, relational reinforcement learning can be potentially applied to a new range of learning tasks. One such task that we investigate is planning in the block's world, where it is assumed that the effects of the actions are unknown to the agent and the agent has to learn a policy. Within this simple domain we show that relational reinforcement learning solves some existing problems with reinforcement learning. In particular, relational reinforcement learning allows to employ structural representations, to make abstraction of specific goals pursued and to exploit the results of previous learning phases when addressing new (more complex) situations.},
author = {D{\v{z}}eroski, Sa{\v{s}}o and {De Raedt}, Luc and Blockeel, Hendrik},
booktitle = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
doi = {10.1023/A:1007694015589},
isbn = {3540647384},
issn = {16113349},
number = {1/2},
pages = {11--22},
publisher = {Kluwer Academic Publishers},
title = {{Relational reinforcement learning}},
url = {http://link.springer.com/10.1023/A:1007694015589},
volume = {1446},
year = {1998}
}
@misc{r_code_changepoint,
abstract = {https://www.r-bloggers.com/change-point-detection-in-time-series-with-r-and-tableau/},
title = {{Change Point Detection in Time Series with R and Tableau | R-bloggers}},
url = {https://www.r-bloggers.com/change-point-detection-in-time-series-with-r-and-tableau/},
urldate = {2017-09-06}
}
@article{Hothorn2006,
abstract = {Abstract Recursive binary partitioning is a popular tool for regression analysis. Two fundamental problems of exhaustive search procedures usually applied to fit such models have been known for a long time: overfitting and a selection bias towards covariates with many possible splits or missing values. While pruning procedures are able to solve the overfitting problem, the variable selection bias still seriously affects the interpretability of tree-structured regression models. For some special cases unbiased procedures have been suggested, however lacking a common theoretical foundation. We propose a unified framework for recursive partitioning which embeds tree-structured regression models into a well defined theory of conditional inference procedures. Stopping criteria based on multiple test procedures are implemented and it is shown that the predictive performance of the resulting trees is as good as the performance of established exhaustive search procedures. It turns out that the partitions and therefore the models induced by both approaches are structurally different, confirming the need for an unbiased variable selection. Moreover, it is shown that the prediction accuracy of trees with early stopping is equivalent to the prediction accuracy of pruned trees with unbiased variable selection. The methodology presented here is applicable to all kinds of regression problems, including nominal, ordinal, numeric, censored as well as multivariate response variables and arbitrary measurement scales of the covariates. Data from studies on glaucoma classification, node positive breast cancer survival and mammography experience are re-analyzed.},
author = {Hothorn, Torsten and Hornik, Kurt and Zeileis, Achim},
doi = {10.1198/106186006X133933},
issn = {10618600},
journal = {J. Comput. Graph. Stat.},
keywords = {Multiple testing,Multivariate regression trees,Ordinal regression trees,Permutation tests,Variable selection},
month = {sep},
number = {3},
pages = {651--674},
publisher = {Taylor {\&} Francis},
title = {{Unbiased recursive partitioning: A conditional inference framework}},
url = {http://www.tandfonline.com/doi/abs/10.1198/106186006X133933},
volume = {15},
year = {2006}
}
@article{Nguyen2019,
author = {Nguyen, Lan Huong and Holmes, Susan},
doi = {10.1371/journal.pcbi.1006907},
editor = {Ouellette, Francis},
issn = {1553-7358},
journal = {PLOS Comput. Biol.},
month = {jun},
number = {6},
pages = {e1006907},
publisher = {Public Library of Science},
title = {{Ten quick tips for effective dimensionality reduction}},
url = {http://dx.plos.org/10.1371/journal.pcbi.1006907},
volume = {15},
year = {2019}
}
@article{Shaban-Nejad2017,
author = {Shaban-Nejad, Arash and Lavigne, Maxime and Okhmatovskaia, Anya and Buckeridge, David L.},
doi = {10.1111/nyas.13271},
issn = {00778923},
journal = {Ann. N. Y. Acad. Sci.},
month = {jan},
number = {1},
pages = {44--53},
title = {{PopHR: a knowledge-based platform to support integration, analysis, and visualization of population health data}},
url = {http://doi.wiley.com/10.1111/nyas.13271},
volume = {1387},
year = {2017}
}
@article{Bottou2011,
abstract = {A plausible definition of "reasoning" could be "algebraically manipulating previously acquired knowledge in order to answer a new question". This definition covers first-order logical inference or probabilistic inference. It also includes much simpler manipulations commonly used to build large learning systems. For instance, we can build an optical character recognition system by first training a character segmenter, an isolated character recognizer, and a language model, using appropriate labelled training sets. Adequately concatenating these modules and fine tuning the resulting system can be viewed as an algebraic operation in a space of models. The resulting model answers a new question, that is, converting the image of a text page into a computer readable text. This observation suggests a conceptual continuity between algebraically rich inference systems, such as logical or probabilistic inference, and simple manipulations, such as the mere concatenation of trainable learning systems. Therefore, instead of trying to bridge the gap between machine learning systems and sophisticated "all-purpose" inference mechanisms, we can instead algebraically enrich the set of manipulations applicable to training systems, and build reasoning capabilities from the ground up. {\textcopyright} 2013 The Author(s).},
archivePrefix = {arXiv},
arxivId = {1102.1808},
author = {Bottou, L{\'{e}}on},
doi = {10.1007/s10994-013-5335-x},
eprint = {1102.1808},
isbn = {0885-6125},
issn = {08856125},
journal = {Mach. Learn.},
keywords = {Machine learning,Reasoning,Recursive networks},
month = {feb},
number = {2},
pages = {133--149},
title = {{From machine learning to machine reasoning: An essay}},
url = {http://arxiv.org/abs/1102.1808},
volume = {94},
year = {2014}
}
@article{Tamara2018,
author = {Tamara, M and Lio, Pietro},
title = {{Personalisable Clinical Decision Support System for Neurological Diseases}},
url = {https://ercim-news.ercim.eu/en116/special/personalisable-clinical-decision-support-system},
year = {2018}
}
@misc{ehr_deep_learning_review_website,
author = {Goku, Mohandas},
keywords = {ehr{\_}deep{\_}learning{\_}review{\_}website},
title = {{Deep Learning with Electronic Health Record (EHR) Systems}},
url = {https://goku.me/blog/EHR},
urldate = {2019-02-12}
}
@article{Ostrom2009,
abstract = {A major problem worldwide is the potential loss of fisheries, forests, and water resources. Understanding of the processes that lead to improvements in or deterioration of natural resources is limited, because scientific disciplines use different concepts and languages to describe and explain complex social-ecological systems (SESs). Without a common framework to organize findings, isolated knowledge does not cumulate. Until recently, accepted theory has assumed that resource users will never self-organize to maintain their resources and that governments must impose solutions. Research in multiple disciplines, however, has found that some government policies accelerate resource destruction, whereas some resource users have invested their time and energy to achieve sustainability. A general framework is used to identify 10 subsystem variables that affect the likelihood of self-organization in efforts to achieve a sustainable SES.},
author = {Ostrom, Elinor},
doi = {10.1126/science.1172133},
issn = {00368075},
journal = {Science (80-. ).},
month = {jul},
number = {5939},
pages = {419--422},
pmid = {19628857},
publisher = {American Association for the Advancement of Science},
title = {{A general framework for analyzing sustainability of social-ecological systems}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19628857},
volume = {325},
year = {2009}
}
@inproceedings{Winston2011,
abstract = {I ask why humans are smarter than other primates, and I hypothesize that an important part of the answer lies in what I call the Strong Story Hypothesis, which holds that story telling and understanding have a central role in human intelligence. Next, I introduce another hypothesis, the Directed Perception Hypothesis, which holds that we derive much of our commonsense, including the commonsense required in story understanding, by deploying our perceptual apparatus on real and imagined events. Then, after discussing methodology, I describe the representations and methods embodied in the Genesis system, a story-understanding system that analyzes stories ranging from pr{\'{e}}cis of Shakespeare's plots to descriptions of conflicts in cyberspace. The Genesis system works with short story summaries, provided in English, together with low-level commonsense rules and higher-level reflection patterns, likewise expressed in English. Using only a small collection of commonsense rules and reflection patterns, Genesis demonstrates several story-understanding capabilities, such as determining that both Macbeth and the 2007 Russia-Estonia Cyberwar involve revenge, even though neither the word revenge nor any of its synonyms are mentioned. Finally, I describe Rao's Visio-Spatial Reasoning System, a system that recognizes activities such as approaching, jumping, and giving, and answers commonsense questions posed by Genesis. Copyright {\textcopyright} 2011, Association for the Advancement of Artificial Intelligence. All rights reserved.},
author = {Winston, Patrick Henry},
booktitle = {AAAI Fall Symp. - Tech. Rep.},
isbn = {9781577355458},
keywords = {Article},
pages = {345--352},
publisher = {Association for the Advancement of Artificial Intelligence},
title = {{The strong story hypothesis and the directed perception hypothesis}},
url = {https://dspace.mit.edu/handle/1721.1/67693},
volume = {FS-11-01},
year = {2011}
}
@article{Botzer2010,
abstract = {The output of binary cuing systems, such as alerts or alarms, depends on the threshold setting-a parameter that is often user-adjustable. However, it is unknown if users are able to adequately adjust thresholds and what information may help them to do so. Two experiments tested threshold settings for a binary classification task based on binary cues. During the task, participants decided whether a product was intact or faulty. Experimental conditions differed in the information participants received: all participants were informed about a product's fault probability and the payoffs associated with decision outcomes; one third also received information regarding conditional probabilities for a fault when the system indicated or did not indicate the existence of one (predictive values); and another third received information about conditional probabilities for the system indicating a fault, in the instance of the existence or lack thereof, of an actual fault (diagnostic values). Threshold settings in all experimental groups were nonoptimal, with settings closest to the optimum with predictive-values information. Results corresponded with a model describing threshold settings as a function of the conditional probabilities for the different outcomes. From a practical perspective, results indicate that predictive-values information best supports decisions about threshold settings. Consequently, for users to adjust thresholds, they should receive information about predictive-values, provided that such values can be computed.},
author = {Botzer, Assaf and Meyer, Joachim and Bak, Peter and Parmet, Yisrael},
doi = {10.1037/a0018758},
issn = {1076898X},
journal = {J. Exp. Psychol. Appl.},
keywords = {alerts,binary categorization,threshold setting,user adjustment},
number = {1},
pages = {1--15},
title = {{User Settings of Cue Thresholds for Binary Categorization Decisions}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0018758},
volume = {16},
year = {2010}
}
@article{Real2018,
abstract = {The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier---AmoebaNet-A---that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-the-art 83.9{\%} top-1 / 96.6{\%} top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.},
archivePrefix = {arXiv},
arxivId = {1802.01548},
author = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V},
doi = {arXiv:1802.01548v5},
eprint = {1802.01548},
isbn = {1802.01548v5},
month = {feb},
title = {{Regularized Evolution for Image Classifier Architecture Search}},
url = {http://arxiv.org/abs/1802.01548},
year = {2018}
}
@article{Pineau2018,
abstract = {This paper describes InfoCatVAE, an extension of the variational autoencoder that enables unsupervised disentangled representation learning. InfoCatVAE uses multimodal distributions for the prior and the inference network and then maximizes the evidence lower bound objective (ELBO). We connect the new ELBO derived for our model with a natural soft clustering objective which explains the robustness of our approach. We then adapt the InfoGANs method to our setting in order to maximize the mutual information between the categorical code and the generated inputs and obtain an improved model.},
archivePrefix = {arXiv},
arxivId = {1806.08240},
author = {Pineau, Edouard and Lelarge, Marc},
eprint = {1806.08240},
month = {jun},
title = {{InfoCatVAE: Representation Learning with Categorical Variational Autoencoders}},
url = {http://arxiv.org/abs/1806.08240},
year = {2018}
}
@article{Clementz2016,
abstract = {Objective:Clinical phenomenology remains the primary means for classifying psychoses despite considerable evidence that this method incompletely captures biologically meaningful differentiations. Rather than relying on clinical diagnoses as the gold standard, this project drew on neurobiological heterogeneity among psychosis cases to delineate subgroups independent of their phenomenological manifestations.Method:A large biomarker panel (neuropsychological, stop signal, saccadic control, and auditory stimulation paradigms) characterizing diverse aspects of brain function was collected on individuals with schizophrenia, schizoaffective disorder, and bipolar disorder with psychosis (N=711), their first-degree relatives (N=883), and demographically comparable healthy subjects (N=278). Biomarker variance across paradigms was exploited to create nine integrated variables that were used to capture neurobiological variance among the psychosis cases. Data on external validating measures (social functioning, struct...},
author = {Clementz, Brett A. and Sweeney, John A. and Hamm, Jordan P. and Ivleva, Elena I. and Ethridge, Lauren E. and Pearlson, Godfrey D. and Keshavan, Matcheri S. and Tamminga, Carol A.},
doi = {10.1176/appi.ajp.2015.14091200},
issn = {15357228},
journal = {Am. J. Psychiatry},
month = {apr},
number = {4},
pages = {373--384},
pmid = {26651391},
title = {{Identification of distinct psychosis biotypes using brain-based biomarkers}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26651391 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5314432 http://ajp.psychiatryonline.org/doi/10.1176/appi.ajp.2015.14091200},
volume = {173},
year = {2016}
}
@article{Muller2019,
abstract = {Neurodegenerative diseases such as Alzheimer's and Parkinson's impact millions of people worldwide. Early diagnosis has proven to greatly increase the chances of slowing down the diseases' progression. Correct diagnosis often relies on the analysis of large amounts of patient data, and thus lends itself well to support from machine learning algorithms, which are able to learn from past diagnosis and see clearly through the complex interactions of a patient's symptoms. Unfortunately, many contemporary machine learning techniques fail to reveal details about how they reach their conclusions, a property considered fundamental when providing a diagnosis. This is one reason why we introduce our Personalisable Clinical Decision Support System PECLIDES that provides a clear insight into the decision making process on top of the diagnosis. Our algorithm enriches the fundamental work of Masheyekhi and Gras in data integration, personal medicine, usability, visualisation and interactivity. Our decision support system is an operation of translational medicine. It is based on random forests, is personalisable and allows a clear insight into the decision making process. A well-structured rule set is created and every rule of the decision making process can be observed by the user (physician). Furthermore, the user has an impact on the creation of the final rule set and the algorithm allows the comparison of different diseases as well as regional differences in the same disease[1][1]. [1]: {\#}fn-3},
author = {Tamara, M and Lio, Pietro},
doi = {10.1101/708818},
journal = {bioRxiv},
month = {jul},
pages = {708818},
publisher = {Cold Spring Harbor Laboratory},
title = {{Personalisable Clinical Decision Support System for Neurological Diseases}},
url = {https://www.biorxiv.org/content/10.1101/708818v1 https://ercim-news.ercim.eu/en116/special/personalisable-clinical-decision-support-system},
year = {2018}
}
@article{Bansal2017,
abstract = {Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty. This work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX},
archivePrefix = {arXiv},
arxivId = {1710.03748},
author = {Bansal, Trapit and Pachocki, Jakub and Sidor, Szymon and Sutskever, Ilya and Mordatch, Igor},
eprint = {1710.03748},
month = {oct},
title = {{Emergent Complexity via Multi-Agent Competition}},
url = {http://arxiv.org/abs/1710.03748},
year = {2017}
}
@article{Bansal2019,
abstract = {We present an environment, benchmark, and deep learning driven automated theorem prover for higher-order logic. Higher-order interactive theorem provers enable the formalization of arbitrary mathematical theories and thereby present an interesting, open-ended challenge for deep learning. We provide an open-source framework based on the HOL Light theorem prover that can be used as a reinforcement learning environment. HOL Light comes with a broad coverage of basic mathematical theorems on calculus and the formal proof of the Kepler conjecture, from which we derive a challenging benchmark for automated reasoning. We also present a deep reinforcement learning driven automated theorem prover, DeepHOL, with strong initial results on this benchmark.},
archivePrefix = {arXiv},
arxivId = {1904.03241},
author = {Bansal, Kshitij and Loos, Sarah M. and Rabe, Markus N. and Szegedy, Christian and Wilcox, Stewart},
eprint = {1904.03241},
month = {apr},
title = {{HOList: An Environment for Machine Learning of Higher-Order Theorem Proving (extended version)}},
url = {http://arxiv.org/abs/1904.03241},
year = {2019}
}
@misc{software_star_aligner,
title = {{10XGenomics/STAR: RNA-seq aligner}},
url = {https://github.com/10XGenomics/STAR},
urldate = {2019-12-24}
}
@article{Gabella2019,
abstract = {Understanding how neural networks learn remains one of the central challenges in machine learning research. From random at the start of training, the weights of a neural network evolve in such a way as to be able to perform a variety of tasks, like classifying images. Here we study the emergence of structure in the weights by applying methods from topological data analysis. We train simple feedforward neural networks on the MNIST dataset and monitor the evolution of the weights. When initialized to zero, the weights follow trajectories that branch off recurrently, thus generating trees that describe the growth of the effective capacity of each layer. When initialized to tiny random values, the weights evolve smoothly along two-dimensional surfaces. We show that natural coordinates on these learning surfaces correspond to important factors of variation.},
archivePrefix = {arXiv},
arxivId = {1902.08160},
author = {Gabella, Maxime and Afambo, Nitya},
eprint = {1902.08160},
month = {feb},
title = {{Topology of Learning in Artificial Neural Networks}},
url = {http://arxiv.org/abs/1902.08160},
year = {2019}
}
@article{Davis2015,
abstract = {Who is taller, Prince William or his baby son Prince George? Can you make a salad out of a polyester shirt? If you stick a pin into a carrot, does it make a hole in the carrot or in the pin? These types of questions may seem silly, but many intelligent tasks, such as understanding texts, computer vision, planning, and scientific reasoning require the same kinds of real-world knowledge and reasoning abilities. For instance, if you see a six-foot-tall person holding a two-foot-tall person in his arms, and you are told they are father and son, you do not have to ask which is which. If you need to make a salad for dinner and are out of lettuce, you do not waste time considering improvising by taking a shirt of the closet and cutting it up. If you read the text, "I stuck a pin in a carrot; when I pulled the pin out, it had a hole," you need not consider the possibility "it" refers to the pin.},
author = {Davis, Ernest and Marcus, Gary},
doi = {10.1145/2701413},
issn = {15577317},
journal = {Commun. ACM},
month = {aug},
number = {9},
pages = {92--103},
publisher = {ACM},
title = {{Commonsense reasoning and commonsense knowledge in artificial intelligence}},
url = {http://dl.acm.org/citation.cfm?doid=2817191.2701413},
volume = {58},
year = {2015}
}
@article{Domingos2012,
abstract = {MACHINE LEARNING SYSTEMS automatically learn programs from data. This is often a very attractive alternative to manually constructing them, and in the last decade the use of machine learning has spread rapidly throughout computer science and beyond. Machine learning is used in Web search, spam filters, recommender systems, ad placement, credit scoring, fraud detection, stock trading, drug design, and many other applications. A recent report from the McKinsey Global Institute asserts that machine learning (a.k.a. data mining or predictive analytics) will be the driver of the next big wave of innovation. Several fine textbooks are available to interested practitioners and researchers (for example, Mitchell and Witten et al.). However, much of the “folk knowledge” that is needed to successfully develop machine learning applications is not readily available in them. As a result, many machine learning projects take much longer than necessary or wind up producing less-than-ideal results. Yet much of this folk knowledge is fairly easy to communicate. This is the purpose of this article.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Domingos, Pedro},
doi = {10.1145/2347736.2347755},
eprint = {9605103},
isbn = {0001-0782},
issn = {00010782},
journal = {Commun. ACM},
month = {oct},
number = {10},
pages = {78},
pmid = {1000183096},
primaryClass = {cs},
publisher = {ACM},
title = {{A few useful things to know about machine learning}},
url = {http://dl.acm.org/citation.cfm?doid=2347736.2347755},
volume = {55},
year = {2012}
}
@misc{code_graoh_deep_learning,
title = {keras-gcn/train.py at master {\textperiodcentered} tkipf/keras-gcn},
url = {https://github.com/tkipf/keras-gcn/blob/master/kegra/train.py},
urldate = {2019-03-05}
}
@article{Shaban-Nejad2018,
abstract = {Advances in computational and data sciences for data management, integration, mining, classification, filtering, visualization along with engineering innovations in medical devices have prompted demands for more comprehensive and coherent strategies to address the most fundamental questions in health care and medicine. Theory, methods, and models from artificial intelligence (AI) are changing the health care landscape in clinical and community settings and have already shown promising results in multiple applications in healthcare including, integrated health information systems, patient education, geocoding health data, social media analytics, epidemic and syndromic surveillance, predictive modeling and decision support, mobile health, and medical imaging (e.g. radiology and retinal image analyses). Health intelligence uses tools and methods from artificial intelligence and data science to provide better insights, reduce waste and wait time, and increase speed, service efficiencies, level of accuracy, and productivity in health care and medicine.},
author = {Shaban-Nejad, Arash and Michalowski, Martin and Buckeridge, David L.},
doi = {10.1038/s41746-018-0058-9},
issn = {2398-6352},
journal = {npj Digit. Med.},
keywords = {Population screening,Research management},
month = {dec},
number = {1},
pages = {53},
publisher = {Nature Publishing Group},
title = {{Health intelligence: how artificial intelligence transforms population and personalized health}},
url = {http://www.nature.com/articles/s41746-018-0058-9},
volume = {1},
year = {2018}
}
@inproceedings{Licklider1962,
abstract = {On-line man-computer communication requires much development before men and computers can work together effectively in formulative thinking and intuitive problem solving. This paper examines some of the directions in which advances can be made and describes on-going programs that seek to improve man-machine interaction in teaching and learning, in planning and design, and in visualizing the internal processes of computers. The paper concludes with a brief discussion of basic problems involved in improving man-computer communication.},
address = {New York, New York, USA},
author = {Licklider, J. C. R. and Clark, Welden E.},
booktitle = {Proc. May 1-3, 1962, spring Jt. Comput. Conf. - AIEE-IRE '62},
doi = {10.1145/1460833.1460847},
pages = {113},
publisher = {ACM Press},
title = {{On-line man-computer communication}},
url = {http://portal.acm.org/citation.cfm?doid=1460833.1460847},
year = {2008}
}
@article{Miotto2016,
abstract = {Secondary use of electronic health records (EHRs) promises to advance clinical research and better inform clinical decision making. Challenges in summarizing and representing patient data prevent widespread practice of predictive modeling using EHRs. Here we present a novel unsupervised deep feature learning method to derive a general-purpose patient representation from EHR data that facilitates clinical predictive modeling. In particular, a three-layer stack of denoising autoencoders was used to capture hierarchical regularities and dependencies in the aggregated EHRs of about 700,000 patients from the Mount Sinai data warehouse. The result is a representation we name "deep patient". We evaluated this representation as broadly predictive of health states by assessing the probability of patients to develop various diseases. We performed evaluation using 76,214 test patients comprising 78 diseases from diverse clinical domains and temporal windows. Our results significantly outperformed those achieved using representations based on raw EHR data and alternative feature learning strategies. Prediction performance for severe diabetes, schizophrenia, and various cancers were among the top performing. These findings indicate that deep learning applied to EHRs can derive patient representations that offer improved clinical predictions, and could provide a machine learning framework for augmenting clinical decision systems.},
author = {Miotto, Riccardo and Li, Li and Kidd, Brian A. and Dudley, Joel T.},
doi = {10.1038/srep26094},
issn = {20452322},
journal = {Sci. Rep.},
keywords = {Outcomes research,Translational research},
month = {sep},
number = {1},
pages = {26094},
publisher = {Nature Publishing Group},
title = {{Deep Patient: An Unsupervised Representation to Predict the Future of Patients from the Electronic Health Records}},
url = {http://www.nature.com/articles/srep26094},
volume = {6},
year = {2016}
}
@article{Fasy2014,
abstract = {We present a short tutorial and introduction to using the R package TDA, which provides some tools for Topological Data Analysis. In particular, it includes implementations of functions that, given some data, provide topological information about the underlying space, such as the distance function, the distance to a measure, the kNN density estimator, the kernel density estimator, and the kernel distance. The salient topological features of the sublevel sets (or superlevel sets) of these functions can be quantified with persistent homology. We provide an R interface for the efficient algorithms of the C++ libraries GUDHI, Dionysus and PHAT, including a function for the persistent homology of the Rips filtration, and one for the persistent homology of sublevel sets (or superlevel sets) of arbitrary functions evaluated over a grid of points. The significance of the features in the resulting persistence diagrams can be analyzed with functions that implement recently developed statistical methods. The R package TDA also includes the implementation of an algorithm for density clustering, which allows us to identify the spatial organization of the probability mass associated to a density function and visualize it by means of a dendrogram, the cluster tree.},
archivePrefix = {arXiv},
arxivId = {1411.1830},
author = {Fasy, Brittany Terese and Kim, Jisu and Lecci, Fabrizio and Maria, Cl{\'{e}}ment},
eprint = {1411.1830},
month = {nov},
title = {{Introduction to the R package TDA}},
url = {http://arxiv.org/abs/1411.1830},
year = {2014}
}
@book{Nielsen2015,
author = {Nielsen, Michael A.},
publisher = {Determination Press},
title = {{Neural Networks and Deep Learning}},
url = {http://neuralnetworksanddeeplearning.com/chap1.html},
year = {2015}
}
@article{Alissa2019,
abstract = {We propose a novel technique for algorithm-selection which adopts a deep-learning approach, specifically a Recurrent-Neural Network with Long-Short-Term-Memory (RNN-LSTM). In contrast to the majority of work in algorithm-selection, the approach does not need any features to be extracted from the data but instead relies on the temporal data sequence as input. A large case-study in the domain of 1-d bin packing is undertaken in which instances can be solved by one of four heuristics. We first evolve a large set of new problem instances that each have a clear "best solver" in terms of the heuristics considered. An RNN-LSTM is trained directly using the sequence data describing each instance to predict the best-performing heuristic. Experiments conducted on small and large problem instances with item sizes generated from two different probability distributions are shown to achieve between 7{\%} to 11{\%} improvement over the single best solver (SBS) (i.e. the single heuristic that achieves the best performance over the instance set) and 0{\%} to 2{\%} lower than the virtual best solver (VBS), i.e the perfect mapping.},
address = {New York, New York, USA},
author = {Alissa, Mohamad and Sim, Kevin and Hart, Emma},
doi = {10.1145/3321707},
isbn = {9781450361118},
journal = {Proc. Genet. Evol. Comput. Conf. - GECCO '19},
keywords = {Algorithm Selection,CCS CONCEPTS • Theory of computation → Packing and,KEYWORDS Deep Learning,Recurrent Neural Network,• Computing methodologies → Machine learning},
pages = {198--206},
publisher = {ACM Press},
title = {{Algorithm Selection Using Deep Learning Without Feature Extraction}},
url = {http://dl.acm.org/citation.cfm?doid=3321707.3321845 https://doi.org/10.1145/3321707.},
year = {2019}
}
@article{Samek2020,
author = {Samek, Wojciech},
doi = {10.1038/s42256-019-0142-0},
issn = {2522-5839},
journal = {Nat. Mach. Intell.},
number = {1},
pages = {16--17},
title = {{Learning with explainable trees}},
url = {http://www.nature.com/articles/s42256-019-0142-0},
volume = {2},
year = {2020}
}
@incollection{Janzing2014,
abstract = {Information-Geometric Causal Inference (IGCI) is a new approach to distinguish between cause and effect for two variables. It is based on an independence assumption between input distribution and causal mechanism that can be phrased in terms of orthogonality in information space. We describe two intuitive reinterpretations of this approach that make IGCI more accessible to a broader audience. Moreover, we show that the described independence is related to the hypothesis that unsupervised learning and semi-supervised learning only work for predicting the cause from the effect and not vice versa.},
archivePrefix = {arXiv},
arxivId = {1402.2499},
author = {Janzing, Dominik and Steudel, Bastian and Shajarisales, Naji and Sch{\"{o}}lkopf, Bernhard},
booktitle = {Meas. Complex. Festschrift Alexey Chervonenkis},
doi = {10.1007/978-3-319-21852-6_18},
eprint = {1402.2499},
isbn = {9783319218526},
month = {feb},
pages = {253--265},
title = {{Justifying information-geometric causal inference}},
url = {http://arxiv.org/abs/1402.2499},
year = {2015}
}
@article{Feroz2009,
abstract = {We present further development and the first public release of our multimodal nested sampling algorithm, called MultiNest. This Bayesian inference tool calculates the evidence, with an associated error estimate, and produces posterior samples from distributions that may contain multiple modes and pronounced (curving) degeneracies in high dimensions. The developments presented here lead to further substantial improvements in sampling efficiency and robustness, as compared to the original algorithm presented in Feroz {\&} Hobson (2008), which itself significantly outperformed existing MCMC techniques in a wide range of astrophysical inference problems. The accuracy and economy of the MultiNest algorithm is demonstrated by application to two toy problems and to a cosmological inference problem focussing on the extension of the vanilla {\$}\backslashLambda{\$}CDM model to include spatial curvature and a varying equation of state for dark energy. The MultiNest software, which is fully parallelized using MPI and includes an interface to CosmoMC, is available at http://www.mrao.cam.ac.uk/software/multinest/. It will also be released as part of the SuperBayeS package, for the analysis of supersymmetric theories of particle physics, at http://www.superbayes.org},
archivePrefix = {arXiv},
arxivId = {0809.3437},
author = {Feroz, F. and Hobson, M. P. and Bridges, M.},
doi = {10.1111/j.1365-2966.2009.14548.x},
eprint = {0809.3437},
isbn = {0035-8711},
issn = {00358711},
journal = {Mon. Not. R. Astron. Soc.},
keywords = {Methods: Data analysis,Methods: Statistical},
month = {oct},
number = {4},
pages = {1601--1614},
title = {{MultiNest: An efficient and robust Bayesian inference tool for cosmology and particle physics}},
url = {http://arxiv.org/abs/0809.3437 http://dx.doi.org/10.1111/j.1365-2966.2009.14548.x https://academic.oup.com/mnras/article-lookup/doi/10.1111/j.1365-2966.2009.14548.x},
volume = {398},
year = {2009}
}
@article{Clark2007,
abstract = {To allow more accurate prediction of hospital length of stay (LOS) after serious injury or illness, a multi-state model is proposed, in which transitions from the hospitalized state to three possible outcome states (home, long-term care, or death) are assumed to follow constant rates for each of a limited number of time periods. This results in a piecewise exponential (PWE) model for each outcome. Transition rates may be affected by time-varying covariates, which can be estimated from a reference database using standard statistical software and Poisson regression. A PWE model combining the three outcomes allows prediction of LOS. Records of 259,941 injured patients from the US Nationwide Inpatient Sample were used to create such a multi-state PWE model with four time periods. Hospital mortality and LOS for patient subgroups were calculated from this model, and time-varying covariate effects were estimated. Early mortality was increased by anatomic injury severity or penetrating mechanism, but these effects diminished with time; age and male sex remained strong predictors of mortality in all time periods. Rates of discharge home decreased steadily with time, while rates of transfer to long-term care peaked at five days. Predicted and observed LOS and mortality were similar for multiple subgroups. Conceptual background and methods of calculation are discussed and demonstrated. Multi-state PWE models may be useful to describe hospital outcomes, especially when many patients are not discharged home},
author = {Clark, David E. and Ryan, Louise M. and Lucas, F. L.},
doi = {10.1080/02664760701592836},
isbn = {02664763},
issn = {02664763},
journal = {J. Appl. Stat.},
keywords = {Competing risks,Injury,LOS,Model,Multi-state,Piecewise exponential},
month = {dec},
number = {10},
pages = {1225--1239},
publisher = {Routledge},
title = {{A multi-state piecewise exponential model of Hospital outcomes after injury}},
url = {http://www.tandfonline.com/doi/abs/10.1080/02664760701592836},
volume = {34},
year = {2007}
}
@article{Banerjee2019b,
author = {Banerjee, Soumya},
doi = {10.7906/INDECS.17.3.16},
issn = {1334-4684},
journal = {INDECS},
keywords = {conflicts,epidemics,simulation models},
month = {sep},
number = {3-B},
pages = {598--614},
publisher = {Croatian Interdisciplinary Society},
title = {{Towards a Quantitative Model of Epidemics during Conflicts}},
url = {https://hrcak.srce.hr/index.php?show=clanak{\&}id{\_}clanak{\_}jezik=329102},
volume = {17},
year = {2019}
}
@article{Kleer1975,
author = {Kleer, Johan De},
month = {dec},
title = {{QUALITATIVE AND QUANTITATIVE KNOWLEDGE IN CLASSICAL MECHANICS A Master ' s Thesis Proposal And Progress Report}},
url = {https://dspace.mit.edu/handle/1721.1/6912},
year = {1975}
}
@article{Hall2009,
abstract = {Abstract More than twelve years have elapsed since the first public release of WEKA . In that time, the software has been rewritten entirely from scratch, evolved substantially and now accompanies a text on data mining [35]. These days, WEKA enjoys widespread ... $\backslash$n},
author = {Hall, Mark and Frank, Eibe and Holmes, Geoffrey and Pfahringer, Bernhard and Reutemann, Peter and Witten, Ian H.},
doi = {10.1145/1656274.1656278},
issn = {19310145},
journal = {ACM SIGKDD Explor. Newsl.},
month = {nov},
number = {1},
pages = {10},
publisher = {ACM},
title = {{The WEKA data mining software}},
url = {http://portal.acm.org/citation.cfm?doid=1656274.1656278},
volume = {11},
year = {2009}
}
@article{Cardinal2017,
abstract = {BACKGROUND Electronic medical records contain information of value for research, but contain identifiable and often highly sensitive confidential information. Patient-identifiable information cannot in general be shared outside clinical care teams without explicit consent, but anonymisation/de-identification allows research uses of clinical data without explicit consent. RESULTS This article presents CRATE (Clinical Records Anonymisation and Text Extraction), an open-source software system with separable functions: (1) it anonymises or de-identifies arbitrary relational databases, with sensitivity and precision similar to previous comparable systems; (2) it uses public secure cryptographic methods to map patient identifiers to research identifiers (pseudonyms); (3) it connects relational databases to external tools for natural language processing; (4) it provides a web front end for research and administrative functions; and (5) it supports a specific model through which patients may consent to be contacted about research. CONCLUSIONS Creation and management of a research database from sensitive clinical records with secure pseudonym generation, full-text indexing, and a consent-to-contact process is possible and practical using entirely free and open-source software.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Cardinal, Rudolf N.},
doi = {10.1186/s12911-017-0437-1},
eprint = {9809069v1},
isbn = {9789871726172},
issn = {14726947},
journal = {BMC Med. Inform. Decis. Mak.},
keywords = {Anonymisation,Clinical informatics,De-identification,Electronic medical records,Open-source software,Pseudonymisation,Psychiatry},
month = {dec},
number = {1},
pages = {50},
pmid = {28441940},
primaryClass = {arXiv:gr-qc},
publisher = {BioMed Central},
title = {{Clinical records anonymisation and text extraction (CRATE): An open-source software system}},
url = {http://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-017-0437-1},
volume = {17},
year = {2017}
}
@misc{Alaa,
abstract = {Understanding the predictions of a machine learning model can be as crucial as the model's accuracy in many application domains. However, the black-box nature of most highly-accurate (complex) models is a major hindrance to their interpretability. To address this issue, we introduce the symbolic metamodeling framework-a general methodology for interpreting predictions by converting "black-box" models into "white-box" functions that are understandable to human subjects. A symbolic metamodel is a model of a model, i.e., a surrogate model of a trained (machine learning) model expressed through a succinct symbolic expression that comprises familiar mathematical functions and can be subjected to symbolic manipulation. We parameterize metamodels using Meijer G-functions-a class of complex-valued contour integrals that depend on real-valued parameters, and whose solutions reduce to familiar algebraic, analytic and closed-form functions for different parameter settings. This parameterization enables efficient optimization of metamodels via gradient descent, and allows discovering the functional forms learned by a model with minimal a priori assumptions. We show that symbolic metamodeling provides a generalized framework for model interpretation-many common forms of model explanation can be analytically derived from a symbolic metamodel.},
author = {Alaa, Ahmed M and {Van Der Schaar}, Mihaela},
title = {{Demystifying Black-box Models with Symbolic Metamodels}}
}
@article{GBD2016AlcoholandDrugUseCollaborators2018,
abstract = {Background: Alcohol and drug use can have negative consequences on the health, economy, productivity, and social aspects of communities. We aimed to use data from the Global Burden of Diseases, Injuries, and Risk Factors Study (GBD) 2016 to calculate global and regional estimates of the prevalence of alcohol, amphetamine, cannabis, cocaine, and opioid dependence, and to estimate global disease burden attributable to alcohol and drug use between 1990 and 2016, and for 195 countries and territories within 21 regions, and within seven super-regions. We also aimed to examine the association between disease burden and Socio-demographic Index (SDI) quintiles. Methods: We searched PubMed, EMBASE, and PsycINFO databases for original epidemiological studies on alcohol and drug use published between Jan 1, 1980, and Sept 7, 2016, with out language restrictions, and used DisMod-MR 2.1, a Bayesian meta-regression tool, to estimate population-level prevalence of substance use disorders. We combined these estimates with disability weights to calculate years of life lived with disability (YLDs), years of life lost (YLLs), and disability-adjusted life-years (DALYs) for 1990–2016. We also used a comparative assessment approach to estimate burden attributable to alcohol and drug use as risk factors for other health outcomes. Findings: Globally, alcohol use disorders were the most prevalent of all substance use disorders, with 100{\textperiodcentered}4 million estimated cases in 2016 (age-standardised prevalence 1320{\textperiodcentered}8 cases per 100 000 people, 95{\%} uncertainty interval [95{\%} UI] 1181{\textperiodcentered}2–1468{\textperiodcentered}0). The most common drug use disorders were cannabis dependence (22{\textperiodcentered}1 million cases; age-standardised prevalence 289{\textperiodcentered}7 cases per 100 000 people, 95{\%} UI 248{\textperiodcentered}9–339{\textperiodcentered}1) and opioid dependence (26{\textperiodcentered}8 million cases; age-standardised prevalence 353{\textperiodcentered}0 cases per 100 000 people, 309{\textperiodcentered}9–405{\textperiodcentered}9). Globally, in 2016, 99{\textperiodcentered}2 million DALYs (95{\%} UI 88{\textperiodcentered}3–111{\textperiodcentered}2) and 4{\textperiodcentered}2{\%} of all DALYs (3{\textperiodcentered}7–4{\textperiodcentered}6) were attributable to alcohol use, and 31{\textperiodcentered}8 million DALYs (27{\textperiodcentered}4–36{\textperiodcentered}6) and 1{\textperiodcentered}3{\%} of all DALYs (1{\textperiodcentered}2–1{\textperiodcentered}5) were attributable to drug use as a risk factor. The burden of disease attributable to alcohol and drug use varied substantially across geographical locations, and much of this burden was due to the effect of substance use on other health outcomes. Contrasting patterns were observed for the association between total alcohol and drug-attributable burden and SDI: alcohol-attributable burden was highest in countries with a low SDI and middle-high middle SDI, whereas the burden due to drugs increased with higher S DI level. Interpretation: Alcohol and drug use are important contributors to global disease burden. Effective interventions should be scaled up to prevent and reduce substance use disease burden. Funding: Bill {\&} Melinda Gates Foundation and Australian National Health and Medical Research Council.},
author = {Degenhardt, Louisa and Charlson, Fiona and Ferrari, Alize and Santomauro, Damian and Erskine, Holly and Mantilla-Herrara, Ana and Whiteford, Harvey and Leung, Janni and Naghavi, Mohsen and Griswold, Max and Rehm, J{\"{u}}rgen and Hall, Wayne and Sartorius, Benn and Scott, James and Vollset, Stein Emil and Knudsen, Ann Kristin and Haro, Josep Maria and Patton, George and Kopec, Jacek and {Carvalho Malta}, Deborah and Topor-Madry, Roman and McGrath, John and Haagsma, Juanita and Allebeck, Peter and Phillips, Michael and Salomon, Joshua and Hay, Simon and Foreman, Kyle and Lim, Stephen and Mokdad, Ali and Smith, Mari and Gakidou, Emmanuela and Murray, Christopher and Vos, Theo},
doi = {10.1016/S2215-0366(18)30337-7},
issn = {22150374},
journal = {The Lancet Psychiatry},
month = {dec},
number = {12},
pages = {987--1012},
pmid = {30392731},
publisher = {Elsevier},
title = {{The global burden of disease attributable to alcohol and drug use in 195 countries and territories, 1990–2016: a systematic analysis for the Global Burden of Disease Study 2016}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/30392731 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC6251968},
volume = {5},
year = {2018}
}
@article{Scarselli2009,
abstract = {Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) is an element of IR(m) that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.},
author = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
doi = {10.1109/TNN.2008.2005605},
issn = {1941-0093},
journal = {IEEE Trans. neural networks},
month = {jan},
number = {1},
pages = {61--80},
pmid = {19068426},
title = {{The graph neural network model.}},
url = {http://ieeexplore.ieee.org/document/4700287/ http://www.ncbi.nlm.nih.gov/pubmed/19068426},
volume = {20},
year = {2009}
}
@article{Bacardit2009,
abstract = {In this paper we empirically evaluate several local search (LS) mechanisms that heuristically edit classification rules and rule sets to improve their performance. Two kinds of operators are studied, (1) rule-wise operators, which edit individual rules, and (2) a rule set-wise operator, which takes the rules from N parents (N {\textgreater} or = 2) to generate a new offspring, selecting the minimum subset of candidate rules that obtains maximum training accuracy. Moreover, various ways of integrating these operators within the evolutionary cycle of learning classifier systems are studied. The combinations of LS operators and policies are integrated in a Pittsburgh approach framework that we call MPLCS for memetic Pittsburgh learning classifier system. MPLCS is systematically evaluated using various metrics. Several datasets were employed with the objective of identifying which combination of operators and policies scale well, are robust to noise, generate compact solutions, and use the least amount of computational resources to solve the problems.},
author = {Bacardit, Jaume and Krasnogor, Natalio},
doi = {10.1162/evco.2009.17.3.307},
issn = {10636560},
journal = {Evol. Comput.},
keywords = {Learning classifier systems,Local search operators,Memetic algorithms,Pittsburgh approach,Smart recombination},
month = {sep},
number = {3},
pages = {307--342},
publisher = {MIT Press 238 Main St., Suite 500, Cambridge, MA 02142-1046 USA journals-info@mit.edu},
title = {{Performance and efficiency of memetic Pittsburgh learning classifier systems}},
url = {http://www.mitpressjournals.org/doi/10.1162/evco.2009.17.3.307},
volume = {17},
year = {2009}
}
@article{Durinck2005,
abstract = {Summary: biomaRt is a new Bioconductor package that integrates BioMart data resources with data analysis software in Bioconductor. It can annotate a wide range of gene or gene product identifiers (e.g. Entrez-Gene and Affymetrix probe identifiers) with information such as gene symbol, chromosomal coordinates, Gene Ontology and OMIM annotation. Furthermore biomaRt enables retrieval of genomic sequences and single nucleotide polymorphism information, which can be used in data analysis. Fast and up-to-date data retrieval is possible as the package executes direct SQL queries to the BioMart databases (e.g. Ensembl). The biomaRt package provides a tight integration of large, public or locally installed BioMart databases with data analysis in Bioconductor creating a powerful environment for biological data mining. {\textcopyright} The Author 2005. Published by Oxford University Press. All rights reserved.},
author = {Durinck, Steffen and Moreau, Yves and Kasprzyk, Arek and Davis, Sean and {De Moor}, Bart and Brazma, Alvis and Huber, Wolfgang},
doi = {10.1093/bioinformatics/bti525},
issn = {13674803},
journal = {Bioinformatics},
month = {aug},
number = {16},
pages = {3439--3440},
pmid = {16082012},
title = {{BioMart and Bioconductor: A powerful link between biological databases and microarray data analysis}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16082012 https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/bti525},
volume = {21},
year = {2005}
}
@incollection{Ying2007,
abstract = {The automatic step detection is a crucial component for the analysis of vegetative locomotor coordination during monitoring the patients with Parkinson's disease. It is aimed to develop the algorithms for automatic step detection in the accelerometer signal, which will be integrated in sensor networks for neurological rehabilitation research. In this paper, three algorithms (Pan-Tompkins method, template matching method and peak detection based on combined dual-axial signals) are detailed described. Finally, these methods will be discussed by means of dis- and advantages.},
address = {Berlin, Heidelberg},
author = {Ying, Hong and Silex, C. and Schnitzer, A. and Leonhardt, S. and Schiek, M.},
booktitle = {4th Int. Work. Wearable Implant. Body Sens. Networks (BSN 2007)},
doi = {10.1007/978-3-540-70994-7_14},
pages = {80--85},
publisher = {Springer Berlin Heidelberg},
title = {{Automatic Step Detection in the Accelerometer Signal}},
url = {http://link.springer.com/10.1007/978-3-540-70994-7{\_}14},
year = {2007}
}
@article{Archer2012,
abstract = {Health status and outcomes are frequently measured on an ordinal scale. For high-throughput genomic datasets, the common approach to analyzing ordinal response data has been to break the problem into one or more dichotomous response analyses. This dichotomous response approach does not make use of all available data and therefore leads to loss of power and increases the number of type I errors. Herein we describe an innovative frequentist approach that combines two statistical techniques, L(1) penalization and continuation ratio models, for modeling an ordinal response using gene expression microarray data. We conducted a simulation study to assess the performance of two computational approaches and two model selection criteria for fitting frequentist L(1) penalized continuation ratio models. Moreover, we empirically compared the approaches using three application datasets, each of which seeks to classify an ordinal class using microarray gene expression data as the predictor variables. We conclude that the L(1) penalized constrained continuation ratio model is a useful approach for modeling an ordinal response for datasets where the number of covariates (p) exceeds the sample size (n) and the decision of whether to use Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC) for selecting the final model should depend upon the similarities between the pathologies underlying the disease states to be classified.},
author = {Archer, K. J. and Williams, A. A.A.},
doi = {10.1002/sim.4484},
issn = {02776715},
journal = {Stat. Med.},
keywords = {Continuation ratio,Gene expression analysis,L 1 penalization,Microarray,Ordinal response},
month = {jun},
number = {14},
pages = {1464--1474},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{L 1 penalized continuation ratio models for ordinal response prediction using high-dimensional datasets}},
url = {http://doi.wiley.com/10.1002/sim.4484},
volume = {31},
year = {2012}
}
@book{James2013,
address = {New York, NY},
author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
doi = {10.1007/978-1-4614-7138-7},
isbn = {978-1-4614-7137-0},
publisher = {Springer New York},
series = {Springer Texts in Statistics},
title = {{An Introduction to Statistical Learning with applications in R}},
url = {http://link.springer.com/10.1007/978-1-4614-7138-7},
volume = {103},
year = {2013}
}
@article{Lenat1986,
abstract = {MCC's CYC project is the building, over the coming decade, of a large knowledge base (or KB) of real world facts and heuristics and - as a part of the KB itself - methods for efficiently reasoning over the KB. As the title of this article suggests, our hypothesis is that of two major limitations to building large intelligent programs might be overcome by using such a system. We briefly illustrate how common sense reasoning and analogy can widen the knowledge acquisition bottleneck. The next section ('How CYC Works') illustrates how those same two abilities can solve problems of the type that stymie current expert systems. We then report how the project is being conducted currently: its strategic philosophy, its tactical methodology, and a case study of how we are currently putting that into practice. We conclude with a discussion of the project's feasibility and timetable.},
author = {Lenat, Doug and Prakash, Mayank and Shepherd, Mary},
doi = {10.1609/AIMAG.V6I4.510},
issn = {07384602},
journal = {AI Mag.},
month = {mar},
number = {4},
pages = {65--85},
title = {{CYC: USING COMMON SENSE KNOWLEDGE TO OVERCOME BRITTLENESS AND KNOWLEDGE ACQUISITION BOTTLENECKS.}},
url = {https://www.aaai.org/ojs/index.php/aimagazine/article/view/510},
volume = {6},
year = {1986}
}
@article{Wal2011,
abstract = {We describe the R package ipw for estimating inverse probability weights. We show how to use the package to fit marginal structural models through inverse probability weighting, to estimate causal effects. Our package can be used with data from a point treatment situation as well as with a time-varying exposure and time-varying confounders. It can be used with binomial, categorical, ordinal and continuous exposure variables.},
author = {van der Wal, Willem M. and Geskus, Ronald B.},
doi = {10.18637/jss.v043.i13},
issn = {1548-7660},
journal = {J. Stat. Softw.},
month = {sep},
number = {13},
pages = {1--23},
title = {{ipw : An R Package for Inverse Probability Weighting}},
url = {http://www.jstatsoft.org/v43/i13/},
volume = {43},
year = {2011}
}
@article{Cardinal2015a,
abstract = {BACKGROUND: The impact of psychotropic drug choice upon admissions for schizophrenia is not well understood. AIMS: To examine the association between antipsychotic/antidepressant use and time in hospital for patients with schizophrenia. METHODS: We conducted an observational study, using 8 years' admission records and electronically generated drug histories from an institution providing secondary mental health care in Cambridgeshire, UK, covering the period 2005-2012 inclusive. Patients with a coded ICD-10 diagnosis of schizophrenia were selected. The primary outcome measure was the time spent as an inpatient in a psychiatric unit. Antipsychotic and antidepressant drugs used by at least 5{\%} of patients overall were examined for associations with admissions. Periods before and after drug commencement were compared for patients having pre-drug admissions, in mirror-image analyses correcting for overall admission rates. Drug use in one 6-month calendar period was used to predict admissions in the next period, across all patients, in a regression analysis accounting for the effects of all other drugs studied and for time. RESULTS: In mirror-image analyses, sulpiride, aripiprazole, clozapine, and olanzapine were associated with fewer subsequent admission days. In regression analyses, sulpiride, mirtazapine, venlafaxine, and clozapine-aripiprazole and clozapine-amisulpride combinations were associated with fewer subsequent admission days. CONCLUSIONS: Use of these drugs was associated with fewer days in hospital. Causation is not implied and these findings require confirmation by randomized controlled trials.},
author = {Cardinal, Rudolf N and Savulich, George and Mann, Louisa M and Fern{\'{a}}ndez-Egea, Emilio},
doi = {10.1038/npjschz.2015.35},
isbn = {2334-265X},
issn = {2334-265X},
journal = {npj Schizophr.},
keywords = {Schizophrenia},
month = {dec},
number = {1},
pages = {15035},
pmid = {27336041},
publisher = {Nature Publishing Group},
title = {{Association between antipsychotic/antidepressant drug treatments and hospital admissions in schizophrenia assessed using a mental health case register}},
url = {http://www.nature.com/articles/npjschz201535},
volume = {1},
year = {2015}
}
@article{Crandall2006,
abstract = {Objectives: To determine whether suicide mortality rates for a cohort of patients seen and subsequently discharged from the ED for a suicide-related complaint were higher than for ED comparison groups. Methods: This was a nonconcurrent cohort study set at a university-affiliated urban ED and Level 1 trauma center. All ED patients 10 years and older, with at least one ED visit between February 1994 and November 2004, were eligible. ED visit characteristics defined the cohort exposure. Patients with visits for suicide attempt or ideation, self-harm, or overdose (exposed) were compared with patients without these visits (unexposed). Exposure classification was determined from billing diagnoses, E-codes (E950-E959), and free-text searching of the ED tracking system data for suicide, overdose, and spelling variants. Emergency department patient data were probabilistically linked to state mortality records. The principal outcome was suicide death. Suicide mortality rates were calculated by using person-year (py) analyses. Relative rates (RR) and 95{\%} confidence intervals (95{\%} CIs) were calculated from Cox proportional hazards models. Results: Among the 218,304 patients, the average follow-up was 6.0 years; there were 408 suicide deaths (incidence rate [IR]: 31.2 per 100,000 py). Males (IR: 48.3) had a higher rate than females (IR: 13.5; RR: 3.6; 95{\%} CI = 2.8 to 4.6). A single ED visit for overdose (RR: 5.7; 95{\%} CI = 4.5 to 7.4), suicidal ideation (RR: 6.7; 95{\%} CI = 5.0 to 9.1), or self-harm (RR: 5.8; 95{\%} CI = 5.1 to 10.6) was strongly associated with increased suicide risk, relative to other patients. Conclusions: The suicide rate among these ED patients is higher than population-based estimates. Rates among patients with suicidal ideation, overdose, or self-harm are especially high, supporting policies that mandate psychiatric interventions in all cases. {\textcopyright} 2006 by the Society for Academic Emergency Medicine.},
author = {Crandall, Cameron and Fullerton-Gleason, Lynne and Aguero, Roberto and LaValley, Jonathon},
doi = {10.1197/j.aem.2005.11.072},
issn = {10696563},
journal = {Acad. Emerg. Med.},
keywords = {Emergency medical services,Follow-up studies,Incidence,Mortality,Suicide},
month = {apr},
number = {4},
pages = {435--442},
pmid = {16531601},
title = {{Subsequent suicide mortality among emergency department patients seen for suicidal behavior}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16531601 http://doi.wiley.com/10.1197/j.aem.2005.11.072},
volume = {13},
year = {2006}
}
@article{Benoit2018a,
abstract = {quanteda is an R package providing a comprehensive workflow and toolkit for natural language processing tasks such as corpus management, tokenization, analysis, and vi-sualization. It has extensive functions for applying dictionary analysis, exploring texts using keywords-in-context, computing document and feature similarities, and discovering multi-word expressions through collocation scoring. Based entirely on sparse operations, it provides highly efficient methods for compiling document-feature matrices and for manipulating these or using them in further quantitative analysis. Using C++ and multi-threading extensively, quanteda is also considerably faster and more efficient than other R and Python packages in processing large textual data. The package is designed for R users needing to apply natural language processing to texts, from documents to final analysis. Its capabilities match or exceed those provided in many end-user software applications, many of which are expensive and not open source. The package is therefore of great benefit to researchers, students, and other analysts with fewer financial resources. While using quanteda requires R programming knowledge, its API is designed to enable powerful, efficient analysis with a minimum of steps. By emphasizing consistent design, furthermore, quanteda lowers the barriers to learning and using NLP and quantitative text analysis even for proficient R programmers. Corpus management quanteda makes it easy to manage texts in the form of a "corpus", which is defined as a collection of texts that includes document-level variables specific to each text, as well as meta-data for documents and for the collection as a whole. With the package, users can easily segment texts by words, paragraphs, sentences, or even user-supplied delimiters and tags, group them into larger documents by document-level variables, or subset them based on logical conditions or combinations of document-level variables.},
author = {Benoit, Kenneth and Watanabe, Kohei and Wang, Haiyan and Nulty, Paul and Obeng, Adam and M{\"{u}}ller, Stefan and Matsuo, Akitaka},
doi = {10.21105/joss.00774},
journal = {J. Open Source Softw.},
number = {30},
title = {{quanteda: An R package for the quantitative analysis of textual data}},
url = {https://doi.org/10.21105/joss.00774},
volume = {3},
year = {2018}
}
@article{Close2014,
abstract = {OBJECTIVE: Lithium users are offered routine renal monitoring but few studies have quantified the risk to renal health. The aim of this study was to assess the association between use of lithium carbonate and incidence of renal failure in patients with bipolar disorder.$\backslash$n$\backslash$nMETHODS: This was a retrospective cohort study using the General Practice Research Database (GPRD) and a nested validation study of lithium exposure and renal failure. A cohort of 6360 participants aged over 18 years had a first recorded diagnosis of bipolar disorder between January 1, 1990 and December 31, 2007. Data were examined from electronic primary care records from 418 general practices across the UK. The primary outcome was the hazard ratio for renal failure in participants exposed to lithium carbonate as compared with non-users of lithium, adjusting for age, gender, co-morbidities, and poly-pharmacy.$\backslash$n$\backslash$nRESULTS: Ever use of lithium was associated with a hazard ratio for renal failure of 2.5 (95{\%} confidence interval 1.6 to 4.0) adjusted for known renal risk factors. Absolute risk was age dependent, with patients of 50 years or older at particular risk of renal failure: Number Needed to Harm (NNH) was 44 (21 to 150).$\backslash$n$\backslash$nCONCLUSIONS: Lithium is associated with an increased risk of renal failure, particularly among the older age group. The absolute risk of renal failure associated with lithium use remains small.},
author = {Close, Helen and Reilly, Joe and Mason, James M. and Kripalani, Mukesh and Wilson, Douglas and Main, John and Hungin, A. Pali S.},
doi = {10.1371/journal.pone.0090169},
editor = {van Os, Jim},
issn = {19326203},
journal = {PLoS One},
month = {mar},
number = {3},
pages = {e90169},
pmid = {24670976},
title = {{Renal failure in lithium-treated bipolar disorder: A retrospective cohort study}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24670976 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3966731 https://dx.plos.org/10.1371/journal.pone.0090169},
volume = {9},
year = {2014}
}
@misc{icos_evolutionary_newcastle_software,
title = {{ICOS Resources :: software and data}},
url = {http://ico2s.org/resources.html},
urldate = {2019-02-20}
}
@article{Poust1976,
abstract = {The effect of chlorothiazide on the pharmacokinetics of lithium in both plasma and RBCs was studied in normal adult males. This was accomplished by administering single, 300 mg. doses of lithium carbonate alone and concurrently with chlorothiazide (0.5 grams/day for one week). Thiazide administration resulted in increases in plasma and RBC concentrations of 26.2 and 25.4{\%}, respectively, as well as a 26.5{\%} decrease in renal lithium clearance. The data were analyzed in terms of a two compartment pharmacokinetic model as previously reported (8). The results of this analysis showed that the change in renal lithium clearance could be accounted for by a 24.1{\%} reduction in the value of ke, the excretion rate constant. It was also shown that changes in plasma lithium concentration during chronic lithium therapy would be expected to increase by 25-30{\%} when chlorothiazide therapy is employed. The model also predicts that changes in RBC concentrations would parallel those occurring in plasma and thus no change in the RBC/plasma lithium ratio would be expected.},
author = {Poust, R I and Mallinger, A. G. and Mallinger, J and Himmelhoch, J M and Neil, J. F. and Hanin, I},
issn = {0098616X},
journal = {Psychopharmacol. Commun.},
number = {3},
pages = {273--284},
pmid = {981712},
title = {{Effect of chlorothiazide on the pharmacokinetics of lithium in plasma and erythrocytes}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/981712},
volume = {2},
year = {1976}
}
@article{Ren2018,
abstract = {Survival analysis is a hotspot in statistical research for modeling time-to-event information with data censorship handling, which has been widely used in many applications such as clinical research, information system and other fields with survivorship bias. Many works have been proposed for survival analysis ranging from traditional statistic methods to machine learning models. However, the existing methodologies either utilize counting-based statistics on the segmented data, or have a pre-assumption on the event probability distribution w.r.t. time. Moreover, few works consider sequential patterns within the feature space. In this paper, we propose a Deep Recurrent Survival Analysis model which combines deep learning for conditional probability prediction at fine-grained level of the data, and survival analysis for tackling the censorship. By capturing the time dependency through modeling the conditional probability of the event for each sample, our method predicts the likelihood of the true event occurrence and estimates the survival rate over time, i.e., the probability of the non-occurrence of the event, for the censored data. Meanwhile, without assuming any specific form of the event probability distribution, our model shows great advantages over the previous works on fitting various sophisticated data distributions. In the experiments on the three real-world tasks from different fields, our model significantly outperforms the state-of-the-art solutions under various metrics.},
archivePrefix = {arXiv},
arxivId = {1809.02403},
author = {Ren, Kan and Qin, Jiarui and Zheng, Lei and Yang, Zhengyu and Zhang, Weinan and Qiu, Lin and Yu, Yong},
doi = {arXiv:1809.02403v2},
eprint = {1809.02403},
month = {sep},
title = {{Deep Recurrent Survival Analysis}},
url = {http://arxiv.org/abs/1809.02403},
year = {2018}
}
@article{Gitlin2016,
abstract = {Despite its virtually universal acceptance as the gold standard in treating bipolar disorder, prescription rates for lithium have been decreasing recently. Although this observation is multifactorial, one obvious potential contributor is the side effect and toxicity burden associated with lithium. Additionally, side effect concerns assuredly play some role in lithium nonadherence. This paper summarizes the knowledge base on side effects and toxicity and suggests optimal management of these problems. Thirst and excessive urination, nausea and diarrhea and tremor are rather common side effects that are typically no more than annoying even though they are rather prevalent. A simple set of management strategies that involve the timing of the lithium dose, minimizing lithium levels within the therapeutic range and, in some situations, the prescription of side effect antidotes will minimize the side effect burden for patients. In contrast, weight gain and cognitive impairment from lithium tend to be more distressing to patients, more difficult to manage and more likely to be associated with lithium nonadherence. Lithium has adverse effects on the kidneys, thyroid gland and parathyroid glands, necessitating monitoring of these organ functions through periodic blood tests. In most cases, lithium-associated renal effects are relatively mild. A small but measurable percentage of lithium-treated patients will show progressive renal impairment. Infrequently, lithium will need to be discontinued because of the progressive renal insufficiency. Lithium-induced hypothyroidism is relatively common but easily diagnosed and treated. Hyperparathyroidism from lithium is a relatively more recently recognized phenomenon.},
author = {Gitlin, Michael},
doi = {10.1186/s40345-016-0068-y},
issn = {21947511},
journal = {Int. J. Bipolar Disord.},
keywords = {Lithium,Nephrotoxicity,Renal function,Side effects,Thyroid},
month = {dec},
number = {1},
pages = {27},
pmid = {27900734},
publisher = {Springer},
title = {{Lithium side effects and toxicity: prevalence and management strategies}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27900734 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5164879},
volume = {4},
year = {2016}
}
@article{Shickel2018,
abstract = {The past decade has seen an explosion in the amount of digital information stored in electronic health records (EHR). While primarily designed for archiving patient clinical information and administrative healthcare tasks, many researchers have found secondary use of these records for various clinical informatics tasks. Over the same period, the machine learning community has seen widespread advances in deep learning techniques, which also have been successfully applied to the vast amount of EHR data. In this paper, we review these deep EHR systems, examining architectures, technical aspects, and clinical applications. We also identify shortcomings of current techniques and discuss avenues of future research for EHR-based deep learning.},
archivePrefix = {arXiv},
arxivId = {1706.03446},
author = {Shickel, Benjamin and Tighe, Patrick James and Bihorac, Azra and Rashidi, Parisa},
doi = {10.1109/JBHI.2017.2767063},
eprint = {1706.03446},
isbn = {978-1-4673-8851-1},
issn = {21682194},
journal = {IEEE J. Biomed. Heal. Informatics},
keywords = {Clinical informatics,deep learning,electronic health records,machine learning,survey},
month = {jun},
number = {5},
pages = {1589--1604},
pmid = {29989977},
title = {{Deep EHR: A Survey of Recent Advances in Deep Learning Techniques for Electronic Health Record (EHR) Analysis}},
url = {http://arxiv.org/abs/1706.03446 http://dx.doi.org/10.1109/JBHI.2017.2767063},
volume = {22},
year = {2018}
}
@article{Fancourt2018,
abstract = {Background There is a recognised need for the identification of factors that might be protective against the development of depression in older adults. Over the past decade, there has been growing research demonstrating the effects of cultural engagement (which combines a number of protective factors including social interaction, cognitive stimulation and gentle physical activity) on the treatment of depression, but as yet not on its prevention. Aims To explore whether cultural engagement in older adults is associated with a reduced risk of developing depression over the following decade. Method Working with data from 2148 adults in the English Longitudinal Study of Ageing who were free from depression at baseline, we used logistic regression models to explore associations between frequency of cultural engagement (including going to museums, theatre and cinema) and the risk of developing depression over the following 10 years using a combined index of the Centre for Epidemiological Studies Depression Scale (CES-D) and physician-diagnosed depression. Results There was a dose–response relationship between frequency of cultural engagement and the risk of developing depression independent of sociodemographic, health-related and social confounders. This equated to a 32{\%} lower risk of developing depression for people who attended every few months (odds ratio (OR) = 0.68, 95{\%} CI 0.47–0.99, P = 0.046) and a 48{\%} lower risk for people who attended once a month or more (OR = 0.52, 95{\%} CI 0.34–0.80, P = 0.003). Results were robust to sensitivity analyses exploring reverse causality, subclinical depressive symptoms and alternative CES-D thresholds. Conclusions Cultural engagement appears to be an independent risk-reducing factor for the development of depression in older age. Declaration of interest None.},
author = {Fancourt, Daisy and Tymoszuk, Urszula},
doi = {10.1192/bjp.2018.267},
issn = {0007-1250},
journal = {Br. J. Psychiatry},
keywords = {Depressive disorders,epidemiology,psychosocial interventions},
month = {dec},
pages = {1--5},
publisher = {Cambridge University Press},
title = {{Cultural engagement and incident depression in older adults: evidence from the English Longitudinal Study of Ageing}},
url = {https://www.cambridge.org/core/product/identifier/S0007125018002672/type/journal{\_}article},
year = {2018}
}
@article{Flechet2017,
abstract = {Purpose: Early diagnosis of acute kidney injury (AKI) remains a major challenge. We developed and validated AKI prediction models in adult ICU patients and made these models available via an online prognostic calculator. We compared predictive performance against serum neutrophil gelatinase-associated lipocalin (NGAL) levels at ICU admission. Methods: Analysis of the large multicenter EPaNIC database. Model development (n = 2123) and validation (n = 2367) were based on clinical information available (1) before and (2) upon ICU admission, (3) after 1 day in ICU and (4) including additional monitoring data from the first 24 h. The primary outcome was a comparison of the predictive performance between models and NGAL for the development of any AKI (AKI-123) and AKI stages 2 or 3 (AKI-23) during the first week of ICU stay. Results: Validation cohort prevalence was 29{\%} for AKI-123 and 15{\%} for AKI-23. The AKI-123 model before ICU admis-sion included age, baseline serum creatinine, diabetes and type of admission (medical/surgical, emergency/planned) and had an AUC of 0.75 (95{\%} CI 0.75–0.75). The AKI-23 model additionally included height and weight (AUC 0.77 (95{\%} CI 0.77–0.77)). Performance consistently improved with progressive data availability to AUCs of 0.82 (95{\%} CI 0.82–0.82) for AKI-123 and 0.84 (95{\%} CI 0.83–0.84) for AKI-23 after 24 h. NGAL was less discriminant with AUCs of 0.74 (95{\%} CI 0.74–0.74) for AKI-123 and 0.79 (95{\%} CI 0.79–0.79) for AKI-23. Take-home message: AKI can be predicted early with models that only use routinely collected clinical information and outperform NGAL measured at ICU admission. The AKI-123 models are available at http:// akipredictor.com. 765 Conclusions: AKI can be predicted early with models that only use routinely collected clinical information and out-perform NGAL measured at ICU admission. The AKI-123 models are available at http://akipredictor.com/.},
author = {Flechet, Marine and G{\"{u}}iza, Fabian and Schetz, Miet and Wouters, Pieter and Vanhorebeek, Ilse and Derese, Inge and Gunst, Jan and Spriet, Isabel and Casaer, Micha{\"{e}}l and {Van den Berghe}, Greet and Meyfroidt, Geert},
doi = {10.1007/s00134-017-4678-3},
issn = {14321238},
journal = {Intensive Care Med.},
keywords = {Acute kidney injury,Early detection,NGAL,Prediction model},
month = {jun},
number = {6},
pages = {764--773},
pmid = {28130688},
title = {{AKIpredictor, an online prognostic calculator for acute kidney injury in adult critically ill patients: development, validation and comparison to serum neutrophil gelatinase-associated lipocalin}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28130688 http://link.springer.com/10.1007/s00134-017-4678-3},
volume = {43},
year = {2017}
}
@misc{Minsky1990,
author = {Minsky, Marvin},
title = {{Symbolic vs Connectionist}},
url = {https://web.media.mit.edu/{~}minsky/papers/SymbolicVs.Connectionist.html},
urldate = {2019-07-08},
year = {1990}
}
@article{Gasparrini2017,
abstract = {Distributed lag non-linear models (DLNMs) are a modelling tool for describing potentially non-linear and delayed dependencies. Here, we illustrate an extension of the DLNM framework through the use of penalized splines within generalized additive models (GAM). This extension offers built-in model selection procedures and the possibility of accommodating assumptions on the shape of the lag structure through specific penalties. In addition, this framework includes, as special cases, simpler models previously proposed for linear relationships (DLMs). Alternative versions of penalized DLNMs are compared with each other and with the standard unpenalized version in a simulation study. Results show that this penalized extension to the DLNM class provides greater flexibility and improved inferential properties. The framework exploits recent theoretical developments of GAMs and is implemented using efficient routines within freely available software. Real-data applications are illustrated through two reproducible examples in time series and survival analysis.},
author = {Gasparrini, Antonio and Scheipl, Fabian and Armstrong, Ben and Kenward, Michael G.},
doi = {10.1111/biom.12645},
issn = {15410420},
journal = {Biometrics},
keywords = {Distributed lag,Exposure-lag-response,Generalized additive models,Latency,Penalized splines},
month = {sep},
number = {3},
pages = {938--948},
pmid = {28134978},
title = {{A penalized framework for distributed lag non-linear models}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28134978 http://doi.wiley.com/10.1111/biom.12645},
volume = {73},
year = {2017}
}
@inproceedings{Condry2016,
abstract = {The last decade has seen huge progress in the development of advanced machine learning models; however, those models are powerless unless human users can interpret them. Here we show how the mind's construction of concepts and meaning can be used to create more interpretable machine learning models. By proposing a novel method of classifying concepts, in terms of 'form' and 'function', we elucidate the nature of meaning and offer proposals to improve model understandability. As machine learning begins to permeate daily life, interpretable models may serve as a bridge between domain-expert authors and non-expert users.},
archivePrefix = {arXiv},
arxivId = {1607.00279},
author = {Condry, Nick},
booktitle = {WHI 2016 (ICML 2016 Work. Hum. Interpret. Mach. Learn.},
eprint = {1607.00279},
month = {jul},
title = {{Meaningful Models: Utilizing Conceptual Structure to Improve Machine Learning Interpretability}},
url = {http://arxiv.org/abs/1607.00279},
year = {2016}
}
@article{Cangea2019,
abstract = {Embodied Question Answering (EQA) is a recently proposed task, where an agent is placed in a rich 3D environment and must act based solely on its egocentric input to answer a given question. The desired outcome is that the agent learns to combine capabilities such as scene understanding, navigation and language understanding in order to perform complex reasoning in the visual world. However, initial advancements combining standard vision and language methods with imitation and reinforcement learning algorithms have shown EQA might be too complex and challenging for these techniques. In order to investigate the feasibility of EQA-type tasks, we build the VideoNavQA dataset that contains pairs of questions and videos generated in the House3D environment. The goal of this dataset is to assess question-answering performance from nearly-ideal navigation paths, while considering a much more complete variety of questions than current instantiations of the EQA task. We investigate several models, adapted from popular VQA methods, on this new benchmark. This establishes an initial understanding of how well VQA-style methods can perform within this novel EQA paradigm.},
archivePrefix = {arXiv},
arxivId = {1908.04950},
author = {Cangea, Cătălina and Belilovsky, Eugene and Li{\`{o}}, Pietro and Courville, Aaron},
eprint = {1908.04950},
month = {aug},
title = {{VideoNavQA: Bridging the Gap between Visual and Embodied Question Answering}},
url = {http://arxiv.org/abs/1908.04950},
year = {2019}
}
@article{Argelaguet2018,
abstract = {Multi-omics studies promise the improved characterization of biological processes across molecular layers. However, methods for the unsupervised integration of the resulting heterogeneous data sets are lacking. We present Multi-Omics Factor Analysis (MOFA), a computational method for discovering the principal sources of variation in multi-omics data sets. MOFA infers a set of (hidden) factors that capture biological and technical sources of variability. It disentangles axes of heterogeneity that are shared across multiple modalities and those specific to individual data modalities. The learnt factors enable a variety of downstream analyses, including identification of sample subgroups, data imputation and the detection of outlier samples. We applied MOFA to a cohort of 200 patient samples of chronic lymphocytic leukaemia, profiled for somatic mutations, RNA expression, DNA methylation and ex vivo drug responses. MOFA identified major dimensions of disease heterogeneity, including immunoglobulin heavy-chain variable region status, trisomy of chromosome 12 and previously underappreciated drivers, such as response to oxidative stress. In a second application, we used MOFA to analyse single-cell multi-omics data, identifying coordinated transcriptional and epigenetic changes along cell differentiation.},
author = {Argelaguet, Ricard and Velten, Britta and Arnol, Damien and Dietrich, Sascha and Zenz, Thorsten and Marioni, John C and Buettner, Florian and Huber, Wolfgang and Stegle, Oliver},
doi = {10.15252/msb.20178124},
issn = {1744-4292},
journal = {Mol. Syst. Biol.},
keywords = {data integration,dimensionality reduction,multi‐omics,personalized medicine,single‐cell omics},
month = {jun},
number = {6},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Multi‐Omics Factor Analysis—a framework for unsupervised integration of multi‐omics data sets}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.15252/msb.20178124},
volume = {14},
year = {2018}
}
@article{Zappia2018,
abstract = {As single-cell RNA-sequencing (scRNA-seq) datasets have become more widespread the number of tools designed to analyse these data has dramatically increased. Navigating the vast sea of tools now available is becoming increasingly challenging for researchers. In order to better facilitate selection of appropriate analysis tools we have created the scRNA-tools database (www.scRNA-tools.org) to catalogue and curate analysis tools as they become available. Our database collects a range of information on each scRNA-seq analysis tool and categorises them according to the analysis tasks they perform. Exploration of this database gives insights into the areas of rapid development of analysis methods for scRNA-seq data. We see that many tools perform tasks specific to scRNA-seq analysis, particularly clustering and ordering of cells. We also find that the scRNA-seq community embraces an open-source and open-science approach, with most tools available under open-source licenses and preprints being extensively used as a means to describe methods. The scRNA-tools database provides a valuable resource for researchers embarking on scRNA-seq analysis and records the growth of the field over time.},
author = {Zappia, Luke and Phipson, Belinda and Oshlack, Alicia},
doi = {10.1371/journal.pcbi.1006245},
editor = {Schneidman, Dina},
issn = {15537358},
journal = {PLoS Comput. Biol.},
month = {jun},
number = {6},
pages = {e1006245},
pmid = {29939984},
title = {{Exploring the single-cell RNA-seq analysis landscape with the scRNA-tools database}},
url = {https://dx.plos.org/10.1371/journal.pcbi.1006245},
volume = {14},
year = {2018}
}
@article{Johnson2016,
abstract = {MIMIC-III, a freely accessible critical care database},
author = {Johnson, Alistair E.W. and Pollard, Tom J. and Shen, Lu and Lehman, Li-wei H. and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and {Anthony Celi}, Leo and Mark, Roger G.},
doi = {10.1038/sdata.2016.35},
issn = {2052-4463},
journal = {Sci. Data},
keywords = {Diagnosis,Health care,Medical research,Outcomes research,Prognosis},
month = {may},
pages = {160035},
publisher = {Nature Publishing Group},
title = {{MIMIC-III, a freely accessible critical care database}},
url = {http://www.nature.com/articles/sdata201635},
volume = {3},
year = {2016}
}
@article{Wiecki2013,
abstract = {Current psychiatric research is in crisis. In this review I will describe the causes of this crisis and highlight recent efforts to overcome current challenges. One particularly promising approach is the emerging field of computational psychiatry. By using methods and insights from computational cognitive neuroscience, computational psychiatry might enable us to move from a symptom-based description of mental illness to descriptors based on objective computational multidimensional functional variables. To exemplify this I will survey recent efforts towards this goal. I will then describe a set of methods that together form a toolbox of cognitive models to aid this research program. At the core of this toolbox are sequential sampling models which have been used to explain diverse cognitive neuroscience phenomena but have so far seen little adoption in psychiatric research. I will then describe how these models can be fitted to subject data and highlight how hierarchical Bayesian estimation provides a rich framework with many desirable properties and benefits compared to traditional optimization-based approaches. Finally, non-parametric Bayesian methods provide general solutions to the problem of classifying mental illness within this framework.},
archivePrefix = {arXiv},
arxivId = {1303.5616},
author = {Wiecki, Thomas V.},
eprint = {1303.5616},
month = {mar},
title = {{Sequential sampling models in computational psychiatry: Bayesian parameter estimation, model selection and classification}},
url = {http://arxiv.org/abs/1303.5616},
year = {2013}
}
@article{Barendregt2003,
abstract = {Epidemiology as an empirical science has developed sophisticated methods to measure the causes and patterns of disease in populations. Nevertheless, for many diseases in many countries only partial data are available. When the partial data are insufficient, but data collection is not an option, it is possible to supplement the data by exploiting the causal relations between the various variables that describe a disease process. We present a simple generic disease model with incidence, one prevalent state, and case fatality and remission. We derive a set of equations that describes this disease process and allows calculation of the complete epidemiology of a disease given a minimum of three input variables. We give the example of asthma with age- specific prevalence, remission, and mortality as inputs. Outputs are incidence and case fatality, among others. The set of equations is embedded in a software package called 'DisMod II', which is made available to the public domain by the World Health Organization. {\textcopyright} 2003 Barendregt et al; licensee BioMed Central Ltd.},
author = {Barendregt, Jan J and van Oortmarssen, Gerrit J. and Vos, Theos and Murray, Christopher JL},
doi = {10.1186/1478-7954-1-4},
issn = {14787954},
journal = {Popul. Health Metr.},
month = {apr},
number = {1},
pages = {4},
pmid = {12773212},
publisher = {BioMed Central},
title = {{A generic model for the assessment of disease epidemiology: The computational basis of DisMod II}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12773212 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC156029},
volume = {1},
year = {2003}
}
@inproceedings{Chen2019a,
abstract = {This paper focuses on a traditional relation extraction task in the context of limited annotated data and a narrow knowledge domain. We explore this task with a clinical corpus consisting of 200 breast cancer follow-up treatment letters in which 16 distinct types of relations are annotated. We experiment with an approach to extracting typed relations called window-bounded co-occurrence (WBC), which uses an adjustable context window around entity mentions of a relevant type, and compare its performance with a more typical intra-sentential co-occurrence baseline. We further introduce a new bag-of-concepts (BoC) approach to feature engineering based on the state-of-the-art word embeddings and word synonyms. We demonstrate the competitiveness of BoC by comparing with methods of higher complexity, and explore its effectiveness on this small dataset.},
archivePrefix = {arXiv},
arxivId = {1904.10743},
author = {Chen, Jiyu and Verspoor, Karin and Zhai, Zenan},
doi = {10.18653/v1/n19-3007},
eprint = {1904.10743},
month = {apr},
pages = {43--52},
title = {{A Bag-of-concepts Model Improves Relation Extraction in a Narrow Knowledge Domain with Limited Data}},
url = {http://arxiv.org/abs/1904.10743},
year = {2019}
}
@article{Hoffmann2018,
abstract = {The parasite Plasmodium falciparum is the main cause of severe malaria (SM). Despite treatment with antimalarial drugs, more than 450,000 SM deaths are reported every year, mainly in African children. The diversity of clinical presentations associated with SM indicate important differences in disease pathogenesis that often require specific treatment, and this clinical heterogeneity of SM is largely unresolved. In this study, we apply new machine learning and inference tools for large-scale data analysis to dissect the heterogeneity in patterns of clinical features associated with SM in 2,695 Gambian children admitted to hospital with Plasmodium falciparum malaria. This quantitative analysis, including the powerful HyperTraPS algorithm for inference of progressive processes, reveals pathways of SM symptom progression and features predicting the severity of individual patient outcomes. Notably, our approach allows the identification and dissection of disease progression pathways without the need for longitudinal observations. Learning these pathways and features from this rich dataset allows us to construct several quantitative measures of the mortality risk associated with a patient presenting with a given set of symptoms. By independently surveying expert practitioners, we show that this data-driven approach agrees with and expands the current state of knowledge on malaria progression, while simultaneously providing a data-supported framework for predicting clinical risk.},
author = {Hoffmann, Till and Johnston, Iain G. and Greenbury, Sam and Cominetti, Ornella and Jallow, Muminatou and Kwiatkowski, Dominic and Barahona, Mauricio and Jones, Nick S. and Casals-Pascual, Climent},
doi = {10.1101/425132},
issn = {2398-6352},
journal = {bioRxiv},
keywords = {Applied mathematics,Developing world,Malaria},
month = {dec},
number = {1},
pages = {425132},
publisher = {Nature Publishing Group},
title = {{Precision identification and prediction of high mortality phenotypes and disease progression pathways in severe malaria without requiring longitudinal data}},
url = {http://www.nature.com/articles/s41746-019-0140-y https://www.biorxiv.org/content/early/2018/09/24/425132},
volume = {2},
year = {2018}
}
@article{Chaudhary2017a,
abstract = {Identifying robust survival subgroups of hepatocellular carcinoma (HCC) will significantly improve patient care. Currently, endeavor of integrating multi-omics data to explicitly predict HCC survival from multiple patient cohorts is lacking. To fill in this gap, we present a deep learning (DL) based model on HCC that robustly differentiates survival subpopulations of patients in six cohorts. We build the DL based, survival-sensitive model on 360 HCC patients' data using RNA-seq, miRNA-seq and methylation data from TCGA, which predicts prognosis as good as an alternative model where genomics and clinical data are both considered. This DL based model provides two optimal subgroups of patients with significant survival differences (P=7.13e-6) and good model fitness (C-index=0.68). More aggressive subtype is associated with frequent TP53 inactivation mutations, higher expression of stemness markers (KRT19, EPCAM) and tumor marker BIRC5, and activated Wnt and Akt signaling pathways. We validated this multi-omics model on five external datasets of various omics types: LIRI-JP cohort (n=230, C-index=0.75), NCI cohort (n=221, C-index=0.67), Chinese cohort (n=166, C-index=0.69), E-TABM-36 cohort (n=40, C-index=0.77), and Hawaiian cohort (n=27, C-index=0.82). This is the first study to employ deep learning to identify multi-omics features linked to the differential survival of HCC patients. Given its robustness over multiple cohorts, we expect this workflow to be useful at predicting HCC prognosis prediction.},
author = {Chaudhary, Kumardeep and Poirion, Olivier and Lu, Liangqun and Garmire, Lana X.},
doi = {10.1101/114892},
journal = {bioRxiv},
month = {sep},
pages = {114892},
publisher = {Cold Spring Harbor Laboratory},
title = {{Deep Learning based multi-omics integration}},
url = {https://www.biorxiv.org/content/early/2017/09/18/114892},
year = {2017}
}
@article{Kellogg2018,
abstract = {This two-year ethnographic study of the primary care departments in two U.S. hospitals examines how managers can bring about micro-level institutional change in professional practice even when such change challenges professionals' specialized expertise, autonomy, individual responsibility, and engagement in complex work, which previous research has shown to create difficulties. In this study, managers in both hospitals attempted to implement the same patient-centered medical home (PCMH) reforms among doctors, had the same external pressures for micro-level institutional change, worked under the same organizational and reimbursement structure, and had the same contextual facilitators of micro-level institutional change present within their organizations. But managers in one hospital successfully accomplished change in professional practice while those in the other did not. I demonstrate that managers can accomplish micro-level institutional change in professional organizations using “subordinate activation tactics”—first empowering and motivating subordinate semi-professionals to activate their favorable structural position vis-{\`{a}}-vis the targeted professionals on behalf of managers and next giving semi-professionals positional tools to use in their daily work to minimize the targeted professionals' concerns about the threats associated with change.},
author = {Kellogg, Katherine C.},
doi = {10.1177/0001839218804527},
issn = {19303815},
journal = {Adm. Sci. Q.},
keywords = {PCMH implementation,healthcare management,hospitals,micro-level institutional change,occupations,professional organizations,professions,semi-professionals},
month = {oct},
pages = {000183921880452},
title = {{Subordinate Activation Tactics: Semi-professionals and Micro-level Institutional Change in Professional Organizations}},
url = {http://journals.sagepub.com/doi/10.1177/0001839218804527},
year = {2018}
}
@article{Mooij2000,
abstract = {The discovery of causal relationships from purely observational data is a fundamental problem in science. The most elementary form of such a causal discovery problem is to decide whether X causes Y or, alternatively, Y causes X, given joint observations of two variables X,Y. An example is to decide whether altitude causes temperature, or vice versa, given only joint measurements of both variables. Even under the simplifying assumptions of no confounding, no feedback loops, and no selection bias, such bivariate causal discovery problems are challenging. Nevertheless, several approaches for addressing those problems have been proposed in recent years. We review two families of such methods: methods based on Additive Noise Models (ANMs) and Information Geometric Causal Inference (IGCI). We present the benchmark CauseEffectPairs that consists of data for 100 different cause-effect pairs selected from 37 data sets from various domains (e.g., meteorology, biology, medicine, engineering, economy, etc.) and motivate our decisions regarding the ground truth causal directions of all pairs. We evaluate the performance of several bivariate causal discovery methods on these real-world benchmark data and in addition on artificially simulated data. Our empirical results on real-world data indicate that certain methods are indeed able to distinguish cause from effect using only purely observational data, although more benchmark data would be needed to obtain statistically significant conclusions. One of the best performing methods overall is the method based on Additive Noise Models that has originally been proposed by Hoyer et al. (2009), which obtains an accuracy of 63 ± 10 {\%} and an AUC of 0.74 ± 0.05 on the real-world benchmark. As the main theoretical contribution of this work we prove the consistency of that method.},
author = {Mooij, Joris M. and Peters, Jonas and Janzing, Dominik and Zscheischler, Jakob and Sch{\"{o}}lkopf, Bernhard},
issn = {15337928},
journal = {J. Mach. Learn. Res.},
number = {32},
pages = {1--102},
publisher = {MIT Press},
title = {{Distinguishing Cause from Effect Using Observational Data: Methods and Benchmarks}},
url = {http://jmlr.org/papers/v17/14-518.html},
volume = {17},
year = {2000}
}
@article{Davis2018,
abstract = {Objective: Young people with asthma often lack engagement in self-management. Smartphone apps offer an attractive, immediate method for obtaining asthma information and self-management support. In this research we developed an evidence-based asthma app tailored to young peoples needs, created using a participatory design approach to optimize user engagement. This paper describes the participatory design process. Methods: This multi-phased research included concept generation and ideation of app design by young people with asthma, and development of asthma information by the research team. Clinical review was sought regarding safety and accuracy of app content. Participants suggestions for improvement and any problems with the app were logged throughout. Our young co-designers were invited back to test a high fidelity prototype app using a “think aloud” process and completed a usability questionnaire. Results: Twenty asthma patients aged 15-24 years contributed to the initial app design. Three respiratory specialists and two pharmacists suggested minor corrections to clinical terminology in the app which were all incorporated. Nine co-designers acted as expert reviewers of the prototype app, of whom eight completed a usability questionnaire. Median usability scores (maximum score 6) indicated high satisfaction with app content, usefulness and ease of use [median item score 5.3 (range 4.7-6.0)]. All feedback was incorporated to create an updated prototype app. Conclusions: A clinically sound asthma app has been developed which is considered highly acceptable to the young co-designers. A six-week test of the engagement, acceptability, and usefulness of the app in young people not involved in the participatory design will follow.},
author = {Davis, S. R. and Peters, D and Calvo, R. A. and Sawyer, S. M. and Foster, J. M. and Smith, L},
doi = {10.1080/02770903.2017.1388391},
issn = {15324303},
journal = {J. Asthma},
keywords = {Adolescents,asthma,ideation activity,lived experience,notifications,participatory design,prototype,young people},
month = {sep},
number = {9},
pages = {1018--1027},
publisher = {Taylor {\&} Francis},
title = {{“Kiss myAsthma”: Using a participatory design approach to develop a self-management app with young people with asthma}},
url = {https://www.tandfonline.com/doi/full/10.1080/02770903.2017.1388391},
volume = {55},
year = {2018}
}
@article{Stewart2019,
abstract = {Tissue-resident immune cells are important for organ homeostasis and defense. The epithelium may contribute to these functions directly or by cross-talk with immune cells. We used single-cell RNA sequencing to resolve the spatiotemporal immune topology of the human kidney. We reveal anatomically defined expression patterns of immune genes within the epithelial compartment, with antimicrobial peptide transcripts evident in pelvic epithelium in the mature, but not fetal, kidney. A network of tissue-resident myeloid and lymphoid immune cells was evident in both fetal and mature kidney, with postnatal acquisition of transcriptional programs that promote infection-defense capabilities. Epithelial-immune cross-talk orchestrated localization of antibacterial macrophages and neutrophils to the regions of the kidney most susceptible to infection. Overall, our study provides a global overview of how the immune landscape of the human kidney is zonated to counter the dominant immunological challenge.},
author = {Stewart, Benjamin J. and Ferdinand, John R. and Young, Matthew D. and Mitchell, Thomas J. and Loudon, Kevin W. and Riding, Alexandra M. and Richoz, Nathan and Frazer, Gordon L. and Staniforth, Joy U. L. and {Vieira Braga}, Felipe A. and Botting, Rachel A. and Popescu, Dorin-Mirel and Vento-Tormo, Roser and Stephenson, Emily and Cagan, Alex and Farndon, Sarah J. and Polanski, Krzysztof and Efremova, Mirjana and Green, Kile and {Del Castillo Velasco-Herrera}, Martin and Guzzo, Charlotte and Collord, Grace and Mamanova, Lira and Aho, Tevita and Armitage, James N. and Riddick, Antony C. P. and Mushtaq, Imran and Farrell, Stephen and Rampling, Dyanne and Nicholson, James and Filby, Andrew and Burge, Johanna and Lisgo, Steven and Lindsay, Susan and Bajenoff, Marc and Warren, Anne Y. and Stewart, Grant D. and Sebire, Neil and Coleman, Nicholas and Haniffa, Muzlifah and Teichmann, Sarah A. and Behjati, Sam and Clatworthy, Menna R.},
doi = {10.1126/science.aat5031},
issn = {0036-8075},
journal = {Science (80-. ).},
month = {sep},
number = {6460},
pages = {1461--1466},
publisher = {American Association for the Advancement of Science},
title = {{Spatiotemporal immune zonation of the human kidney}},
url = {http://www.sciencemag.org/lookup/doi/10.1126/science.aat5031},
volume = {365},
year = {2019}
}
@article{Sandborn2018a,
author = {Sandborn, William J. and Ferrante, Marc and Bhandari, Bal R. and D'Haens, Geert R. and Berliba, Elina and Feagan, Brian G. and Laskowski, Janelle and Friedrich, Stuart and Durante, Michael and Tuttle, Jay},
doi = {10.1016/s0016-5085(18)34449-4},
issn = {00165085},
journal = {Gastroenterology},
month = {may},
number = {6},
pages = {S--1360--S--1361},
publisher = {Elsevier BV},
title = {{882 - Efficacy and Safety of Anti-Interleukin-23 Therapy with Mirikizumab (LY3074828) in Patients with Moderate-To-Severe Ulcerative Colitis in a Phase 2 Study}},
volume = {154},
year = {2018}
}
@article{Huang2019,
abstract = {In this paper, we show that every {\$}(2{\^{}}{\{}n-1{\}}+1){\$}-vertex induced subgraph of the {\$}n{\$}-dimensional cube graph has maximum degree at least {\$}\backslashsqrt{\{}n{\}}{\$}. This result is best possible, and improves a logarithmic lower bound shown by Chung, F$\backslash$"uredi, Graham and Seymour in 1988. As a direct consequence, we prove that the sensitivity and degree of a boolean function are polynomially related, solving an outstanding foundational problem in theoretical computer science, the Sensitivity Conjecture of Nisan and Szegedy.},
archivePrefix = {arXiv},
arxivId = {1907.00847},
author = {Huang, Hao},
eprint = {1907.00847},
month = {jul},
title = {{Induced subgraphs of hypercubes and a proof of the Sensitivity Conjecture}},
url = {http://arxiv.org/abs/1907.00847},
year = {2019}
}
@misc{pyprob,
title = {{etalumis/pyprob: A PyTorch-based library for probabilistic programming and inference compilation}},
url = {https://github.com/etalumis/pyprob},
urldate = {2019-09-18}
}
@article{Slagle1963,
abstract = {Thesis (Ph. D.)--Massachusetts Institute of Technology, Dept. of Mathematics, 1961.},
author = {Slagle, James R.},
doi = {10.1145/321186.321193},
issn = {1557735X},
journal = {J. ACM},
keywords = {Mathematics,Thesis},
number = {4},
pages = {507--520},
publisher = {Massachusetts Institute of Technology},
title = {{A Heuristic Program that Solves Symbolic Integration Problems in Freshman Calculus}},
volume = {10},
year = {1963}
}
@article{Ohrnberger2017,
abstract = {There is a strong link between mental health and physical health, but little is known about the pathways from one to the other. We analyse the direct and indirect effects of past mental health on present physical health and past physical health on present mental health using lifestyle choices and social capital in a mediation framework. We use data on 10,693 individuals aged 50 years and over from six waves (2002–2012) of the English Longitudinal Study of Ageing. Mental health is measured by the Centre for Epidemiological Studies Depression Scale (CES) and physical health by the Activities of Daily Living (ADL). We find significant direct and indirect effects for both forms of health, with indirect effects explaining 10{\%} of the effect of past mental health on physical health and 8{\%} of the effect of past physical health on mental health. Physical activity is the largest contributor to the indirect effects. There are stronger indirect effects for males in mental health (9.9{\%}) and for older age groups in mental health (13.6{\%}) and in physical health (12.6{\%}). Health policies aiming at changing physical and mental health need to consider not only the direct cross-effects but also the indirect cross-effects between mental health and physical health.},
author = {Ohrnberger, Julius and Fichera, Eleonora and Sutton, Matt},
doi = {10.1016/j.socscimed.2017.11.008},
issn = {18735347},
journal = {Soc. Sci. Med.},
keywords = {Mediation analysis,Mental health,Older population,Physical health,UK},
pages = {42--49},
title = {{The relationship between physical and mental health: A mediation analysis}},
volume = {195},
year = {2017}
}
@misc{mhrn_website,
title = {{Mental Health Research Network}},
url = {http://hcsrn.org/mhrn/en/},
urldate = {2018-11-13}
}
@article{Regli2017,
author = {Regli, William},
doi = {10.1145/2983529},
journal = {Commun. ACM},
month = {mar},
number = {4},
pages = {26--28},
title = {{Wanted: Toolsmiths}},
url = {http://dl.acm.org/citation.cfm?doid=3069398.2983529},
volume = {60},
year = {2017}
}
@article{Zappia2018a,
abstract = {As single-cell RNA-sequencing (scRNA-seq) datasets have become more widespread the number of tools designed to analyse these data has dramatically increased. Navigating the vast sea of tools now available is becoming increasingly challenging for researchers. In order to better facilitate selection of appropriate analysis tools we have created the scRNA-tools database (www.scRNA-tools.org) to catalogue and curate analysis tools as they become available. Our database collects a range of information on each scRNA-seq analysis tool and categorises them according to the analysis tasks they perform. Exploration of this database gives insights into the areas of rapid development of analysis methods for scRNA-seq data. We see that many tools perform tasks specific to scRNA-seq analysis, particularly clustering and ordering of cells. We also find that the scRNA-seq community embraces an open-source and open-science approach, with most tools available under open-source licenses and preprints being extensively used as a means to describe methods. The scRNA-tools database provides a valuable resource for researchers embarking on scRNA-seq analysis and records the growth of the field over time.},
author = {Zappia, Luke and Phipson, Belinda and Oshlack, Alicia},
doi = {10.1371/journal.pcbi.1006245},
editor = {Schneidman, Dina},
issn = {15537358},
journal = {PLoS Comput. Biol.},
month = {jun},
number = {6},
pages = {e1006245},
pmid = {29939984},
publisher = {Public Library of Science},
title = {{Exploring the single-cell RNA-seq analysis landscape with the scRNA-tools database}},
url = {https://dx.plos.org/10.1371/journal.pcbi.1006245},
volume = {14},
year = {2018}
}
@inproceedings{Guo2009,
abstract = {In the data mining research, mining association rules is an important topic. Apriori algorithm submitted by Agrawal and R. Srikant in 1994 is the most effective algorithm. Aimed at two problems of discovering frequent itemsets in a large database and mining association rules from frequent itemsets, the author makes some research on mining frequent itemsets algorithm based on apriori algorithm and mining association rules algorithm based on improved measure system. Mining association rules algorithm based on support, confidence and interestingness is improved, aiming at creating interestingness useless rules and losing useful rules. Useless rules are cancelled, creating more reasonable association rules including negative items. The above method is used to mine association rules to the 2002 student score list of computer specialized field in Inner Mongolia university of science and technology.},
author = {Guo, Hong and Zhou, Ya},
booktitle = {3rd Int. Conf. Genet. Evol. Comput. WGEC 2009},
doi = {10.1109/WGEC.2009.15},
isbn = {9780769538990},
keywords = {Apriori,Association rule,Data mining,Genetic algorithm,Premature convergence},
pages = {117--120},
title = {{An algorithm for mining association rules based on improved genetic algorithm and its application}},
year = {2009}
}
@article{Betzel2017,
abstract = {Network neuroscience is the emerging discipline concerned with investigating the complex patterns of interconnections found in neural systems, and to identify principles with which to understand them. Within this discipline, one particularly powerful approach is network generative modeling, in which wiring rules are algorithmically implemented to produce synthetic network architectures with the same properties as observed in empirical network data. Successful models can highlight the principles by which a network is organized and potentially uncover the mechanisms by which it grows and develops. Here we review the prospects and promise of generative models for network neuroscience. We begin with a primer on network generative models, with a discussion of compressibility and predictability, utility in intuiting mechanisms, and a short history on their use in network science broadly. We then discuss generative models in practice and application, paying particular attention to the critical need for cross-validation. Next, we review generative models of biological neural networks, both at the cellular and large-scale level, and across a variety of species including $\backslash$emph{\{}C. elegans{\}}, $\backslash$emph{\{}Drosophila{\}}, mouse, rat, cat, macaque, and human. We offer a careful treatment of a few relevant distinctions, including differences between generative models and null models, sufficiency and redundancy, inferring and claiming mechanism, and functional and structural connectivity. We close with a discussion of future directions, outlining exciting frontiers both in empirical data collection efforts as well as in method and theory development that, together, further the utility of the generative network modeling approach for network neuroscience.},
archivePrefix = {arXiv},
arxivId = {1708.07958},
author = {Betzel, Richard F. and Bassett, Danielle S.},
eprint = {1708.07958},
month = {aug},
title = {{Generative Models for Network Neuroscience: Prospects and Promise}},
url = {http://arxiv.org/abs/1708.07958},
year = {2017}
}
@book{Calvo2014,
address = {New York},
author = {Calvo, Rafael A. and D'Mello, Sidney and Gratch, Jonathan and Kappas, Arvid},
publisher = {Oxford University Press},
title = {{The Oxford Handbook of Affective Computing}},
url = {https://dl.acm.org/citation.cfm?id=2787789},
year = {2014}
}
@article{Iwagami2017a,
abstract = {{\textcopyright} 2017 The Authors. Pharmacoepidemiology {\&} Drug Safety Published by John Wiley {\&} Sons Ltd Purpose: People with chronic kidney disease (CKD) have an increased prevalence of depression, anxiety, and neuropathic pain. We examined prevalence, incidence, indication for, and choice of antidepressants among patients with and without CKD. Methods: Using the UK Clinical Practice Research Datalink, we identified patients with CKD (two measurements of estimated glomerular filtration rate {\textless} 60 mL/min/1.73m 2 for ≥3 months) between April 2004 and March 2014. We compared those with CKD to a general population cohort without CKD (matched on age, sex, general practice, and calendar time [index date]). We identified any antidepressant prescribing in the six months prior to index date (prevalence), the first prescription after index date among non-prevalent users (incidence), and recorded diagnoses (indication). We compared antidepressant choice between patients with and without CKD among patients with a diagnosis of depression. Results: There were 242 349 matched patients (median age 76 [interquartile range 70–82] , male 39.3{\%}) with and without CKD. Prevalence of antidepressant prescribing was 16.3 and 11.9{\%}, and incidence was 57.2 and 42.4/1000 person-years, in patients with and without CKD, respectively. After adjusting for confounders, CKD remained associated with higher prevalence and incidence of antidepressant prescription. Regardless of CKD status, selective serotonin reuptake inhibitors were predominantly prescribed for depression or anxiety, while tricyclic antidepressants were prescribed for neuropathic pain or other reasons. Antidepressant choice was similar in depressed patients with and without CKD. Conclusions: The rate of antidepressant prescribing was nearly one and a half times higher among people with CKD than in the general population. {\textcopyright} 2017 The Authors. Pharmacoepidemiology {\&} Drug Safety Published by John Wiley {\&} Sons Ltd.},
author = {Iwagami, Masao and Tomlinson, Laurie A. and Mansfield, Kathryn E. and McDonald, Helen I. and Smeeth, Liam and Nitsch, Dorothea},
doi = {10.1002/pds.4212},
issn = {10991557},
journal = {Pharmacoepidemiol. Drug Saf.},
keywords = {antidepressants,chronic kidney disease,depression,incidence,prevalence},
month = {jul},
number = {7},
pages = {792--801},
title = {{Prevalence, incidence, indication, and choice of antidepressants in patients with and without chronic kidney disease: a matched cohort study in UK Clinical Practice Research Datalink}},
url = {http://doi.wiley.com/10.1002/pds.4212},
volume = {26},
year = {2017}
}
@article{Tyson2003b,
abstract = {The physiological responses of cells to external and internal stimuli are governed by genes and proteins interacting in complex networks whose dynamical properties are impossible to understand by intuitive reasoning alone. Recent advances by theoretical biologists have demonstrated that molecular regulatory networks can be accurately modeled in mathematical terms. These models shed light on the design principles of biological control systems and make predictions that have been verified experimentally.},
archivePrefix = {arXiv},
arxivId = {arXiv:cond-mat/9907068v1},
author = {Tyson, John J and Chen, Katherine C and Novak, Bela},
doi = {10.1016/S0955-0674(03)00017-6},
eprint = {9907068v1},
isbn = {0955-0674},
issn = {09550674},
journal = {Curr. Opin. Cell Biol.},
month = {apr},
number = {2},
pages = {221--231},
pmid = {12648679},
primaryClass = {arXiv:cond-mat},
title = {{Sniffers, buzzers, toggles and blinkers: Dynamics of regulatory and signaling pathways in the cell}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12648679},
volume = {15},
year = {2003}
}
@article{Hopfield1982,
abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
author = {Hopfield, J J},
doi = {10.1073/pnas.79.8.2554},
issn = {0027-8424},
journal = {Proc. Natl. Acad. Sci. U. S. A.},
month = {apr},
number = {8},
pages = {2554--8},
pmid = {6953413},
publisher = {National Academy of Sciences},
title = {{Neural networks and physical systems with emergent collective computational abilities.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/6953413 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC346238},
volume = {79},
year = {1982}
}
@article{Aronson2010,
abstract = {MetaMap is a widely available program providing access to the concepts in the unified medical language system (UMLS) Metathesaurus from biomedical text. This study reports on MetaMap's evolution over more than a decade, concentrating on those features arising out of the research needs of the biomedical informatics community both within and outside of the National Library of Medicine. Such features include the detection of author-defined acronyms/abbreviations, the ability to browse the Metathesaurus for concepts even tenuously related to input text, the detection of negation in situations in which the polarity of predications is important, word sense disambiguation (WSD), and various technical and algorithmic features. Near-term plans for MetaMap development include the incorporation of chemical name recognition and enhanced WSD.},
author = {Aronson, Alan R and Lang, Fran{\c{c}}ois Michel},
doi = {10.1136/jamia.2009.002733},
issn = {10675027},
journal = {J. Am. Med. Informatics Assoc.},
number = {3},
pages = {229--236},
pmid = {20442139},
publisher = {American Medical Informatics Association},
title = {{An overview of MetaMap: Historical perspective and recent advances}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20442139 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2995713},
volume = {17},
year = {2010}
}
@article{Lindsay2019,
abstract = {Background: Physical activity (PA) plays a role in the prevention of a range of diseases including obesity and cardiometabolic disorders. Large population-based descriptive studies of PA, incorporating precise measurement, are needed to understand the relative burden of insufficient PA levels and to inform the tailoring of interventions. Combined heart and movement sensing enables the study of physical activity energy expenditure (PAEE) and intensity distribution. We aimed to describe the sociodemographic correlates of PAEE and moderate-to-vigorous physical activity (MVPA) in UK adults. Methods: The Fenland study is a population-based cohort study of 12,435 adults aged 29-64 years-old in Cambridgeshire, UK. Following individual calibration (treadmill), participants wore a combined heart rate and movement sensor continuously for 6 days in free-living, from which we derived PAEE (kJ•day- 1•kg- 1) and time in MVPA ({\textgreater} 3 {\&} {\textgreater} 4 METs) in bouts greater than 1 min and 10 min. Socio-demographic information was self-reported. Stratum-specific summary statistics and multivariable analyses were performed. Results: Women accumulated a mean (sd) 50(20) kJ•day- 1•kg- 1 of PAEE, and 83(67) and 33(39) minutes•day- 1 of 1-min bouted and 10-min bouted MVPA respectively. By contrast, men recorded 59(23) kJ•day- 1•kg- 1, 124(84) and 60(58) minutes•day- 1. Age and BMI were also important correlates of PA. Association with age was inverse in both sexes, more strongly so for PAEE than MVPA. Obese individuals accumulated less PA than their normal-weight counterparts, whether considering PAEE or allometrically-scaled PAEE (- 10 kJ•day- 1•kg- 1 or - 15 kJ•day- 1•kg-2/3 in men). Higher income and manual work were associated with higher PA; manual workers recorded 13-16 kJ•kg- 1•day- 1 more PAEE than sedentary counterparts. Overall, 86{\%} of women and 96{\%} of men accumulated a daily average of MVPA ({\textgreater} 3 METs) corresponding to 150 min per week. These values were 49 and 74{\%} if only considering bouts {\textgreater} 10 min (15 and 31{\%} for {\textgreater} 4 METs). Conclusions: PA varied by age, sex and BMI, and was higher in manual workers and those with higher incomes. Light physical activity was the main driver of PAEE; a component of PA that is currently not quantified as a target in UK guidelines.},
author = {Lindsay, Tim and Westgate, Kate and Wijndaele, Katrien and Hollidge, Stefanie and Kerrison, Nicola and Forouhi, Nita and Griffin, Simon and Wareham, Nick and Brage, S{\o}ren},
doi = {10.1186/s12966-019-0882-6},
issn = {14795868},
journal = {Int. J. Behav. Nutr. Phys. Act.},
keywords = {Physical activity,combined sensing,energy expenditure,sociodemographic},
month = {aug},
number = {1},
pages = {19003442},
publisher = {Cold Spring Harbor Laboratory Press},
title = {{Descriptive epidemiology of physical activity energy expenditure in UK adults (The Fenland study)}},
volume = {16},
year = {2019}
}
@article{Cranmer2019a,
abstract = {We introduce an approach for imposing physically motivated inductive biases on graph networks to learn interpretable representations and improved zero-shot generalization. Our experiments show that our graph network models, which implement this inductive bias, can learn message representations equivalent to the true force vector when trained on n-body gravitational and spring-like simulations. We use symbolic regression to fit explicit algebraic equations to our trained model's message function and recover the symbolic form of Newton's law of gravitation without prior knowledge. We also show that our model generalizes better at inference time to systems with more bodies than had been experienced during training. Our approach is extensible, in principle, to any unknown interaction law learned by a graph network, and offers a valuable technique for interpreting and inferring explicit causal theories about the world from implicit knowledge captured by deep learning.},
archivePrefix = {arXiv},
arxivId = {1909.05862},
author = {Cranmer, Miles D. and Xu, Rui and Battaglia, Peter and Ho, Shirley},
eprint = {1909.05862},
month = {sep},
title = {{Learning Symbolic Physics with Graph Networks}},
url = {http://arxiv.org/abs/1909.05862},
year = {2019}
}
@inproceedings{Zilke2016,
abstract = {Neural network classifiers are known to be able to learn very accurate models. In the recent past, researchers have even been able to train neural networks with multiple hidden layers (deep neural networks) more effectively and efficiently. However, the major downside of neural networks is that it is not trivial to understand the way how they derive their classification decisions. To solve this problem, there has been research on extracting better understandable rules from neural networks. However, most authors focus on nets with only one single hidden layer. The present paper introduces a new decompositional algorithm – DeepRED – that is able to extract rules from deep neural networks. The evaluation of the proposed algorithm shows its ability to outperform a pedagogical baseline on several tasks, including the successful extraction of rules from a neural network realizing the XOR function.},
author = {Zilke, Jan Ruben and Menc{\'{i}}a, Eneldo Loza and Janssen, Frederik},
booktitle = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
doi = {10.1007/978-3-319-46307-0_29},
isbn = {9783319463063},
issn = {16113349},
month = {oct},
pages = {457--473},
publisher = {Springer, Cham},
title = {{DeepRED – Rule extraction from deep neural networks}},
url = {http://link.springer.com/10.1007/978-3-319-46307-0{\_}29},
volume = {9956 LNAI},
year = {2016}
}
@article{Ursu2019,
abstract = {To investigate the mechanistic novelty of new drugs introduced in 2018 — a record year for FDA approvals — we tracked the year of first approval of new drugs in the United States, as well as in Europe and Japan. We identified 39 drugs that have mode-of-action (MoA) biomolecular targets annotated (as defined in Nat. Rev. Drug Discov. 16, 19–34; 2017), according to primary literature or package insert information. Here, we discuss the 13 MoA targets that had not been previously modulated by an approved drug, that is, novel molecular drug targets introduced in 2018 (Table 1). Six of the novel drug targets emerged from already established protein families: kinases (two), G protein-coupled receptors (one), enzymes (two) and transporters (one). Four monoclonal antibody (mAb)-based drugs target secreted proteins (a growth factor, a coagulation factor, a cytokine and a neurotransmitter peptide). Two proteins are of viral origin. From a therapeutic area perspective, oncology, haematological disorders (beyond cancer) and infectious diseases each gained three novel targets. In the oncology area, larotrectinib, the first drug targeting tropomyosin receptor kinases, also represented the first approval of a drug that was developed specifically for a tumour-agnostic indication. Caplacizumab, which targets von Willebrand factor to treat acquired thrombotic thrombocytopenic purpura, is the first nanobody to be approved, and ibalizumab, a mAb that targets the CD4 co-receptor used by HIV to enter T cells, is the first biologic to be approved for HIV infection. There were also particularly notable advances in the central nervous system disorders, with the approval of the first anti-migraine therapies with a novel mechanism — targeting calcitonin-gene-related peptide (CGRP) signalling MoA — since the 1990s. Two mAb drugs, fremanezumab and galcanezumab, target CGRP, whereas erenumab targets the CGRP receptor and is also a pioneering approval of a GPCR-targeted mAb. Finally, prior to their approval, the target development level category (Nat. Rev. Drug Discov. 17, 317–332; 2018) for ten of the human proteins was 'Tchem'; that is, knowledge about small molecules modulating these proteins was available in the public domain. Not surprisingly, the three proteins that were classified as 'Tbio' (that is, with well-understood biology but lacking small molecule annotations) were targeted by biologics.},
author = {Ursu, Oleg and Glick, Meir and Oprea, Tudor},
doi = {10.1038/d41573-019-00052-5},
issn = {1474-1776},
journal = {Nat. Rev. Drug Discov.},
month = {mar},
title = {{Novel drug targets in 2018}},
url = {http://www.nature.com/articles/d41573-019-00052-5},
year = {2019}
}
@article{Hamilton2017,
abstract = {Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.},
archivePrefix = {arXiv},
arxivId = {1709.05584},
author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
eprint = {1709.05584},
month = {sep},
title = {{Representation Learning on Graphs: Methods and Applications}},
url = {http://arxiv.org/abs/1709.05584},
year = {2017}
}
@article{Goldstein2017,
abstract = {{\textcopyright} The Author 2016. Objective: Electronic health records (EHRs) are an increasingly common data source for clinical risk prediction, presenting both unique analytic opportunities and challenges. We sought to evaluate the current state of EHR based risk prediction modeling through a systematic review of clinical prediction studies using EHR data. Methods: We searched PubMed for articles that reported on the use of an EHR to develop a risk prediction model from 2009 to 2014. Articles were extracted by two reviewers, and we abstracted information on study design, use of EHR data, model building, and performance from each publication and supplementary documentation. Results: We identified 107 articles from 15 different countries. Studies were generally very large (median sample size1/426 100) and utilized a diverse array of predictors. Most used validation techniques (n1/494 of 107) and reported model coefficients for reproducibility (n1/483). However, studies did not fully leverage the breadth of EHR data, as they uncommonly used longitudinal information (n1/437) and employed relatively few predictor variables (median1/427 variables). Less than half of the studies were multicenter (n1/450) and only 26 performed validation across sites. Many studies did not fully address biases of EHR data such as missing data or loss to follow- up. Average c-statistics for different outcomes were: mortality (0.84), clinical prediction (0.83), hospitalization (0.71), and service utilization (0.71). Conclusions: EHR data present both opportunities and challenges for clinical risk prediction. There is room for improvement in designing such studies.},
author = {Goldstein, Benjamin A and Navar, Ann Marie and Pencina, Michael J and Ioannidis, John P.A.},
doi = {10.1093/jamia/ocw042},
issn = {1527974X},
journal = {J. Am. Med. Informatics Assoc.},
keywords = {Electronic medical record,Review,Risk assessment},
month = {jan},
number = {1},
pages = {198--208},
pmid = {27189013},
title = {{Opportunities and challenges in developing risk prediction models with electronic health records data: A systematic review}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27189013 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5201180 https://academic.oup.com/jamia/article-lookup/doi/10.1093/jamia/ocw042},
volume = {24},
year = {2017}
}
@article{Mordvintsev2020,
author = {Mordvintsev, Alexander and Randazzo, Ettore and Niklasson, Eyvind and Levin, Michael},
doi = {10.23915/distill.00023},
issn = {2476-0757},
journal = {Distill},
month = {feb},
number = {2},
pages = {e23},
title = {{Growing Neural Cellular Automata}},
volume = {5},
year = {2020}
}
@misc{software_altair_dataviz,
title = {{Altair: Declarative Visualization in Python}},
url = {https://altair-viz.github.io/}
}
@article{Heywood2019,
abstract = {Domestic violence and abuse (DVA) is a serious public health issue, threatening the health of individuals the world over. Whilst DVA can be experienced by both men and women, the majority is still experienced by women; around 30{\%} of women worldwide who have been in a relationship report that they have experienced violence at the hands of their partner, and every week in England and Wales two women are killed by their current or ex-partner. The purpose of this study was to explore the concept of thrivership with women who have experienced DVA, to contribute to our understandings of what constitutes ‘thriving' post-abuse, and how women affected can move from surviving to thriving. Thirty-seven women took part in this qualitative study which consisted of six focus groups and four in-depth interviews undertaken in one region of the UK in 2018. Data were analysed using a thematic analysis approach. Initial findings were reported back to a group of participants to invite respondent validation and ensure co-production of data. The process of ‘thrivership' – moving from surviving to thriving after DVA - is a fluid, non-linear journey of self-discovery featuring three ‘stages' of victim, survivor, and thriver. Thriving after DVA is characterised by a positive outlook and looking to the future, improved health and well-being, a reclamation of the self, and a new social network. Crucial to ensuring ‘thrivership' are three key components that we propose as the ‘Thrivership Model', all of which are underpinned by education and awareness building at different levels: (1) Provision of Safety, (2) Sharing the Story, (3) Social Response. The study findings provide a new view of thriving post-abuse by women who have lived through it. The proposed Thrivership Model has been developed to illustrate what is required from DVA-services and public health practitioners for the thrivership process to take place, so that more women may be supported towards ‘thriving' after abuse.},
author = {Heywood, Isobel and Sammut, Dana and Bradbury-Jones, Caroline},
doi = {10.1186/s12905-019-0789-z},
issn = {1472-6874},
journal = {BMC Womens. Health},
keywords = {Gynecology,Maternal and Child Health,Reproductive Medicine},
month = {dec},
number = {1},
pages = {106},
publisher = {BioMed Central},
title = {{A qualitative exploration of ‘thrivership' among women who have experienced domestic violence and abuse: Development of a new model}},
url = {https://bmcwomenshealth.biomedcentral.com/articles/10.1186/s12905-019-0789-z},
volume = {19},
year = {2019}
}
@misc{Winston,
author = {Winston, Patrick},
title = {{Genesis Project}},
url = {http://groups.csail.mit.edu/genesis/trilogy.html},
urldate = {2019-12-02}
}
@article{Amodio2019,
abstract = {Biomedical researchers are generating high-throughput, high-dimensional single-cell data at a staggering rate. As costs of data generation decrease, experimental design is moving towards measurement of many different single-cell samples in the same dataset. These samples can correspond to different patients, conditions, or treatments. While scalability of methods to datasets of these sizes is a challenge on its own, dealing with large-scale experimental design presents a whole new set of problems, including batch effects and sample comparison issues. Currently, there are no computational tools that can both handle large amounts of data in a scalable manner (many cells) and at the same time deal with many samples (many patients or conditions). Moreover, data analysis currently involves the use of different tools that each operate on their own data representation, not guaranteeing a synchronized analysis pipeline. For instance, data visualization methods can be disjoint and mismatched with the clustering method. For this purpose, we present SAUCIE, a deep neural network that leverages the high degree of parallelization and scalability offered by neural networks, as well as the deep representation of data that can be learned by them to perform many single-cell data analysis tasks, all on a unified representation. A well-known limitation of neural networks is their interpretability. Our key contribution here are newly formulated regularizations (penalties) that render features learned in hidden layers of the neural network interpretable. When large multi-patient datasets are fed into SAUCIE, the various hidden layers contain denoised and batch-corrected data, a low dimensional visualization, unsupervised clustering, as well as other information that can be used to explore the data. We show this capability by analyzing a newly generated 180-sample dataset consisting of T cells from dengue patients in India, measured with mass cytometry. We show that SAUCIE, for the first time, can batch correct and process this 11-million cell data to identify cluster-based signatures of acute dengue infection and create a patient manifold, stratifying immune response to dengue on the basis of single-cell measurements.},
author = {Amodio, Matthew and van Dijk, David and Srinivasan, Krishnan and Chen, William S. and Mohsen, Hussein and Moon, Kevin R. and Campbell, Allison and Zhao, Yujiao and Wang, Xiaomei and Venkataswamy, Manjunatha and Desai, Anita and Ravi, V. and Kumar, Priti and Montgomery, Ruth and Wolf, Guy and Krishnaswamy, Smita},
doi = {10.1038/s41592-019-0576-7},
issn = {1548-7091},
journal = {Nat. Methods},
keywords = {Computational biology and bioinformatics,Computational models,Gene expression,Machine learning},
month = {oct},
pages = {1--7},
publisher = {Nature Publishing Group},
title = {{Exploring single-cell data with deep multitasking neural networks}},
url = {http://www.nature.com/articles/s41592-019-0576-7},
year = {2019}
}
@article{Le2019,
abstract = {Technological advances in next-generation sequencing (NGS) and chromatographic assays [e.g., liquid chromatography mass spectrometry (LC-MS)] have made it possible to identify thousands of microbe and metabolite species, and to measure their relative abundance. In this paper, we propose a sparse neural encoder-decoder network to predict metabolite abundances from microbe abundances. Using paired data from a cohort of inflammatory bowel disease (IBD) patients, we show that our neural encoder-decoder model outperforms linear univariate and multivariate methods in terms of accuracy, sparsity, and stability. Importantly, we show that our neural encoder-decoder model is not simply a black box designed to maximize predictive accuracy. Rather, the network's hidden layer (i.e., the latent space, comprised only of sparsely weighted microbe counts) actually captures key microbe-metabolite relationships that are themselves clinically meaningful. Although this hidden layer is learned without any knowledge of the patient's diagnosis, we show that the learned latent features are structured in a way that predicts IBD and treatment status with high accuracy. By imposing a non-negative weights constraint, the network becomes a directed graph where each downstream node is interpretable as the additive combination of the upstream nodes. Here, the middle layer comprises distinct microbe-metabolite axes that relate key microbial biomarkers with metabolite biomarkers. By pre-processing the microbiome and metabolome data using compositional data analysis methods, we ensure that our proposed multi-omics workflow will generalize to any pair of -omics data. To the best of our knowledge, this work is the first application of neural encoder-decoders for the interpretable integration of multi-omics biological data.},
author = {Le, Vuong and Quinn, Thomas P. and Tran, Truyen and Venkatesh, Svetha},
doi = {10.1101/686394},
journal = {bioRxiv},
month = {jun},
pages = {686394},
publisher = {Cold Spring Harbor Laboratory},
title = {{Deep in the Bowel: Highly Interpretable Neural Encoder-Decoder Networks Predict Gut Metabolites from Gut Microbiome}},
url = {https://www.biorxiv.org/content/10.1101/686394v1},
year = {2019}
}
@article{Liu2019b,
abstract = {We introduce graph normalizing flows: a new, reversible graph neural network model for prediction and generation. On supervised tasks, graph normalizing flows perform similarly to message passing neural networks, but at a significantly reduced memory footprint, allowing them to scale to larger graphs. In the unsupervised case, we combine graph normalizing flows with a novel graph auto-encoder to create a generative model of graph structures. Our model is permutation-invariant, generating entire graphs with a single feed-forward pass, and achieves competitive results with the state-of-the art auto-regressive models, while being better suited to parallel computing architectures.},
archivePrefix = {arXiv},
arxivId = {1905.13177},
author = {Liu, Jenny and Kumar, Aviral and Ba, Jimmy and Kiros, Jamie and Swersky, Kevin},
eprint = {1905.13177},
month = {may},
title = {{Graph Normalizing Flows}},
url = {http://arxiv.org/abs/1905.13177},
year = {2019}
}
@misc{ONS_death_data_table,
author = {ONS},
title = {{Death registrations summary tables - England and Wales - Office for National Statistics}},
url = {https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/deaths/datasets/deathregistrationssummarytablesenglandandwalesreferencetables},
urldate = {2019-09-24}
}
@article{Payne2020,
abstract = {Background: Health services have failed to respond to the pressures of multimorbidity. Improved measures of multimorbidity are needed for conducting research, planning services and allocating resources. Methods: We modelled the association between 37 morbidities and 3 key outcomes (primary care consultations, unplanned hospital admission, death) at 1 and 5 years. We extracted development (n = 300 000) and validation (n = 150 000) samples from the UK Clinical Practice Research Datalink. We constructed a general-outcome multimorbidity score by averaging the standardized weights of the separate outcome scores. We compared performance with the Charlson Comorbidity Index. RESULTS: Models that included all 37 conditions were acceptable predictors of general practitioner consultations (C-index 0.732, 95{\%} confidence interval [CI] 0.731-0.734), unplanned hospital admission (C-index 0.742, 95{\%} CI 0.737-0.747) and death at 1 year (C-index 0.912, 95{\%} CI 0.905-0.918). Models reduced to the 20 conditions with the greatest combined prevalence/weight showed similar predictive ability (C-indices 0.727, 95{\%} CI 0.725-0.728; 0.738, 95{\%} CI 0.732-0.743; and 0.910, 95{\%} CI 0.904-0.917, respectively). They also predicted 5-year outcomes similarly for consultations and death (C-indices 0.735, 95{\%} CI 0.734-0.736, and 0.889, 95{\%} CI 0.885-0.892, respectively) but performed less well for admissions (C-index 0.708, 95{\%} CI 0.705-0.712). The performance of the generaloutcome score was similar to that of the outcome-specific models. These models performed significantly better than those based on the Charlson Comorbidity Index for consultations (C-index 0.691, 95{\%} CI 0.690-0.693) and admissions (C-index 0.703, 95{\%} CI 0.697-0.709) and similarly for mortality (C-index 0.907, 95{\%} CI 0.900-0.914). INTERPRETATION: The Cambridge Multimorbidity Score is robust and can be either tailored or not tailored to specific health outcomes. It will be valuable to those planning clinical services, policymakers allocating resources and researchers seeking to account for the effect of multimorbidity.},
author = {Payne, Rupert A. and Mendonca, Silvia C. and Elliott, Marc N. and Saunders, Catherine L. and Edwards, Duncan A. and Marshall, Martin and Roland, Martin},
doi = {10.1503/cmaj.190757},
issn = {14882329},
journal = {CMAJ},
month = {feb},
number = {5},
pages = {E107--E114},
publisher = {Canadian Medical Association},
title = {{Development and validation of the Cambridge Multimorbidity Score}},
volume = {192},
year = {2020}
}
@misc{nicbidwell_connectionnetworks,
title = {{Connectivity Strategies Where People Matter: Community-led small-scale telecommunication infrastructure networks in the global South | Association for Progressive Communications}},
url = {https://www.apc.org/en/node/35445/},
urldate = {2019-05-06}
}
@article{Mi2019,
abstract = {A simultaneous improvement in both ecological and economic efficiency is necessary to achieve the Sustainable Development Goals (SDGs). The new sharing economy has potential to promote the needed shifts in collective consumption behaviour, but better governance models are urgently required.},
author = {Mi, Zhifu and Coffman, D'Maris M.},
doi = {10.1038/s41467-019-09260-4},
issn = {20411723},
journal = {Nat. Commun.},
keywords = {Policy,Society},
month = {dec},
number = {1},
pages = {1214},
publisher = {Nature Publishing Group},
title = {{The sharing economy promotes sustainable societies}},
url = {http://www.nature.com/articles/s41467-019-09260-4},
volume = {10},
year = {2019}
}
@article{Esteva2017,
abstract = {Skin cancer, the most common human malignancy, is primarily diagnosed visually, beginning with an initial clinical screening and followed potentially by dermoscopic analysis, a biopsy and histopathological examination. Automated classification of skin lesions using images is a challenging task owing to the fine-grained variability in the appearance of skin lesions. Deep convolutional neural networks (CNNs) show potential for general and highly variable tasks across many fine-grained object categories. Here we demonstrate classification of skin lesions using a single CNN, trained end-to-end from images directly, using only pixels and disease labels as inputs. We train a CNN using a dataset of 129,450 clinical images-two orders of magnitude larger than previous datasets-consisting of 2,032 different diseases. We test its performance against 21 board-certified dermatologists on biopsy-proven clinical images with two critical binary classification use cases: keratinocyte carcinomas versus benign seborrheic keratoses; and malignant melanomas versus benign nevi. The first case represents the identification of the most common cancers, the second represents the identification of the deadliest skin cancer. The CNN achieves performance on par with all tested experts across both tasks, demonstrating an artificial intelligence capable of classifying skin cancer with a level of competence comparable to dermatologists. Outfitted with deep neural networks, mobile devices can potentially extend the reach of dermatologists outside of the clinic. It is projected that 6.3 billion smartphone subscriptions will exist by the year 2021 (ref. 13) and can therefore potentially provide low-cost universal access to vital diagnostic care.},
author = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A. and Ko, Justin and Swetter, Susan M. and Blau, Helen M. and Thrun, Sebastian},
doi = {10.1038/nature21056},
issn = {1476-4687},
journal = {Nature},
keywords = {Diagnosis,Machine learning,Skin cancer},
month = {feb},
number = {7639},
pages = {115--118},
pmid = {28117445},
publisher = {Nature Publishing Group},
title = {{Dermatologist-level classification of skin cancer with deep neural networks.}},
url = {http://www.nature.com/articles/nature21056 http://www.ncbi.nlm.nih.gov/pubmed/28117445},
volume = {542},
year = {2017}
}
@article{Madiraju2018a,
abstract = {Unsupervised learning of time series data, also known as temporal clustering, is a challenging problem in machine learning. Here we propose a novel algorithm, Deep Temporal Clustering (DTC), to naturally integrate dimensionality reduction and temporal clustering into a single end-to-end learning framework, fully unsupervised. The algorithm utilizes an autoencoder for temporal dimensionality reduction and a novel temporal clustering layer for cluster assignment. Then it jointly optimizes the clustering objective and the dimensionality reduction objec tive. Based on requirement and application, the temporal clustering layer can be customized with any temporal similarity metric. Several similarity metrics and state-of-the-art algorithms are considered and compared. To gain insight into temporal features that the network has learned for its clustering, we apply a visualization method that generates a region of interest heatmap for the time series. The viability of the algorithm is demonstrated using time series data from diverse domains, ranging from earthquakes to spacecraft sensor data. In each case, we show that the proposed algorithm outperforms traditional methods. The superior performance is attributed to the fully integrated temporal dimensionality reduction and clustering criterion.},
archivePrefix = {arXiv},
arxivId = {1802.01059},
author = {Madiraju, Naveen Sai and Sadat, Seid M. and Fisher, Dimitry and Karimabadi, Homa},
eprint = {1802.01059},
month = {feb},
title = {{Deep Temporal Clustering : Fully Unsupervised Learning of Time-Domain Features}},
url = {http://arxiv.org/abs/1802.01059},
year = {2018}
}
@article{Clark,
author = {Clark and Chalmers, David},
doi = {10.1145/298972.298994},
issn = {00952737},
title = {{The extended mind}},
url = {https://www.jstor.org/stable/3328150?seq=1{\#}metadata{\_}info{\_}tab{\_}contents}
}
@article{Taylor2019,
abstract = {Modern medicine requires generalised approaches to the synthesis and integration of multimodal data, often at different biological scales, that can be applied to a variety of evidence structures, such as complex disease analyses and epidemiological models. However, current methods are either slow and expensive, or ineffective due to the inability to model the complex relationships between data modes which differ in scale and format. We address these issues by proposing a cross-modal deep learning architecture and co-attention mechanism to accurately model the relationships between the different data modes, while further reducing patient diagnosis time. Differentiating Parkinson's Disease (PD) patients from healthy patients forms the basis of the evaluation. The model outperforms the previous state-of-the-art unimodal analysis by 2.35{\%}, while also being 53{\%} more parameter efficient than the industry standard cross-modal model. Furthermore, the evaluation of the attention coefficients allows for qualitative insights to be obtained. Through the coupling with bioinformatics, a novel link between the interferon-gamma-mediated pathway, DNA methylation and PD was identified. We believe that our approach is general and could optimise the process of medical evidence synthesis and decision making in an actionable way.},
archivePrefix = {arXiv},
arxivId = {1909.06442},
author = {Taylor, Devin and Spasov, Simeon and Li{\`{o}}, Pietro},
eprint = {1909.06442},
month = {sep},
title = {{Co-Attentive Cross-Modal Deep Learning for Medical Evidence Synthesis and Decision Making}},
url = {http://arxiv.org/abs/1909.06442},
year = {2019}
}
@misc{Barendregt2014,
author = {Barendregt, J. and {World Health Organization}},
booktitle = {Heal. Stat. Inf. Syst.},
keywords = {Adult mortality,Burden of disease,Child mortality,Death,Death rate,Diabetes mellitus,European Region [region],Fatal outcome,GBD,Global Burden of Disease,Glucose intolerance,Hyperglycaemia,Neonate mortality,Netherlands [country],diabetes [subject],mortality [subject]},
publisher = {World Health Organization},
title = {{WHO | Software tools}},
url = {https://www.who.int/healthinfo/global{\_}burden{\_}disease/tools{\_}software/en/},
urldate = {2019-11-28},
year = {2014}
}
@article{Hoffman2014,
abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size {\{}$\backslash$epsilon{\}} and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS perform at least as efficiently as and sometimes more efficiently than a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter {\{}$\backslash$epsilon{\}} on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all. NUTS is also suitable for applications such as BUGS-style automatic inference engines that require efficient "turnkey" sampling algorithms.},
archivePrefix = {arXiv},
arxivId = {1111.4246},
author = {Hoffman, Matthew D. and Gelman, Andrew},
doi = {10.1190/1.3627885},
eprint = {1111.4246},
isbn = {1532-4435},
issn = {15337928},
journal = {J. Mach. Learn. Res.},
pages = {1593--1623},
pmid = {1107811},
title = {{The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo}},
url = {http://www.jmlr.org/papers/v15/hoffman14a.html http://arxiv.org/abs/1111.4246},
volume = {15},
year = {2011}
}
@misc{mit_brain_course,
title = {{Brains, Minds and Machines Summer Course | MIT OpenCourseWare}},
url = {https://ocw.mit.edu/resources/res-9-003-brains-minds-and-machines-summer-course-summer-2015/},
urldate = {2019-12-31}
}
@inproceedings{Matejka2017,
abstract = {Figure 1. A collection of data sets produced by our technique. While different in appearance, each has the same summary statistics (mean, std. deviation, and Pearson's corr.) to 2 decimal places. (x ͞ =54.02, y ͞ = 48.09, sdx = 14.52, sdy = 24.79, Pearson's r = +0.32) ABSTRACT Datasets which are identical over a number of statistical properties, yet produce dissimilar graphs, are frequently used to illustrate the importance of graphical representations when exploring data. This paper presents a novel method for generating such datasets, along with several examples. Our technique varies from previous approaches in that new datasets are iteratively generated from a seed dataset through random perturbations of individual data points, and can be directed towards a desired outcome through a simulated annealing optimization strategy. Our method has the benefit of being agnostic to the particular statistical properties that are to remain constant between the datasets, and allows for control over the graphical appearance of resulting output.},
address = {New York, New York, USA},
author = {Matejka, Justin and Fitzmaurice, George},
booktitle = {Conf. Hum. Factors Comput. Syst. - Proc.},
doi = {10.1145/3025453.3025912},
isbn = {9781450346559},
keywords = {anscombe,scatter plots,visualization},
pages = {1290--1294},
publisher = {ACM Press},
title = {{Same stats, different graphs: Generating datasets with varied appearance and identical statistics through simulated annealing}},
url = {http://dl.acm.org/citation.cfm?doid=3025453.3025912},
volume = {2017-May},
year = {2017}
}
@article{Li2016,
abstract = {We present persona-based models for handling the issue of speaker consistency in neural response generation. A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style. A dyadic speaker-addressee model captures properties of interactions between two interlocutors. Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models, with similar gains in speaker consistency as measured by human judges.},
archivePrefix = {arXiv},
arxivId = {1603.06155},
author = {Li, Jiwei and Galley, Michel and Brockett, Chris and Spithourakis, Georgios P. and Gao, Jianfeng and Dolan, Bill},
doi = {0.18653/v1/P16-1094},
eprint = {1603.06155},
isbn = {9781510827585},
month = {mar},
title = {{A Persona-Based Neural Conversation Model}},
url = {http://arxiv.org/abs/1603.06155},
year = {2016}
}
@article{Vallejos2019,
abstract = {PHATE enhances the visualization of high-dimensional data.},
author = {Vallejos, Catalina A.},
doi = {10.1038/s41587-019-0330-9},
issn = {1087-0156},
journal = {Nat. Biotechnol.},
keywords = {Bioinformatics,Data mining,Statistical methods},
month = {dec},
number = {12},
pages = {1423--1424},
publisher = {Nature Publishing Group},
title = {{Exploring a world of a thousand dimensions}},
url = {http://www.nature.com/articles/s41587-019-0330-9},
volume = {37},
year = {2019}
}
@article{Brodersen2015a,
abstract = {An important problem in econometrics and marketing is to infer the causal impact that a designed market intervention has exerted on an outcome metric over time. This paper proposes to infer causal impact on the basis of a diffusion-regression state-space model that predicts the counterfactual market response in a synthetic control that would have occurred had no intervention taken place. In contrast to classical difference-in-differences schemes, state-space models make it possible to (i) infer the temporal evolution of attributable impact, (ii) incorporate empirical priors on the parameters in a fully Bayesian treatment, and (iii) flexibly accommodate multiple sources of variation, including local trends, seasonality and the time-varying influence of contemporaneous covariates. Using a Markov chain Monte Carlo algorithm for posterior inference, we illustrate the statistical properties of our approach on simulated data. We then demonstrate its practical utility by estimating the causal effect of an online advertising campaign on search-related site visits. We discuss the strengths and limitations of state-space models in enabling causal attribution in those settings where a randomised experiment is unavailable. The CausalImpact R package provides an implementation of our approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.00356v1},
author = {Brodersen, Kay H. and Gallusser, Fabian and Koehler, Jim and Remy, Nicolas and Scott, Steven L.},
doi = {10.1214/14-AOAS788},
eprint = {arXiv:1506.00356v1},
isbn = {0885-3185},
issn = {19417330},
journal = {Ann. Appl. Stat.},
keywords = {Advertising,Causal inference,Counterfactual,Difference in differences,Econometrics,Market research,Observational,Synthetic control},
month = {mar},
number = {1},
pages = {247--274},
publisher = {Institute of Mathematical Statistics},
title = {{Inferring causal impact using bayesian structural time-series models}},
url = {http://projecteuclid.org/euclid.aoas/1430226092},
volume = {9},
year = {2015}
}
@article{Bai2018,
abstract = {There has been an increasing interest in learning low-dimensional vector representations of medical concepts from Electronic Health Records (EHRs). Vector representations of medical concepts facilitate exploratory analysis and predictive modeling of EHR data to gain insights about the patterns of care and health outcomes. EHRs contain structured data such as diagnostic codes and laboratory tests, as well as unstructured free text data in form of clinical notes, which provide more detail about condition and treatment of patients. In this work, we propose a method that jointly learns vector representations of medical concepts and words. This is achieved by a novel learning scheme based on the word2vec model. Our model learns those relationships by integrating clinical notes and sets of accompanying medical codes and by defining joint contexts for each observed word and medical code. In our experiments, we learned joint representations using MIMIC-III data. Using the learned representations of words and medical codes, we evaluated phenotypes for 6 diseases discovered by our and baseline method. The experimental results show that for each of the 6 diseases our method finds highly relevant words. We also show that our representations can be very useful when predicting the reason for the next visit. The jointly learned representations of medical concepts and words capture not only similarity between codes or words themselves, but also similarity between codes and words. They can be used to extract phenotypes of different diseases. The representations learned by the joint model are also useful for construction of patient features.},
author = {Bai, Tian and Chanda, Ashis Kumar and Egleston, Brian L. and Vucetic, Slobodan},
doi = {10.1186/s12911-018-0672-0},
issn = {14726947},
journal = {BMC Med. Inform. Decis. Mak.},
keywords = {Distributed representation,Electronic health records,Healthcare,Natural language processing},
month = {dec},
number = {S4},
pages = {123},
publisher = {BioMed Central},
title = {{EHR phenotyping via jointly embedding medical concepts and words into a unified vector space}},
url = {https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-018-0672-0},
volume = {18},
year = {2018}
}
@article{Bender2005,
abstract = {Simulation studies present an important statistical tool to investigate the performance, properties and adequacy of statistical models in pre-specified situations. One of the most important statistical models in medical research is the proportional hazards model of Cox. In this paper, techniques to generate survival times for simulation studies regarding Cox proportional hazards models are presented. A general formula describing the relation between the hazard and the corresponding survival time of the Cox model is derived, which is useful in simulation studies. It is shown how the exponential, the Weibull and the Gompertz distribution can be applied to generate appropriate survival times for simulation studies. Additionally, the general relation between hazard and survival time can be used to develop own distributions for special situations and to handle flexibly parameterized proportional hazards models. The use of distributions other than the exponential distribution is indispensable to investigate the characteristics of the Cox proportional hazards model, especially in non-standard situations, where the partial likelihood depends on the baseline hazard. A simulation study investigating the effect of measurement errors in the German Uranium Miners Cohort Study is considered to illustrate the proposed simulation techniques and to emphasize the importance of a careful modelling of the baseline hazard in Cox models.},
author = {Bender, Ralf and Augustin, Thomas and Blettner, Maria},
doi = {10.1002/sim.2059},
issn = {02776715},
journal = {Stat. Med.},
keywords = {Cox model,Exponential distribution,Gompertz distribution,Proportional hazards model,Simulation,Survival times,Weibull distribution},
month = {jun},
number = {11},
pages = {1713--1723},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Generating survival times to simulate Cox proportional hazards models}},
url = {http://doi.wiley.com/10.1002/sim.2059},
volume = {24},
year = {2005}
}
@inproceedings{Sabour2017,
abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
archivePrefix = {arXiv},
arxivId = {1710.09829},
author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
booktitle = {Adv. Neural Inf. Process. Syst.},
eprint = {1710.09829},
issn = {10495258},
month = {oct},
pages = {3857--3867},
title = {{Dynamic routing between capsules}},
url = {http://arxiv.org/abs/1710.09829},
volume = {2017-Decem},
year = {2017}
}
@article{Wood1989,
abstract = {1. We have given an oral load of lithium carbonate to healthy volunteers in order to investigate the transport of lithium across the erythrocyte membrane in vivo and the effects of known inhibitors of that transport. 2. Using this technique we have shown that pretreatment with either digoxin, an inhibitor of the sodium/potassium pump, or dipyridamole, an inhibitor of the anion transporter, does not alter the plasma or erythrocyte lithium concentration profiles, nor any of the pharmacokinetic variables derived from these data, and we conclude that these two transport pathways do not contribute significantly to the in vivo handling of lithium by erythrocytes. 3. We have also shown that erythrocyte lithium concentrations measured directly differ significantly from the predicted concentrations calculated using the two-compartment pharmacokinetic model which has been used in some earlier comparisons of in vitro and in vivo lithium handling. 4. We suggest that the in vivo administration of lithium carbonate may permit a specific measure of the in vivo activity of the sodium/sodium countertransport pathway.},
author = {Wood, AJ and Aronson, JK and Bunch, C and Grahame‐Smith, DG},
doi = {10.1111/j.1365-2125.1989.tb03436.x},
issn = {13652125},
journal = {Br. J. Clin. Pharmacol.},
month = {jun},
number = {6},
pages = {749--756},
pmid = {2757891},
title = {{A study of the transport of lithium across the erythrocyte membrane in vivo and of the effects of the ion transport inhibitors digoxin and dipyridamole.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/2757891 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC1379801 http://doi.wiley.com/10.1111/j.1365-2125.1989.tb03436.x},
volume = {27},
year = {1989}
}
@misc{Benton2007,
abstract = {BACKGROUND: Depression is much more prevalent among those with chronic medical conditions compared to the general population of the United States. Depression is recognized as a cause of increased morbidity and mortality and has been associated with higher health care costs, adverse health behaviors, significant functional impairment, lost work productivity, occupational disability and increased health care utilization.$\backslash$n$\backslash$nMETHOD: Searches of Medline, OVIDMedline, PubMed and PsycINFO of all English-language articles published between 1966 and 2007 were conducted using the keywords mood disorders, medical comorbidity, depression, antidepressant therapy. Supplemental references were manually extracted from relevant articles and chapters. Reviews of mechanistic studies and open label and randomized controlled trials of depression in patients with medical co morbidities were reviewed.$\backslash$n$\backslash$nRESULTS: Depressive disorders are prevalent among the medically ill and the relationship between depression and medical illness may be bidirectional. Antidepressant medications are effective in the treatment of depression in the medically ill.$\backslash$n$\backslash$nCONCLUSIONS: Depressive disorders can adversely impact the course of medical illnesses. Available antidepressant treatments are effective for the treatment of depression in the medically ill. Early identification and treatment of depression in medical illness can positively influence medical outcomes and quality of life.},
author = {Benton, Tami and Staab, Jeffrey and Evans, Dwight L.},
booktitle = {Ann. Clin. Psychiatry},
doi = {10.1080/10401230701653542},
issn = {10401237},
keywords = {Depression,Medical comorbidity,Mood disorders},
month = {oct},
number = {4},
pages = {289--303},
title = {{Medical co-morbidity in depressive disorders}},
url = {http://www.portico.org/Portico/article?article=pf1m9kf754},
volume = {19},
year = {2007}
}
@article{Kuznetsova2017,
abstract = {One of the frequent questions by users of the mixed model function lmer of the lme4 package has been: How can I get p values for the F and t tests for objects returned by lmer? The lmerTest package extends the 'lmerMod' class of the lme4 package, by overloading the anova and summary functions by providing p values for tests for fixed effects. We have implemented the Satterthwaite's method for approximating degrees of freedom for the t and F tests. We have also implemented the construction of Type I - III ANOVA tables. Furthermore, one may also obtain the summary as well as the anova table using the Kenward-Roger approximation for denominator degrees of freedom (based on the KRmodcomp function from the pbkrtest package). Some other convenient mixed model analysis tools such as a step method, that performs backward elimination of nonsignificant effects - both random and fixed, calculation of population means and multiple comparison tests together with plot facilities are provided by the package as well.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kuznetsova, Alexandra and Brockhoff, Per B. and Christensen, Rune H. B.},
doi = {10.18637/jss.v082.i13},
eprint = {arXiv:1011.1669v3},
isbn = {0014-3820},
issn = {1548-7660},
journal = {J. Stat. Softw.},
month = {dec},
number = {13},
pages = {1--26},
pmid = {17492965},
title = {{lmerTest Package: Tests in Linear Mixed Effects Models}},
url = {http://www.jstatsoft.org/v82/i13/},
volume = {82},
year = {2017}
}
@article{Greef2019,
abstract = {The human naive T-cell receptor (TCR) repertoire is extremely diverse and accurately estimating its distribution is challenging. We address this challenge by combining a quantitative sequencing protocol of TCRA and TCRB sequences with computational modelling. We observed the vast majority of TCR chains only once in our samples, confirming the enormous diversity of the naive repertoire. However, a substantial number of sequences were observed multiple times within samples, and we demonstrated that this is due to expression by many cells in the naive pool. We reason that $\alpha$ and $\beta$ chains are frequently observed due to a combination of selective processes and summation over multiple clones expressing these chains. We test the contribution of both mechanisms by predicting samples from phenomenological and mechanistically modelled repertoire distributions. By comparing these with sequencing data, we show that frequently observed chains are likely to be derived from multiple clones. Still, a neutral model of T-cell homeostasis cannot account for the observed distributions. We conclude that the data are only compatible with distributions of many small clones in combination with a sufficient number of very large naive T-cell clones, the latter most likely as a result of peripheral selection.},
author = {de Greef, Peter C. and Oakes, Theres and Gerritsen, Bram and Ismail, Mazlina and James, M and Hermsen, Rutger and Chain, Benjamin and de Boer, Rob J.},
doi = {10.1101/691501},
journal = {bioRxiv},
month = {jul},
number = {D},
pages = {691501},
publisher = {Cold Spring Harbor Laboratory},
title = {{The naive T-cell receptor repertoire has an extremely broad distribution of clone sizes}},
url = {https://www.biorxiv.org/content/10.1101/691501v1?ct=},
year = {2019}
}
@inproceedings{Salakhutdinov2007,
abstract = {Most of the existing approaches to collab- orative ltering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical mod- els, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present ecient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Net ix data set, containing over 100 mil- lion user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of mul- tiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6{\%} better than the score of Net ix's own system},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {1606.07129},
author = {Salakhutdinov, Ruslan and Mnih, Andriy and Hinton, Geoffrey},
booktitle = {Proc. 24th Int. Conf. Mach. Learn. - ICML '07},
doi = {10.1145/1273496.1273596},
eprint = {1606.07129},
isbn = {9781595937933},
issn = {1468-1218},
pages = {791--798},
pmid = {19932002},
publisher = {ACM Press},
title = {{Restricted Boltzmann machines for collaborative filtering}},
url = {http://portal.acm.org/citation.cfm?doid=1273496.1273596},
year = {2008}
}
@article{Kartsonaki2016,
abstract = {Survival analysis is the analysis of data involving times to some event of interest. The distinguishing features of survival, or time-to-event, data and the objectives of survival analysis are described. Some fundamental concepts of survival analysis are introduced and commonly used methods of analysis are described.},
archivePrefix = {arXiv},
arxivId = {arXiv:1305.6995v1},
author = {Kartsonaki, Christiana},
doi = {10.1016/j.mpdhp.2016.06.005},
eprint = {arXiv:1305.6995v1},
isbn = {9780387244471},
issn = {18767621},
journal = {Diagnostic Histopathol.},
keywords = {Cox proportional hazards model,Kaplan–Meier curve,failure times,hazard,survival data,time-to-event data},
month = {jul},
number = {7},
pages = {263--270},
pmid = {8875548},
publisher = {Elsevier},
title = {{Survival analysis}},
url = {https://www.sciencedirect.com/science/article/pii/S1756231716300639},
volume = {22},
year = {2016}
}
@article{Vancamelbeke2017,
abstract = {Background: Intestinal barrier defects are common in patients with inflammatory bowel disease (IBD). To identify which components could underlie these changes, we performed an in-depth analysis of epithelial barrier genes in IBD. Methods: A set of 128 intestinal barrier genes was selected. Polygenic risk scores were generated based on selected barrier gene variants that were associated with Crohn's disease (CD) or ulcerative colitis (UC) in our study. Gene expression was analyzed using microarray and quantitative reverse transcription polymerase chain reaction. Influence of barrier gene variants on expression was studied by cis-expression quantitative trait loci mapping and comparing patients with low- and high-risk scores. Results: Barrier risk scores were significantly higher in patients with IBD than controls. At single-gene level, the associated barrier single-nucleotide polymorphisms were most significantly enriched in PTGER4 for CD and HNF4A for UC. As a group, the regulating proteins were most enriched for CD and UC. Expression analysis showed that many epithelial barrier genes were significantly dysregulated in active CD and UC, with overrepresentation of mucus layer genes. In uninflamed CD ileum and IBD colon, most barrier gene levels restored to normal, except for MUC1 and MUC4 that remained persistently increased compared with controls. Expression levels did not depend on cis-regulatory variants nor combined genetic risk. Conclusions: We found genetic and transcriptomic dysregulations of key epithelial barrier genes and components in IBD. Of these, we believe that mucus genes, in particular MUC1 and MUC4, play an essential role in the pathogenesis of IBD and could represent interesting targets for treatment.},
author = {Vancamelbeke, Maaike and Vanuytsel, Tim and Farr{\'{e}}, Ricard and Verstockt, Sare and Ferrante, Marc and {Van Assche}, Gert and Rutgeerts, Paul and Schuit, Frans and Vermeire, S{\'{e}}verine and Arijs, Ingrid and Cleynen, Isabelle},
doi = {10.1097/MIB.0000000000001246},
isbn = {0000000000},
issn = {15364844},
journal = {Inflamm. Bowel Dis.},
keywords = {genetic analysis,inflammatory bowel disease,intestinal barrier,mucosal gene expression},
month = {oct},
number = {10},
pages = {1718--1729},
pmid = {28885228},
title = {{Genetic and Transcriptomic Bases of Intestinal Epithelial Barrier Dysfunction in Inflammatory Bowel Disease}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28885228 http://insights.ovid.com/crossref?an=00054725-201710000-00007},
volume = {23},
year = {2017}
}
@article{Wootton2018,
abstract = {Background Smoking prevalence is higher amongst individuals with schizophrenia and depression compared to the general population. Mendelian randomisation (MR) can examine whether this association is causal using genetic variants identified in genome-wide association studies (GWAS). Methods We conducted a GWAS of lifetime smoking behaviour (capturing smoking duration, heaviness and cessation) in a sample of 462,690 individuals from the UK Biobank, and validated the findings via two-sample MR analyses of positive control outcomes (e.g., lung cancer). Having established the validity of our instrument, we used bi-directional two-sample Mendelian randomisation to explore its effects on schizophrenia and depression. Outcomes There was strong evidence to suggest smoking is a causal risk factor for both schizophrenia (OR = 2.27, 95{\%} CI = 1.67 - 3.08, P {\textless} 0.001) and depression (OR = 1.99, 95{\%} CI = 1.71 - 2.32, P {\textless} 0.001). We also found some evidence that genetic risk for both schizophrenia and depression cause increased lifetime smoking ($\beta$ = 0.022, 95{\%} CI = 0.005 - 0.038, P = 0.009; $\beta$= 0.091, 95{\%} CI = 0.027 - 0.155, P = 0.005). Interpretation These findings suggest that the association between smoking, schizophrenia and depression is due, at least in part, to a causal effect of smoking, providing further evidence for the detrimental consequences of smoking for mental health. Funding This work was supported by the Medical Research Council Integrative Epidemiology Unit, the NIHR Biomedical Research Centre, University Hospitals Bristol NHS Foundation Trust and the University of Bristol. Evidence before this study The association between smoking and mental health (especially schizophrenia and depression) is often assumed to be the result of self-medication (for example, to alleviate symptoms). However, more recent evidence has suggested that smoking might also be a risk factor for schizophrenia and depression. This alternative direction of effect is supported by meta-analyses and previous prospective observational evidence using related individuals to control for genetic and environmental confounding. However, observational evidence cannot completely account for confounding or the possibility of reverse causation. One way to get around these problems is Mendelian randomisation (MR). Previous MR studies of smoking and mental health have not shown an effect of smoking on depression and are inconclusive for the effects of smoking on schizophrenia. However, these studies have only looked at individual aspects of smoking behaviour and some studies required stratifying participants into smokers and non-smokers, reducing power. Added value of this study We have developed a novel genetic instrument for lifetime smoking exposure which can be used within a two-sample MR framework, using publicly-available GWAS summary statistics. We were therefore able to test the bi-directional association between smoking with schizophrenia and depression to see if the effects are causal. We found strong evidence to suggest that smoking is a causal risk factor for both schizophrenia and depression. There was some evidence to suggest that risk of schizophrenia and depression increases lifetime smoking (consistent with the self-medication hypothesis) but the effects were stronger for depression than schizophrenia. Implications of all the available evidence This study was the first to demonstrate evidence for an effect of lifetime smoking exposure on risk of schizophrenia and depression within a causal inference framework. This emphasises the detrimental public health consequences of smoking, not just for physical health, but also to mental illness.},
author = {Wootton, Robyn E. and Richmond, Rebecca C. and Stuijfzand, Bobby G. and Lawn, Rebecca B. and Sallis, Hannah M. and Taylor, Gemma M. J. and Hemani, Gibran and Jones, Hannah J. and Zammit, Stanley and Smith, George Davey and Munaf{\`{o}}, Marcus R.},
doi = {10.1101/381301},
issn = {0033-2917},
journal = {bioRxiv},
keywords = {ALSPAC,Mendelian randomisation,UK Biobank,depression,schizophrenia,smoking,tobacco},
month = {nov},
pages = {381301},
publisher = {Cambridge University Press},
title = {{Causal effects of lifetime smoking on risk for depression and schizophrenia: Evidence from a Mendelian randomisation study}},
url = {https://www.cambridge.org/core/product/identifier/S0033291719002678/type/journal{\_}article https://www.biorxiv.org/content/10.1101/381301v2},
year = {2018}
}
@article{Mallick2019,
abstract = {Microbial community metabolomics, particularly in the human gut, are beginning to provide a new route to identify functions and ecology disrupted in disease. However, these data can be costly and difficult to obtain at scale, while amplicon or shotgun metagenomic sequencing data are readily available for populations of many thousands. Here, we describe a computational approach to predict potentially unobserved metabolites in new microbial communities, given a model trained on paired metabolomes and metagenomes from the environment of interest. Focusing on two independent human gut microbiome datasets, we demonstrate that our framework successfully recovers community metabolic trends for more than 50{\%} of associated metabolites. Similar accuracy is maintained using amplicon profiles of coral-associated, murine gut, and human vaginal microbiomes. We also provide an expected performance score to guide application of the model in new samples. Our results thus demonstrate that this ‘predictive metabolomic' approach can aid in experimental design and provide useful insights into the thousands of community profiles for which only metagenomes are currently available.},
author = {Mallick, Himel and Franzosa, Eric A. and Mclver, Lauren J. and Banerjee, Soumya and Sirota-Madi, Alexandra and Kostic, Aleksandar D. and Clish, Clary B. and Vlamakis, Hera and Xavier, Ramnik J. and Huttenhower, Curtis},
doi = {10.1038/s41467-019-10927-1},
issn = {2041-1723},
journal = {Nat. Commun.},
keywords = {Machine learning,Metabolomics,Metagenomics,Microbiome,Statistical methods},
month = {dec},
number = {1},
pages = {3136},
publisher = {Nature Publishing Group},
title = {{Predictive metabolomic profiling of microbial communities using amplicon or metagenomic sequences}},
url = {http://www.nature.com/articles/s41467-019-10927-1},
volume = {10},
year = {2019}
}
@article{Sanchez-Gonzalez2019,
abstract = {We introduce an approach for imposing physically informed inductive biases in learned simulation models. We combine graph networks with a differentiable ordinary differential equation integrator as a mechanism for predicting future states, and a Hamiltonian as an internal representation. We find that our approach outperforms baselines without these biases in terms of predictive accuracy, energy accuracy, and zero-shot generalization to time-step sizes and integrator orders not experienced during training. This advances the state-of-the-art of learned simulation, and in principle is applicable beyond physical domains.},
archivePrefix = {arXiv},
arxivId = {1909.12790},
author = {Sanchez-Gonzalez, Alvaro and Bapst, Victor and Cranmer, Kyle and Battaglia, Peter},
eprint = {1909.12790},
month = {sep},
title = {{Hamiltonian Graph Networks with ODE Integrators}},
url = {http://arxiv.org/abs/1909.12790},
year = {2019}
}
@misc{meta_analysis_website,
abstract = {http://www.wvbauer.com/doku.php},
title = {{Homepage [Wolfgang Viechtbauer] meta analysis}},
url = {http://www.wvbauer.com/doku.php},
urldate = {2017-11-16}
}
@article{Moeyersoms2016,
abstract = {Predictive modeling applications increasingly use data representing people's behavior, opinions, and interactions. Fine-grained behavior data often has different structure from traditional data, being very high-dimensional and sparse. Models built from these data are quite difficult to interpret, since they contain many thousands or even many millions of features. Listing features with large model coefficients is not sufficient, because the model coefficients do not incorporate information on feature presence, which is key when analysing sparse data. In this paper we introduce two alternatives for explaining predictive models by listing important features. We evaluate these alternatives in terms of explanation "bang for the buck,", i.e., how many examples' inferences are explained for a given number of features listed. The bottom line: (i) The proposed alternatives have double the bang-for-the-buck as compared to just listing the high-coefficient features, and (ii) interestingly, although they come from different sources and motivations, the two new alternatives provide strikingly similar rankings of important features.},
archivePrefix = {arXiv},
arxivId = {1607.06280},
author = {Moeyersoms, Julie and D'Alessandro, Brian and Provost, Foster and Martens, David},
eprint = {1607.06280},
journal = {ICML Work. Hum. Interpret. Mach. Learn.},
month = {jul},
number = {Whi},
title = {{Explaining Classification Models Built on High-Dimensional Sparse Data}},
url = {http://arxiv.org/abs/1607.06280},
year = {2016}
}
@article{Dimitri2017,
abstract = {Background Identification of underlying mechanisms behind drugs side effects is of extreme interest and importance in drugs discovery today. Therefore machine learning methodology, linking such different multi features aspects and able to make predictions, are crucial for understanding side effects. Methods In this paper we present DrugClust, a machine learning algorithm for drugs side effects prediction. DrugClust pipeline works as follows: first drugs are clustered with respect to their features and then side effects predictions are made, according to Bayesian scores. Biological validation of resulting clusters can be done via enrichment analysis, another functionality implemented in the methodology. This last tool is of extreme interest for drug discovery, given that it can be used as a validation of the clusters obtained, as well as for the study of new possible interactions between certain side effects and nontargeted pathways. Results Results were evaluated on a 5-folds cross validations procedure, and extensive comparisons were made with available datasets in the field: Zhang et al. (2015), Liu et al. (2012) and Mizutani et al. (2012). Results are promising and show better performances in most of the cases with respect to the available literature. Availability DrugClust is an R package freely available at: https://cran.r-project.org/web/packages/DrugClust/index.html.},
author = {Dimitri, Giovanna Maria and Li{\'{o}}, Pietro},
doi = {10.1016/j.compbiolchem.2017.03.008},
issn = {14769271},
journal = {Comput. Biol. Chem.},
keywords = {Drugs side effects,Machine learning,R package DrugClust},
month = {jun},
pages = {204--210},
publisher = {Elsevier},
title = {{DrugClust: A machine learning approach for drugs side effects prediction}},
url = {https://www.sciencedirect.com/science/article/pii/S1476927116302195?via{\%}3Dihub},
volume = {68},
year = {2017}
}
@article{Quinn2019,
abstract = {Many scientists are now interested in studying the correlative relationships between microbes and metabolites. However, these kinds of analyses are complicated by the compositional (i.e., relative) nature of the data. Recently, Morton et al. proposed a neural network architecture called mmvec to predict metabolite abundances from microbe presence. They introduce this method as a scale invariant solution to the integration of multi-omics compositional data, and claim that “mmvec is the only method robust to scale deviations”. We do not doubt the utility of mmvec, but write in defense of simple linear statistics. In fact, when used correctly, correlation and proportionality can actually outperform the mmvec neural network.},
author = {Quinn, Thomas P. and Erb, Ionas},
doi = {10.1101/847475},
journal = {bioRxiv},
month = {nov},
pages = {847475},
publisher = {Cold Spring Harbor Laboratory},
title = {{Another look at microbe–metabolite interactions: how scale invariant correlations can outperform a neural network}},
year = {2019}
}
@article{Gronau2017,
abstract = {The marginal likelihood plays an important role in many areas of Bayesian statistics such as parameter estimation, model comparison, and model averaging. In most applications, however, the marginal likelihood is not analytically tractable and must be approximated using numerical methods. Here we provide a tutorial on bridge sampling (Bennett, 1976; Meng {\&} Wong, 1996), a reliable and relatively straightforward sampling method that allows researchers to obtain the marginal likelihood for models of varying complexity. First, we introduce bridge sampling and three related sampling methods using the beta-binomial model as a running example. We then apply bridge sampling to estimate the marginal likelihood for the Expectancy Valence (EV) model—a popular model for reinforcement learning. Our results indicate that bridge sampling provides accurate estimates for both a single participant and a hierarchical version of the EV model. We conclude that bridge sampling is an attractive method for mathematical psychologists who typically aim to approximate the marginal likelihood for a limited set of possibly high-dimensional models.},
archivePrefix = {arXiv},
arxivId = {1703.05984},
author = {Gronau, Quentin F. and Sarafoglou, Alexandra and Matzke, Dora and Ly, Alexander and Boehm, Udo and Marsman, Maarten and Leslie, David S. and Forster, Jonathan J. and Wagenmakers, Eric Jan and Steingroever, Helen},
doi = {10.1016/j.jmp.2017.09.005},
eprint = {1703.05984},
isbn = {0022-2496},
issn = {10960880},
journal = {J. Math. Psychol.},
keywords = {Bayes factor,Hierarchical model,Marginal likelihood,Normalizing constant,Predictive accuracy,Reinforcement learning},
month = {mar},
pages = {80--97},
pmid = {29200501},
title = {{A tutorial on bridge sampling}},
url = {http://arxiv.org/abs/1703.05984},
volume = {81},
year = {2017}
}
@article{Iwagami2018,
abstract = {{\textcopyright} 2018 Iwagami et al. Objective: We investigated the burden of chronic kidney disease (CKD) among patients with severe mental illness (SMI). Methods: We identified patients with SMI among all those aged 25–74 registered in the UK Clinical Practice Research Datalink as on March 31, 2014. We compared the prevalence of CKD (two measurements of estimated glomerular filtration rate {\textless}60 mL/min/1.73 m2 for ≥3 months) and renal replacement therapy between patients with and without SMI. For patients with and without a history of lithium prescription separately, we used logistic regression to examine the association between SMI and CKD, adjusting for demographics, lifestyle characteristics, and known CKD risk factors. Results: The CKD prevalence was 14.6{\%} among patients with SMI and a history of lithium prescription (n = 4,295), 3.3{\%} among patients with SMI and no history of lithium prescription (n = 24,101), and 2.1{\%} among patients without SMI (n = 2,387,988; P {\textless} 0.001). The prevalence of renal replacement therapy was 0.23{\%}, 0.15{\%}, and 0.11{\%}, respectively (P = 0.012). Compared to patients without SMI, the fully adjusted odds ratio for CKD was 6.49 (95{\%} CI 5.84–7.21) for patients with SMI and a history of lithium prescription and 1.45 (95{\%} CI 1.34–1.58) for patients with SMI and no history of lithium prescription. The higher prevalence of CKD in patients with SMI may, in part, be explained by more frequent blood testing as compared to the general population. Conclusion: CKD is identified more commonly among patients with SMI than in the general population.},
author = {Iwagami, Masao and Mansfield, Kathryn E. and Hayes, Joseph F. and Walters, Kate and Osborn, David P.J. and Smeeth, Liam and Nitsch, Dorothea and Tomlinson, Laurie A.},
doi = {10.2147/CLEP.S154841},
issn = {11791349},
journal = {Clin. Epidemiol.},
keywords = {Bipolar disorder,Chronic kidney disease,Lithium,Schizophrenia,Severe mental illness},
month = {apr},
pages = {421--429},
title = {{Severe mental illness and chronic kidney disease: A cross-sectional study in the united kingdom}},
url = {https://www.dovepress.com/severe-mental-illness-and-chronic-kidney-disease-a-cross-sectional-stu-peer-reviewed-article-CLEP},
volume = {10},
year = {2018}
}
@inproceedings{Watters2017a,
abstract = {From just a glance, humans can make rich predictions about the future of a wide range of physical systems. On the other hand, modern approaches from engineering, robotics, and graphics are often restricted to narrow domains or require information about the underlying state. We introduce the Visual Interaction Network, a generalpurpose model for learning the dynamics of a physical system from raw visual observations. Our model consists of a perceptual front-end based on convolutional neural networks and a dynamics predictor based on interaction networks. Through joint training, the perceptual front-end learns to parse a dynamic visual scene into a set of factored latent object representations. The dynamics predictor learns to roll these states forward in time by computing their interactions, producing a predicted physical trajectory of arbitrary length. We found that from just six input video frames the Visual Interaction Network can generate accurate future trajectories of hundreds of time steps on a wide range of physical systems. Our model can also be applied to scenes with invisible objects, inferring their future states from their effects on the visible objects, and can implicitly infer the unknown mass of objects. This work opens new opportunities for model-based decision-making and planning from raw sensory observations in complex physical environments.},
author = {Watters, Nicholas and Tacchetti, Andrea and Pascanu, Th{\'{e}}ophane Weber Razvan and Battaglia, Peter and Zoran, Daniel},
booktitle = {Adv. Neural Inf. Process. Syst.},
issn = {10495258},
pages = {4540--4548},
title = {{Visual interaction networks: Learning a physics simulator from video}},
url = {https://papers.nips.cc/paper/7040-visual-interaction-networks-learning-a-physics-simulator-from-video},
volume = {2017-Decem},
year = {2017}
}
@article{Clithero-Eridon2019,
abstract = {Background: A socially accountable health professional education curriculum aims to produce fit-for-purpose graduates to work in areas of need. 'Fit-for-purpose' can be assessed by monitoring graduate practice attributes. Aim: The aim of this article was to identify whether graduates of 'fit-for-purpose' programmes are socially accountable. Setting: The setting for this project was all 37 district hospitals in the KwaZulu-Natal province in Durban, South Africa. Methods: We surveyed healthcare professionals working at district hospitals in the KwaZulu-Natal province. We compared four social accountability indicators identified by the Training for Health Network Framework, comparing medical doctors educated at the Nelson R. Mandela School of Medicine (NRMSM) with medical doctors educated at other South African and non-South African medical schools. In addition, we explored medical doctors' characteristics and reasons for leaving or staying at district hospitals. Results: The pursuit of specialisation or skills development were identified as reasons for leaving in the next 5 years. Although one-third of all medical doctors reported an intention to stay, graduates from non-South African schools remained working at a district hospital longer than graduates of NRMSM or other South African schools and they held a majority of leadership positions. Across all schools, graduates who worked at the district hospital longer than 5 years cited remaining close to family and enjoyment of the work and lifestyle as motivating factors. Conclusion: Using a social accountability approach, this research assists in identifying areas of improvement in workforce development. Tracking what medical doctors do and where they work after graduation is important to ensure that medical schools are meeting their social accountability mandate to meet community needs.},
author = {Clithero-Eridon, Amy and Albright, Danielle and Crandall, Cameron and Ross, Andrew},
doi = {10.4102/phcfm.v11i1.1962},
issn = {20712936},
journal = {African J. Prim. Heal. Care Fam. Med.},
keywords = {District hospitals,Physician recruitment,Physician retention,Physician workforce,Social accountability},
month = {apr},
number = {1},
pages = {e1--e7},
pmid = {31038340},
title = {{Contribution of the Nelson R. Mandela School of Medicine to a socially accountable health workforce}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/31038340 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC6489146 http://www.phcfm.org/index.php/PHCFM/article/view/1962},
volume = {11},
year = {2019}
}
@article{Eslami2018,
abstract = {Scene representation—the process of converting visual sensory data into concise descriptions—is a requirement for intelligent behavior. Recent work has shown that neural networks excel at this task when provided with large, labeled datasets. However, removing the reliance on human labeling remains an important open problem. To this end, we introduce the Generative Query Network (GQN), a framework within which machines learn to represent scenes using only their own sensors. The GQN takes as input images of a scene taken from different viewpoints, constructs an internal representation, and uses this representation to predict the appearance of that scene from previously unobserved viewpoints. The GQN demonstrates representation learning without human labels or domain knowledge, paving the way toward machines that autonomously learn to understand the world around them.},
author = {Eslami, S. M.Ali and {Jimenez Rezende}, Danilo and Besse, Frederic and Viola, Fabio and Morcos, Ari S. and Garnelo, Marta and Ruderman, Avraham and Rusu, Andrei A. and Danihelka, Ivo and Gregor, Karol and Reichert, David P. and Buesing, Lars and Weber, Theophane and Vinyals, Oriol and Rosenbaum, Dan and Rabinowitz, Neil and King, Helen and Hillier, Chloe and Botvinick, Matt and Wierstra, Daan and Kavukcuoglu, Koray and Hassabis, Demis},
doi = {10.1126/science.aar6170},
issn = {10959203},
journal = {Science},
month = {jun},
number = {6394},
pages = {1204--1210},
pmid = {29903970},
publisher = {American Association for the Advancement of Science},
title = {{Neural scene representation and rendering}},
url = {https://science.sciencemag.org/content/360/6394/1204.full?ijkey=kGcNflzOLiIKQ{\&}keytype=ref{\&}siteid=sci},
volume = {360},
year = {2018}
}
@article{Dimitri2017a,
abstract = {Abstract Background We present a multiplex network model for the analysis of Intracranial Pressure (ICP) and Heart Rate (HR) behaviour after severe brain traumatic injuries in pediatric patients. The ICP monitoring is of vital importance for checking life threathening conditions, and understanding the behaviour of these parameters is crucial for a successful intervention of the clinician. Our own observations, exhibit cross-talks interaction events happening between HR and ICP, i.e. transients in which both the ICP and the HR showed an increase of 20{\%} with respect to their baseline value in the window considered. We used a complex event processing methodology, to investigate the relationship between HR and ICP, after traumatic brain injuries (TBI). In particular our goal has been to analyse events of simultaneous increase by HR and ICP (i.e. cross-talks), modelling the two time series as a unique multiplex network system (Lacasa et al., Sci Rep 5:15508-15508, 2014). Methods and data We used a complex network approach based on visibility graphs (Lacasa et al., Sci Rep 5:15508-15508, 2014) to model and study the behaviour of our system and to investigate how and if network topological measures can give information on the possible detection of crosstalks events taking place in the system. Each time series was converted as a layer in a multiplex network. We therefore studied the network structure, focusing on the behaviour of the two time series in the cross-talks events windows detected. We used a dataset of 27 TBI pediatric patients, admitted to Addenbrooke's Hospital, Cambridge, Pediatric Intensive Care Unit (PICU) between August 2012 and December 2014. Results Following a preliminary statistical exploration of the two time series of ICP and HR, we analysed the multiplex network proposed, focusing on two standard topological network metrics: the mutual interaction, and the average edge overlap (Lacasa et al., Sci Rep 5:15508-15508, 2014). We compared results obtained for these two indicators, considering windows in which a cross talks event between HR and ICP was detected with windows in which cross talks events were not present. The analysis of such metrics gave us interesting insights on the time series behaviour. More specifically we observed an increase in the value of the mutual interaction in the case of cross talk as compared to non cross talk. This seems to suggest that mutual interaction could be a potentially interesting “marker” for cross...},
author = {Dimitri, Giovanna Maria and Agrawal, Shruti and Young, Adam and Donnelly, Joseph and Liu, Xiuyun and Smielewski, Peter and Hutchinson, Peter and Czosnyka, Marek and Li{\'{o}}, Pietro and Haubrich, Christina},
doi = {10.1007/s41109-017-0050-3},
issn = {23648228},
journal = {Appl. Netw. Sci.},
keywords = {ICP,Multiplex time series network,Visibility graph},
month = {dec},
number = {1},
pages = {29},
publisher = {Springer International Publishing},
title = {{A multiplex network approach for the analysis of intracranial pressure and heart rate data in traumatic brain injured patients}},
url = {http://appliednetsci.springeropen.com/articles/10.1007/s41109-017-0050-3},
volume = {2},
year = {2017}
}
@article{McKinney2017,
author = {McKinney, Brandon C.},
doi = {10.1176/appi.ajp.2017.17101074},
issn = {0002-953X},
journal = {Am. J. Psychiatry},
keywords = {Molecular Biology,Mood Disorders-Unipolar,Neuropathology,Suicide},
month = {dec},
number = {12},
pages = {1134--1136},
pmid = {29191035},
title = {{Epigenetic Programming: A Putative Neurobiological Mechanism Linking Childhood Maltreatment and Risk for Adult Psychopathology}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/29191035 http://ajp.psychiatryonline.org/doi/10.1176/appi.ajp.2017.17101074},
volume = {174},
year = {2017}
}
@article{Satija2015a,
abstract = {Spatial localization is a key determinant of cellular fate and behavior, but methods for spatially resolved, transcriptome-wide gene expression profiling across complex tissues are lacking. RNA staining methods assay only a small number of transcripts, whereas single-cell RNA-seq, which measures global gene expression, separates cells from their native spatial context. Here we present Seurat, a computational strategy to infer cellular localization by integrating single-cell RNA-seq data with in situ RNA patterns. We applied Seurat to spatially map 851 single cells from dissociated zebrafish (Danio rerio) embryos and generated a transcriptome-wide map of spatial patterning. We confirmed Seurat's accuracy using several experimental approaches, then used the strategy to identify a set of archetypal expression patterns and spatial markers. Seurat correctly localizes rare subpopulations, accurately mapping both spatially restricted and scattered groups. Seurat will be applicable to mapping cellular localization within complex patterned tissues in diverse systems.},
author = {Satija, Rahul and Farrell, Jeffrey A and Gennert, David and Schier, Alexander F and Regev, Aviv},
doi = {10.1038/nbt.3192},
isbn = {1546-1696 (Electronic)$\backslash$r1087-0156 (Linking)},
issn = {1087-0156},
journal = {Nat. Biotechnol.},
month = {apr},
number = {5},
pages = {495--502},
pmid = {25867923},
publisher = {Nature Research},
title = {{Spatial reconstruction of single-cell gene expression data}},
url = {http://www.nature.com/doifinder/10.1038/nbt.3192 http://dx.doi.org/10.1038/nbt.3192},
volume = {33},
year = {2015}
}
@article{Dehmamy2019,
abstract = {To deepen our understanding of graph neural networks, we investigate the representation power of Graph Convolutional Networks (GCN) through the looking glass of graph moments, a key property of graph topology encoding path of various lengths. We find that GCNs are rather restrictive in learning graph moments. Without careful design, GCNs can fail miserably even with multiple layers and nonlinear activation functions. We analyze theoretically the expressiveness of GCNs, arriving at a modular GCN design, using different propagation rules. Our modular design is capable of distinguishing graphs from different graph generation models for surprisingly small graphs, a notoriously difficult problem in network science. Our investigation suggests that, depth is much more influential than width, with deeper GCNs being more capable of learning higher order graph moments. Additionally, combining GCN modules with different propagation rules is critical to the representation power of GCNs.},
archivePrefix = {arXiv},
arxivId = {1907.05008},
author = {Dehmamy, Nima and Barab{\'{a}}si, Albert-L{\'{a}}szl{\'{o}} and Yu, Rose},
eprint = {1907.05008},
month = {jul},
title = {{Understanding the Representation Power of Graph Neural Networks in Learning Graph Topology}},
url = {http://arxiv.org/abs/1907.05008},
year = {2019}
}
@article{Govender2017,
abstract = {In recent years, regulatory T cells (Treg)-based immunotherapy has emerged as a promising strategy to promote operational tolerance after solid organ transplantation (SOT). However, a main hurdle for the therapeutic use of Treg in transplantation is their low frequency, particularly in non-lymphopenic hosts. We aimed to expand Treg directly in vivo and determine their efficacy in promoting donor-specific tolerance, using a stringent experimental model. Administration of the IL-2/JES6-1 immune complex at the time of transplantation resulted in significant expansion of donor-specific Treg, which suppressed alloreactive T cells. IL-2-mediated Treg expansion in combination with short-term CD154-CD40 co-stimulation blockade, but not CTLA-4 Ig or rapamycin, led to tolerance to MHC-mismatched skin grafts in non-lymphopenic mice, mainly by hindering alloreactive CD8(+) effector T cells and the production of alloantibodies. Importantly, this treatment also allowed prolonged survival of allografts in the presence of either donor-specific or cross-reactive memory cells. However, late rejection occurred in sensitized hosts, partly mediated by activated B cells. Overall, these data illustrate the potential but also some important limitations of Treg-based therapy in clinical SOT as well as the importance of concomitant immunomodulatory strategies in particular in sensitized hosts.},
author = {Govender, Lerisa and Wyss, Jean Christophe and Kumar, Rajesh and Pascual, Manuel and Golshayan, Dela},
doi = {10.3389/fimmu.2017.00421},
issn = {16643224},
journal = {Front. Immunol.},
keywords = {Alloantibody,Memory T cells,Regulatory T cells,Tolerance,Transplantation},
number = {APR},
pages = {421},
pmid = {28484450},
publisher = {Frontiers Media SA},
title = {{IL-2-mediated in vivo expansion of regulatory T cells combined with CD154-CD40 co-stimulation blockade but not CTLA-4 Ig prolongs allograft survival in naive and sensitized mice}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28484450 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5399033},
volume = {8},
year = {2017}
}
@article{Thistlethwaite1960,
abstract = {This study presents a method of testing casual hypotheses, called regression-discontinuity analysis, in situations where the investigator is unable to randomly assign Ss to experimental and control groups. The Ss were selected from near winners—5126 students who received certificates of merit and 2848 students who merely received letters of commendation. Comparison of the results obtained from the new mode of analysis with those obtained when the ex post facto design was applied to the same data. The new analysis suggested that public recognition for achievement tends to increase the likelihood that the recipient will receive a scholarship but did not support the inference that recognition affects the student's attitudes and career plans. From Psyc Abstracts 36:01:1AF09T. (PsycINFO Database Record (c) 2013 APA, all rights reserved)},
author = {Thistlethwaite, Donald L. and Campbell, Donald T.},
doi = {10.1037/h0044319},
isbn = {0066-4308 (Print)$\backslash$r0066-4308 (Linking)},
issn = {00220663},
journal = {J. Educ. Psychol.},
keywords = {ANALYSIS,EDUCATIONAL MEASUREMENT,KL,REGRESSION-DISCONTINUITY,STATISTICS},
number = {6},
pages = {309--317},
pmid = {13149141},
title = {{Regression-discontinuity analysis: An alternative to the ex post facto experiment}},
url = {http://content.apa.org/journals/edu/51/6/309},
volume = {51},
year = {1960}
}
@article{Winston2016,
abstract = {A founding father of artificial intelligence.},
author = {Winston, Patrick Henry},
doi = {10.1038/530282a},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {14764687},
journal = {Nature},
number = {7590},
pages = {282},
pmid = {26887486},
title = {{Marvin L. Minsky (1927-2016)}},
volume = {530},
year = {2016}
}
@article{Bocchetta2017,
abstract = {BACKGROUND Recent observational studies have focused on lithium treatment in the elderly, with particular reference to safety in terms of thyroid and renal functions. The purpose of this study was to compare the clinical characteristics of patients starting lithium treatment before (N = 79) or after (N = 31) the age of 65 years. Patients were followed up for 6 years with focus on renal function and prescription of levothyroxine and methimazole. RESULTS At baseline, median lithium serum concentration was 0.55 mmol/l. The estimated glomerular filtration rate was lower than 60 ml/min/1.73 m2 in 43 (39{\%}) patients. In a multiple regression analysis controlling for age and gender, we found a significant effect of duration of lithium treatment on estimated glomerular filtration rate (-0.85 ml/min/1.73 m2 per year of prior exposure). The annual decline during follow-up was 2.3 ml/min/1.73 m2. Two patients were prescribed levothyroxine, and two were prescribed methimazole for the first time during follow-up. CONCLUSIONS Median lithium serum concentration in this cohort of elderly patients with mainly bipolar disorders was lower than the therapeutic range indicated for younger adults. The decline in glomerular filtration rate may be accelerated by long-term lithium use. Thyroid and renal functions continue to require close monitoring throughout the course of lithium treatment. Trial registration NP/2013/3836. Registered 24 June 2013.},
author = {Bocchetta, Alberto and Cabras, Francesca and Pinna, Martina and Poddighe, Antonio and Sardu, Claudia and Ardau, Raffaella and Chillotti, Caterina and {Del Zompo}, Maria},
doi = {10.1186/s40345-017-0089-1},
issn = {21947511},
journal = {Int. J. Bipolar Disord.},
keywords = {Bipolar disorders,Creatinine,Depression,Elderly,Kidney,Lithium,Mood disorders,Nephrotoxicity,Thyroid},
month = {dec},
number = {1},
pages = {19},
pmid = {28393327},
publisher = {Springer},
title = {{An observational study of 110 elderly lithium-treated patients followed up for 6 years with particular reference to renal function}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28393327 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5457744},
volume = {5},
year = {2017}
}
@inproceedings{Santurkar2018,
abstract = {Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called "internal covariate shift". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.},
archivePrefix = {arXiv},
arxivId = {1805.11604},
author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
booktitle = {Adv. Neural Inf. Process. Syst.},
eprint = {1805.11604},
issn = {10495258},
month = {may},
pages = {2483--2493},
title = {{How does batch normalization help optimization?}},
url = {http://arxiv.org/abs/1805.11604},
volume = {2018-Decem},
year = {2018}
}
@article{Pearl2010,
abstract = {This paper summarizes recent advances in causal inference and underscores the paradigmatic shifts that must be undertaken in moving from traditional statistical analysis to causal analysis of multivariate data. Special emphasis is placed on the assumptions that underlie all causal inferences, the languages used in formulating those assumptions, the conditional nature of all causal and counterfactual claims, and the methods that have been developed for the assessment of such claims. These advances are illustrated using a general theory of causation based on the Structural Causal Model (SCM) described in Pearl (2000a), which subsumes and unifies other approaches to causation, and provides a coherent mathematical foundation for the analysis of causes and counterfactuals. In particular, the paper surveys the development of mathematical tools for inferring (from a combination of data and assumptions) answers to three types of causal queries: those about (1) the effects of potential interventions, (2) probabilities of counterfactuals, and (3) direct and indirect effects (also known as "mediation"). Finally, the paper defines the formal and conceptual relationships between the structural and potential-outcome frameworks and presents tools for a symbiotic analysis that uses the strong features of both. The tools are demonstrated in the analyses of mediation, causes of effects, and probabilities of causation. Copyright {\textcopyright} 2010 The Berkeley Electronic Press. All rights reserved.},
author = {Pearl, Judea},
doi = {10.2202/1557-4679.1203},
issn = {15574679},
journal = {Int. J. Biostat.},
keywords = {Causal effects,Causes of effects,Confounding,Counterfactuals,Graphical methods,Mediation,Policy evaluation,Potential-outcome,Structural equation models},
month = {feb},
number = {2},
pages = {Article 7},
pmid = {20305706},
publisher = {Berkeley Electronic Press},
title = {{An introduction to causal inference}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20305706 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2836213},
volume = {6},
year = {2010}
}
@misc{cycells_url,
author = {Warrender, C E},
howpublished = {$\backslash$url{\{}http://sourceforge.net/projects/cycells{\}}},
title = {{CyCells (Open source software)}},
url = {http://sourceforge.net/projects/cycells},
year = {2003}
}
@misc{A2019,
author = {A, Alexa and Rahnenfuhrer, J},
title = {{topGO: Enrichment Analysis for Gene Ontology}},
url = {https://bioconductor.org/packages/release/bioc/html/topGO.html},
year = {2019}
}
@article{Perakslis2010,
abstract = {Much enthusiasm and energy are being directed toward open- source software approaches, precompetitive data sharing, and external innovation in the biopharmaceutical industry. At Johnson {\&} Johnson (J{\&}J), we have undertaken aggressive approaches to optimizing productivity and progress to address unmet medical needs. In this article, tranSMART, a fully translational data warehouse based on the open-source i2b2 platform, serves as a case study and the basis for discussing the positive role that informatics can play in accelerating translational research.},
author = {Perakslis, E D and {Van Dam}, J and Szalma, S},
doi = {10.1038/clpt.2010.21},
isbn = {1532-6535 (Linking)},
issn = {00099236},
journal = {Clin. Pharmacol. Ther.},
number = {5},
pages = {614--616},
pmid = {20376001},
title = {{How informatics can potentiate precompetitive open-source collaboration to jump-start drug discovery and development.}},
url = {http://dx.doi.org/10.1038/clpt.2010.21},
volume = {87},
year = {2010}
}
@article{Miller1995,
abstract = {This database links English nouns, verbs, adjectives, and adverbs to sets of synonyms that are in turn linked through semantic relations that determine word definitions. {\textcopyright} 1995, ACM. All rights reserved.},
author = {Miller, George A.},
doi = {10.1145/219717.219748},
issn = {15577317},
journal = {Commun. ACM},
month = {nov},
number = {11},
pages = {39--41},
publisher = {ACM},
title = {{WordNet: A Lexical Database for English}},
url = {http://portal.acm.org/citation.cfm?doid=219717.219748},
volume = {38},
year = {1995}
}
@misc{bridgesampling_software,
author = {Gronau, Quentin F. and Singmann, Henrik},
title = {{bridgesampling: Bridge Sampling for Marginal Likelihoods and Bayes Factors}},
url = {https://cran.r-project.org/package=bridgesampling},
year = {2018}
}
@misc{software_graphdeeplearning,
title = {{DeepGraphLearning/graphvite: A general and high-performance graph embedding system for various applications}},
url = {https://github.com/DeepGraphLearning/graphvite},
urldate = {2019-08-08}
}
@inproceedings{Schlichtkrull2018,
abstract = {Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to handle the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8{\%} on FB15k-237 over a decoder-only baseline.},
archivePrefix = {arXiv},
arxivId = {1703.06103},
author = {Schlichtkrull, Michael and Kipf, Thomas N. and Bloem, Peter and van den Berg, Rianne and Titov, Ivan and Welling, Max},
booktitle = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
doi = {10.1007/978-3-319-93417-4_38},
eprint = {1703.06103},
isbn = {9783319934167},
issn = {16113349},
month = {jun},
pages = {593--607},
publisher = {Springer, Cham},
title = {{Modeling Relational Data with Graph Convolutional Networks}},
url = {http://link.springer.com/10.1007/978-3-319-93417-4{\_}38},
volume = {10843 LNCS},
year = {2018}
}
@incollection{Steinruecken2019,
abstract = {The Automatic Statistician project aims to automate data science, producing predictions and human-readable reports from raw datasets with minimal human intervention. Alongside basic graphs and statistics, the generated reports contain a curation of high-level insights about the dataset, that are obtained from (1) an automated construction of models for the dataset, (2) a comparison of these models, and (3) a software component that turns these results into natural language descriptions. This chapter describes the common architecture of such Automatic Statistician systems, and discusses some of the design decisions and technical challenges.},
author = {Steinruecken, Christian and Smith, Emma and Janz, David and Lloyd, James and Ghahramani, Zoubin},
doi = {10.1007/978-3-030-05318-5_9},
pages = {161--173},
publisher = {Springer, Cham},
title = {{The Automatic Statistician}},
url = {http://link.springer.com/10.1007/978-3-030-05318-5{\_}9},
year = {2019}
}
@article{Kerr2015,
abstract = {Purpose - The World Food Program provides food aid to areas of the world where food security is poor or non-existent - often failed states. Food can be a weapon in such places and food aid shipments a target for capture. This paper investigates the cost-effectiveness of international efforts to protect World Food Program aid shipments destined for Somalia from seaborne pirates off the Horn of Africa. Findings - The lessons of history were ignored by those attempting to prevent food aid shipments from falling into the hands of pirates. The international community initially used very expensive naval assets to protect shipments. Over time, in an effort to reduce costs, the strategy and assets used to secure shipments evolved. This slow, cost-reduction-driven evolution of the international community's anti-piracy efforts off the Horn of Africa has distinct parallels with the evolution of anti-piracy efforts in the eighteenth and nineteenth centuries. One difference between the historic and current anti-piracy strategies is that there does not appear to be an exit strategy for the latter. Practical implications - Future anti-piracy initiatives might look to previous strategies to avoid the costly experience associated with Somaliabound food aid shipments. Social implications - Achieving food security objectives can be a resource-intensive activity in failed states. This paper provides insights into how the resource cost of providing food security can be reduced.},
author = {Kerr, William A.},
doi = {10.1108/S1574-871520150000015018},
issn = {15748715},
journal = {Front. Econ. Glob.},
keywords = {Food aid,Food security,Naval assets,Piracy,World Food Program},
pages = {145--159},
publisher = {Emerald Group Publishing Ltd.},
title = {{Food security and anti-piracy strategies: The economics of protecting world food program shipments}},
volume = {15},
year = {2015}
}
@article{Mlecnik2019,
abstract = {Summary: Large scale technologies produce massive amounts of experimental data that need to be investigated. To improve their biological interpretation we have developed ClueGO, a Cytoscape App that selects representative Gene Onology terms and pathways for one or multiple lists of genes/proteins and visualizes them into functionally organized networks. Because of its reliability, userfriendliness and support of many species ClueGO gained a large community of users. To further allow scientists programmatic access to ClueGO with R, Python, JavaScript etc., we implemented the cyREST API into ClueGO. In this article we describe this novel, complementary way of accessing ClueGO via REST, and provide R and Phyton examples to demonstrate how ClueGO workflows can be integrated into bioinformatic analysis pipelines. Availability and implementation: ClueGO is available in the Cytoscape App Store (http://apps.cyto scape.org/apps/cluego).},
author = {Mlecnik, Bernhard and Galon, J{\'{e}}r{\^{o}}me and Bindea, Gabriela},
doi = {10.1093/bioinformatics/btz163},
editor = {Wren, Jonathan},
issn = {14602059},
journal = {Bioinformatics},
month = {oct},
number = {19},
pages = {3864--3866},
publisher = {Narnia},
title = {{Automated exploration of gene ontology term and pathway networks with ClueGO-REST}},
url = {https://academic.oup.com/bioinformatics/article/35/19/3864/5371065},
volume = {35},
year = {2019}
}
@article{Korsunsky2018,
abstract = {The rapidly emerging diversity of single cell RNAseq datasets allows us to characterize the transcriptional behavior of cell types across a wide variety of biological and clinical conditions. With this comprehensive breadth comes a major analytical challenge. The same cell type across tissues, from different donors, or in different disease states, may appear to express different genes. A joint analysis of multiple datasets requires the integration of cells across diverse conditions. This is particularly challenging when datasets are assayed with different technologies in which real biological differences are interspersed with technical differences. We present Harmony, an algorithm that projects cells into a shared embedding in which cells group by cell type rather than dataset-specific conditions. Unlike available single-cell integration methods, Harmony can simultaneously account for multiple experimental and biological factors. We develop objective metrics to evaluate the quality of data integration. In four separate analyses, we demonstrate the superior performance of Harmony to four single-cell-specific integration algorithms. Moreover, we show that Harmony requires dramatically fewer computational resources. It is the only available algorithm that makes the integration of ∼ 106 cells feasible on a personal computer. We demonstrate that Harmony identifies both broad populations and fine-grained subpopulations of PBMCs from datasets with large experimental differences. In a meta-analysis of 14,746 cells from 5 studies of human pancreatic islet cells, Harmony accounts for variation among technologies and donors to successfully align several rare subpopulations. In the resulting integrated embedding, we identify a previously unidentified population of potentially dysfunctional alpha islet cells, enriched for genes active in the Endoplasmic Reticulum (ER) stress response. The abundance of these alpha cells correlates across donors with the proportion of dysfunctional beta cells also enriched in ER stress response genes. Harmony is a fast and flexible general purpose integration algorithm that enables the identification of shared fine-grained subpopulations across a variety of experimental and biological conditions.},
author = {Korsunsky, Ilya and Fan, Jean and Slowikowski, Kamil and Zhang, Fan and Wei, Kevin and Baglaenko, Yuriy and Brenner, Michael and Loh, Po-Ru and Raychaudhuri, Soumya},
doi = {10.1101/461954},
journal = {bioRxiv},
month = {nov},
pages = {461954},
publisher = {Cold Spring Harbor Laboratory},
title = {{Fast, sensitive, and accurate integration of single cell data with Harmony}},
url = {https://www.biorxiv.org/content/10.1101/461954v2},
year = {2018}
}
@inproceedings{Kowch2019,
abstract = {This paper discusses principles and practices that can optimize artificial intelligence (AI) in teaching and learning from the perspectives of leading organizational change and by reimaging learning activities with AI as a collaborative partner. Based on the Kowch's participatory teaching and learning (PTL) principles for networked organizations, the authors analyze the integration of new talent patterns emerging in more agile interdisciplinary education systems connected through information and technologies, and propose principles for collaboration through machines as AI participants and principles for designing new education systems where the teams with AI can thrive. Taking a different angle from the traditional linear education system design tasks and education institution redesigns, the proposed principles assume more time for education and training leaders to take stronger leadership roles in the creation of better teams with AI augmentation. We offer principles for designing education institutions that are capable of adapting with these innovations and we also offer principles for designing these next generation learning environments. Finally by 'zooming in' on instruction and AI, we use Activity theory to imagine better inclusions of social and cultural components with AI as an important, emerging, and unscripted new partner.},
author = {Kowch, Eugene G. and Liu, Juhong Christie},
booktitle = {Proc. - Int. Jt. Conf. Information, Media Eng. ICIME 2018},
doi = {10.1109/ICIME.2018.00075},
isbn = {9781538676165},
keywords = {activity system,artificial intelligence,innovative learning environment,participatory teaching and learning (PTL)},
month = {jan},
pages = {320--325},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Principles for Teaching, Leading, and Participatory Learning with a New Participant: AI}},
year = {2019}
}
@phdthesis{Winston1975,
author = {Winston, Patrick Henry},
school = {Massachussets institute of technology},
title = {{A Blocks World Learning Example}},
url = {https://users.cs.cf.ac.uk/Dave.Marshall/AI2/node145.html},
year = {1975}
}
@article{Bhattacharya2018,
abstract = {Patients associated with multiple co-occurring health conditions often face aggravated complications and less favorable outcomes. Co-occurring conditions are especially prevalent among individuals suffering from kidney disease, an increasingly widespread condition affecting 13{\%} of the general population in the US. This study aims to identify and characterize patterns of co-occurring medical conditions in patients employing a probabilistic framework. Specifically, we apply topic modeling in a non-traditional way to find associations across SNOMED-CT codes assigned and recorded in the EHRs of {\textgreater}13,000 patients diagnosed with kidney disease. Unlike most prior work on topic modeling, we apply the method to codes rather than to natural language. Moreover, we quantitatively evaluate the topics, assessing their tightness and distinctiveness, and also assess the medical validity of our results. Our experiments show that each topic is succinctly characterized by a few highly probable and unique disease codes, indicating that the topics are tight. Furthermore, inter-topic distance between each pair of topics is typically high, illustrating distinctiveness. Last, most coded conditions grouped together within a topic, are indeed reported to co-occur in the medical literature. Notably, our results uncover a few indirect associations among conditions that have hitherto not been reported as correlated in the medical literature.},
author = {Bhattacharya, Moumita and Jurkovitz, Claudine and Shatkay, Hagit},
doi = {10.1016/j.jbi.2018.04.008},
issn = {15320464},
journal = {J. Biomed. Inform.},
keywords = {Co-occurring medical conditions,Electronic health records,SNOMED-CT codes,Topic modeling},
month = {jun},
pages = {31--40},
publisher = {Academic Press},
title = {{Co-occurrence of medical conditions: Exposing patterns through probabilistic topic modeling of snomed codes}},
url = {https://www.sciencedirect.com/science/article/pii/S1532046418300728},
volume = {82},
year = {2018}
}
@inproceedings{Miskov-Zivanov2007,
address = {New York, New York, USA},
author = {Miskov-Zivanov, Natasa and Zuliani, Paolo and Clarke, Edmund M. and Faeder, James R.},
booktitle = {Proc. Int. Conf. Bioinformatics, Comput. Biol. Biomed. Informatics - BCB'13},
doi = {10.1145/2506583.2512390},
isbn = {9781450324342},
keywords = {Boolean networks,Formal methods,Immune system,Statistical model checking,T cell},
pages = {728--729},
publisher = {ACM Press},
title = {{Studies of biological networks with statistical model checking}},
url = {http://dl.acm.org/citation.cfm?doid=2506583.2512390},
year = {2013}
}
@article{Reed2015,
abstract = {We propose the neural programmer-interpreter (NPI): a recurrent and compositional neural network that learns to represent and execute programs. NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value program memory, and domain-specific encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability compared to sequence-to-sequence LSTMs. The program memory allows efficient learning of additional tasks by building on existing programs. NPI can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of computation, lessening the long-term memory burden on recurrent hidden units. In this work we train the NPI with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, NPI learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single NPI learns to execute these programs and all 21 associated subprograms.},
archivePrefix = {arXiv},
arxivId = {1511.06279},
author = {Reed, Scott and de Freitas, Nando},
eprint = {1511.06279},
month = {nov},
title = {{Neural Programmer-Interpreters}},
url = {http://arxiv.org/abs/1511.06279},
year = {2015}
}
@article{Ng2017,
abstract = {Scheduling surgeries is a challenging task due to the fundamental uncertainty of the clinical environment, as well as the risks and costs associated with under- and over-booking. We investigate neural regression algorithms to estimate the parameters of surgery case durations, focusing on the issue of heteroscedasticity. We seek to simultaneously estimate the duration of each surgery, as well as a surgery-specific notion of our uncertainty about its duration. Estimating this uncertainty can lead to more nuanced and effective scheduling strategies, as we are able to schedule surgeries more efficiently while allowing an informed and case-specific margin of error. Using surgery records {\%}from the UC San Diego Health System, from a large United States health system we demonstrate potential improvements on the order of 20{\%} (in terms of minutes overbooked) compared to current scheduling techniques. Moreover, we demonstrate that surgery durations are indeed heteroscedastic. We show that models that estimate case-specific uncertainty better fit the data (log likelihood). Additionally, we show that the heteroscedastic predictions can more optimally trade off between over and under-booking minutes, especially when idle minutes and scheduling collisions confer disparate costs.},
archivePrefix = {arXiv},
arxivId = {1702.05386},
author = {Ng, Nathan and Gabriel, Rodney A and McAuley, Julian and Elkan, Charles and Lipton, Zachary C},
eprint = {1702.05386},
month = {feb},
title = {{Predicting Surgery Duration with Neural Heteroscedastic Regression}},
url = {http://arxiv.org/abs/1702.05386},
year = {2017}
}
@article{Tighe2014,
abstract = {reasoning, racioc{\'{i}}nio},
author = {Tighe, Elizabeth L. and Schatschneider, Christopher},
doi = {10.1007/s11145-013-9435-6},
issn = {09224777},
journal = {Read. Writ.},
keywords = {Dominance analysis,Oral reading fluency,Reading comprehension,Reasoning,Working memory},
month = {jan},
number = {1},
pages = {101--127},
pmid = {26346315},
publisher = {NIH Public Access},
title = {{A dominance analysis approach to determining predictor importance in third, seventh, and tenth grade reading comprehension skills}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26346315 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4557879},
volume = {27},
year = {2014}
}
@article{Wang2019b,
abstract = {Chronic kidney disease (CKD), characterized as renal dysfunction, is recognized as a major public health problem with high morbidity and mortality worldwide. Unfortunately, there are no obvious clinical symptoms in early stage disease until severe damage has occurred. Further complicating early diagnosis and treatment is the lack of sensitive and specific biomarkers. As such, novel biomarkers are urgently needed. Metabolomics has shown an increasing potential for identifying underlying disease mechanisms, facilitating clinical diagnosis and developing pharmaceutical treatments for CKD. Recent advances in metabolomics revealed that CKD was closely associated with the dysregulation of numerous metabolites, such as amino acids, lipids, nucleotides and glycoses, that might be exploited as potential biomarkers. In this review, we summarize recent metabolomic applications based on animal model studies and in patients with CKD and highlight several biomarkers that may play important roles in diagnosis, intervention and development of new therapeutic strategies.},
author = {Wang, Yan-Ni and Ma, Shi-Xing and Chen, Yuan-Yuan and Chen, Lin and Liu, Bao-Li and Liu, Qing-Quan and Zhao, Ying-Yong},
doi = {10.1016/J.CCA.2019.08.030},
issn = {0009-8981},
journal = {Clin. Chim. Acta},
month = {dec},
pages = {54--63},
publisher = {Elsevier},
title = {{Chronic kidney disease: Biomarker diagnosis to therapeutic targets}},
url = {https://www.sciencedirect.com/science/article/pii/S000989811932025X?via{\%}3Dihub},
volume = {499},
year = {2019}
}
@article{Waters1982,
abstract = {A group of 29 bipolar manic-depressives completed a 12-month double-blind cross-over trial of low-dose and high-dose lithium prophylaxis. Twelve patients relapsed, and significantly more of the relapses occurred during the low-dose 6-month phase of the trial. There was a trend for relapse to occur within 2 months of an abrupt drop in plasma lithium level, and to occur more often in women than in men. The efficacy of low-dose lithium prophylaxis and the significance of rebound relapse are discussed.},
author = {Waters, B and Lapierre, Y and Gagnon, A and Cahudhry, R and Tremblay, A and Sarantidis, D and Gray, R},
doi = {10.1097/00004714-198304000-00027},
issn = {00063223},
journal = {Biol. Psychiatry},
month = {nov},
number = {11},
pages = {1323--1329},
pmid = {6817830},
title = {{Determination of the optimal concentration of lithium for the prophylaxis of manic-depressive disorder}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/6817830},
volume = {17},
year = {1982}
}
@book{gilks1995markov,
author = {Gilks, W R and Richardson, S and Spiegelhalter, D},
publisher = {Chapman {\&} Hall/CRC},
title = {{Markov Chain Monte Carlo in practice: interdisciplinary statistics}},
volume = {2},
year = {1995}
}
@misc{hastie_glm_elastic_net_matlab,
abstract = {https://web.stanford.edu/{\~{}}hastie/glmnet{\_}matlab/},
author = {Hastie, Trevor},
title = {{Glmnet in Matlab}},
url = {https://web.stanford.edu/{~}hastie/glmnet{\_}matlab/},
urldate = {2017-11-21}
}
@article{Carpenter2017,
abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.2.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propa- gation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line, through R using the RStan package, or through Python using the PyStan package. All three interfaces support sampling or optimization-based inference and analysis, and RStan and PyStan also provide access to log probabilities, gradients, Hessians, and data I/O.},
archivePrefix = {arXiv},
arxivId = {1210.1088},
author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
doi = {10.18637/jss.v076.i01},
eprint = {1210.1088},
isbn = {0394-6320 (Print) 0394-6320 (Linking)},
issn = {1548-7660},
journal = {J. Stat. Softw.},
month = {jan},
number = {1},
pages = {1--32},
pmid = {19505410},
title = {{Stan : A Probabilistic Programming Language}},
url = {http://www.jstatsoft.org/v76/i01/},
volume = {76},
year = {2017}
}
@article{Hajek1966,
abstract = {The presented method is an application of the mathematical logic and computer technique to the research problems of concrete sciences. Let us assume a model (precisely a unary semantic model), i. e. a finite nonempty system of objects and a finite system of properties, to be given. It is known, for each object and each property, whether or not the object possesses the property. (E. g., objects are patients and properties are diseases or facts that some medicines were administered etc.) The research worker usually looks for the relations among the properties that hold true for all or almost all the objects. E. g. that, for each object, the following holds true: if the object possesses the propertyP1 and does not possess the propertyP2 then it possesses the propertyP3. If he finds this relation hold true in the model (of objects under consideration) then he is able to formulate and verify the hypothesis of validity of such a relation for all existing objects in general. The mains of mathematical logic make possible to find a suitable class of formulas of the predicate calculus (i. e. a class of formalized statements) to which the investigation of the model can be confined. (They are some elementary disjunctions.) The means of computer technique make possible to generate and verify all these special formulas automatically in an suitable ordering. In the output device of the computer there will appear all the hypotheses true or almost true in model. Hence the GUHA method (General Unary Hypotheses Automaton) can be considered as a substitution for an intuition in a certain phase of scientific research, say, as an “offering of hypotheses”. In this paper, the main ideas, the needed logical theory and programme description are given. Some further related problems are solved. Finally the experience of practical applications of the GUHA method on IBM 7040-System is described.},
author = {H{\'{a}}jek, P. and Havel, I. and Chytil, M.},
doi = {10.1007/BF02345483},
issn = {0010485X},
journal = {Computing},
month = {dec},
number = {4},
pages = {293--308},
publisher = {Springer-Verlag},
title = {{The GUHA method of automatic hypotheses determination}},
url = {http://link.springer.com/10.1007/BF02345483},
volume = {1},
year = {1966}
}
@article{Gal2015,
abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1506.02142},
author = {Gal, Yarin and Ghahramani, Zoubin},
doi = {10.1109/TKDE.2015.2507132},
eprint = {1506.02142},
isbn = {1506.02142},
issn = {10414347},
month = {jun},
pmid = {88045},
title = {{Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning}},
url = {http://arxiv.org/abs/1506.02142},
year = {2015}
}
@article{Lakkaraju2017,
abstract = {Decision makers, such as doctors and judges, make crucial decisions such as recommending treatments to patients, and granting bails to defendants on a daily basis. Such decisions typically involve weighting the potential benefits of taking an action against the costs involved. In this work, we aim to automate this task of learning cost-effective, interpretable and actionable treatment regimes. We formulate this as a problem of learning a decision list – a sequence of if-then-else rules – which maps characteristics of subjects (eg., diagnostic test results of patients) to treatments. We propose a novel objective to construct a decision list which maximizes outcomes for the population, and minimizes overall costs. Since we do not observe the outcomes corresponding to counterfactual scenarios, we use techniques from causal inference literature to infer them. We model the problem of learning the decision list as a Markov Decision Process (MDP) and employ a variant of the Upper Confidence Bound for Trees (UCT) strategy which leverages customized checks for pruning the search space effectively. Experimental results on real world observational data capturing judicial bail decisions and treatment recommendations for asthma patients demonstrate the effectiveness of our approach.},
author = {Lakkaraju, Himabindu and Rudin, Cynthia},
issn = {1938-7228},
journal = {Proc. 20th Int. Conf. Artif. Intell. Stat.},
month = {apr},
pages = {166--175},
title = {{Learning Cost-Effective and Interpretable Treatment Regimes}},
url = {http://proceedings.mlr.press/v54/lakkaraju17a.html},
volume = {54},
year = {2017}
}
@misc{software_cellranger,
title = {{10XGenomics/cellranger: 10x Genomics Single Cell 3' Gene Expression and VDJ Assembly}},
url = {https://github.com/10XGenomics/cellranger},
urldate = {2019-12-24}
}
@article{Boettiger2013,
author = {Boettiger, Carl and Ross, Noam and Hastings, Alan},
doi = {10.1007/s12080-013-0192-6},
issn = {1874-1738},
journal = {Theor. Ecol.},
keywords = {bifurcation,critical slowing down,early warning signals,regime shifts},
month = {jun},
number = {3},
pages = {255--264},
title = {{Early warning signals: the charted and uncharted territories}},
url = {http://link.springer.com/10.1007/s12080-013-0192-6},
volume = {6},
year = {2013}
}
@article{Letham2015,
abstract = {We aim to produce predictive models that are not only accurate, but are also interpretable to human experts. Our models are decision lists, which consist of a series of if {\ldots} then. . . statements (e.g., if high blood pressure, then stroke) that discretize a high-dimensional, multivariate feature space into a series of simple, readily interpretable decision statements. We introduce a generative model called Bayesian Rule Lists that yields a posterior distribution over possible decision lists. It employs a novel prior structure to encourage sparsity. Our experiments show that Bayesian Rule Lists has predictive accuracy on par with the current top algorithms for prediction in machine learning. Our method is motivated by recent developments in personalized medicine, and can be used to produce highly accurate and interpretable medical scoring systems. We demonstrate this by producing an alternative to the CHADS2 score, actively used in clinical practice for estimating the risk of stroke in patients that have atrial fibrillation. Our model is as interpretable as CHADS2, but more accurate.},
archivePrefix = {arXiv},
arxivId = {arXiv:1511.01644v1},
author = {Letham, Benjamin and Rudin, Cynthia and McCormick, Tyler H. and Madigan, David},
doi = {10.1214/15-AOAS848},
eprint = {arXiv:1511.01644v1},
isbn = {9781577356288},
issn = {19417330},
journal = {Ann. Appl. Stat.},
keywords = {Bayesian analysis,Classification,Interpretability},
month = {nov},
number = {3},
pages = {1350--1371},
title = {{Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model}},
url = {http://arxiv.org/abs/1511.01644 http://dx.doi.org/10.1214/15-AOAS848},
volume = {9},
year = {2015}
}
@misc{uci_ml_data,
file = {::},
title = {{UCI Machine Learning Repository: Data Sets}},
url = {http://archive.ics.uci.edu/ml/datasets.html},
urldate = {2013-08-22}
}
@article{Haberman2014,
abstract = {Interactions between the host and gut microbial community likely contribute to Crohn disease (CD) pathogenesis; however, direct evidence for these interactions at the onset of disease is lacking. Here, we characterized the global pattern of ileal gene expression and the ileal microbial community in 359 treatment-naive pediatric patients with CD, patients with ulcerative colitis (UC), and control individuals. We identified core gene expression profiles and microbial communities in the affected CD ilea that are preserved in the unaffected ilea of patients with colon-only CD but not present in those with UC or control individuals; therefore, this signature is specific to CD and independent of clinical inflammation. An abnormal increase of antimicrobial dual oxidase (DUOX2) expression was detected in association with an expansion of Proteobacteria in both UC and CD, while expression of lipoprotein APOA1 gene was downregulated and associated with CD-specific alterations in Firmicutes. The increased DUOX2 and decreased APOA1 gene expression signature favored oxidative stress and Th1 polarization and was maximally altered in patients with more severe mucosal injury. A regression model that included APOA1 gene expression and microbial abundance more accurately predicted month 6 steroid-free remission than a model using clinical factors alone. These CD-specific host and microbe profiles identify the ileum as the primary inductive site for all forms of CD and may direct prognostic and therapeutic approaches.},
author = {Haberman, Yael and Tickle, Timothy L. and Dexheimer, Phillip J. and Kim, Mi Ok and Tang, Dora and Karns, Rebekah and Baldassano, Robert N. and Noe, Joshua D. and Rosh, Joel and Markowitz, James and Heyman, Melvin B. and Griffiths, Anne M. and Crandall, Wallace V. and Mack, David R. and Baker, Susan S. and Huttenhower, Curtis and Keljo, David J. and Hyams, Jeffrey S. and Kugathasan, Subra and Walters, Thomas D. and Aronow, Bruce and Xavier, Ramnik J. and Gevers, Dirk and Denson, Lee A.},
doi = {10.1172/JCI75436},
isbn = {1558-8238 (Electronic)$\backslash$r0021-9738 (Linking)},
issn = {15588238},
journal = {J. Clin. Invest.},
month = {aug},
number = {8},
pages = {3617--3633},
pmid = {25003194},
publisher = {American Society for Clinical Investigation},
title = {{Pediatric Crohn disease patients exhibit specific ileal transcriptome and microbiome signature}},
url = {https://www.jci.org/articles/view/75436},
volume = {124},
year = {2014}
}
@misc{cite_pseudonymizer,
title = {{OpenPseudononymiser}},
url = {https://www.openpseudonymiser.org/},
urldate = {2018-04-18}
}
@article{Close2014a,
abstract = {OBJECTIVE: Lithium users are offered routine renal monitoring but few studies have quantified the risk to renal health. The aim of this study was to assess the association between use of lithium carbonate and incidence of renal failure in patients with bipolar disorder.$\backslash$n$\backslash$nMETHODS: This was a retrospective cohort study using the General Practice Research Database (GPRD) and a nested validation study of lithium exposure and renal failure. A cohort of 6360 participants aged over 18 years had a first recorded diagnosis of bipolar disorder between January 1, 1990 and December 31, 2007. Data were examined from electronic primary care records from 418 general practices across the UK. The primary outcome was the hazard ratio for renal failure in participants exposed to lithium carbonate as compared with non-users of lithium, adjusting for age, gender, co-morbidities, and poly-pharmacy.$\backslash$n$\backslash$nRESULTS: Ever use of lithium was associated with a hazard ratio for renal failure of 2.5 (95{\%} confidence interval 1.6 to 4.0) adjusted for known renal risk factors. Absolute risk was age dependent, with patients of 50 years or older at particular risk of renal failure: Number Needed to Harm (NNH) was 44 (21 to 150).$\backslash$n$\backslash$nCONCLUSIONS: Lithium is associated with an increased risk of renal failure, particularly among the older age group. The absolute risk of renal failure associated with lithium use remains small.},
author = {Close, Helen and Reilly, Joe and Mason, James M. and Kripalani, Mukesh and Wilson, Douglas and Main, John and Hungin, A. Pali S.},
doi = {10.1371/journal.pone.0090169},
editor = {van Os, Jim},
issn = {19326203},
journal = {PLoS One},
month = {mar},
number = {3},
pages = {e90169},
publisher = {Public Library of Science},
title = {{Renal failure in lithium-treated bipolar disorder: A retrospective cohort study}},
url = {https://dx.plos.org/10.1371/journal.pone.0090169},
volume = {9},
year = {2014}
}
@inproceedings{Wang2018,
abstract = {Diagrams in mechanised reasoning systems are typically encoded into symbolic representations that can be easily processed with rule-based expert systems. This relies on human experts to define diagram-to-symbol mapping and the set of rules to reason with the sym- bols. We present a new method of using Deep artificial Neural Networks (DNN) to learn continuous, vector-form representations of diagrams with- out any human input, and entirely from datasets of diagrammatic reason- ing problems. Based on this DNN, we developed a novel reasoning system, Euler-Net, to solve syllogisms with Euler diagrams. Euler-Net takes two diagrams representing the premises in a syllogism as input, and outputs either a categorical (subset, intersection or disjoint) or diagrammatic con- clusion (generating an Euler diagram representing the conclusion) to the syllogism. Euler-Net can achieve 99.5{\%} accuracy for generating syllogism conclusions, and learns meaningful representations. We propose that our framework can be applied to other types of diagrams, especially the ones we are less sure how to formalise symbolically.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Wang, Duo and Jamnik, Mateja and Li{\`{o}}, Pietro},
booktitle = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
doi = {10.1007/978-3-319-91376-6_36},
eprint = {arXiv:1011.1669v3},
isbn = {9783319913759},
issn = {16113349},
month = {jun},
pages = {390--398},
pmid = {1000198484},
publisher = {Springer, Cham},
title = {{Investigating diagrammatic reasoning with deep neural networks}},
url = {http://link.springer.com/10.1007/978-3-319-91376-6{\_}36},
volume = {10871 LNAI},
year = {2018}
}
@misc{Banerjee2019c,
author = {Banerjee, Soumya and Ghose, Joyeeta},
doi = {10.17605/OSF.IO/25GNZ},
keywords = {compassionate artificial intelligence,complex systems,data analytics,data science for developing nations,dynamical systems,empathetic artificial intelligence,humanized artificial intelligence,machine learning,open data science,teaching},
publisher = {OSF},
title = {{Teaching resources for data analytics in complex systems}},
url = {https://osf.io/25gnz/},
urldate = {2019-12-15},
year = {2019}
}
@article{Cook2019,
abstract = {Each year, countless hours of productive research time is spent brainstorming creative acronyms for surveys, simulations, codes, and conferences. We present ACRONYM, a command-line program developed specifically to assist astronomers in identifying the best acronyms for ongoing projects. The code returns all approximately-English-language words that appear within an input string of text, regardless of whether the letters occur at the beginning of the component words (in true astronomer fashion).},
archivePrefix = {arXiv},
arxivId = {1903.12180},
author = {Cook, B. A},
eprint = {1903.12180},
month = {mar},
title = {{ACRONYM: Acronym CReatiON for You and Me}},
url = {http://arxiv.org/abs/1903.12180},
year = {2019}
}
@article{Liberis2018,
abstract = {Antibodies play essential roles in the immune system of vertebrates and are powerful tools in research and diagnostics. While hypervariable regions of antibodies, which are responsible for binding, can be readily identified from their amino acid sequence, it remains challenging to accurately pinpoint which amino acids will be in contact with the antigen (the paratope).},
author = {Liberis, Edgar and Veli{\v{c}}kovi{\'{c}}, Petar and Sormanni, Pietro and Vendruscolo, Michele and Li{\`{o}}, Pietro},
doi = {10.1093/bioinformatics/bty305},
editor = {Hancock, John},
issn = {1367-4803},
journal = {Bioinformatics},
month = {sep},
number = {17},
pages = {2944--2950},
publisher = {Oxford University Press},
title = {{Parapred: antibody paratope prediction using convolutional and recurrent neural networks}},
url = {https://academic.oup.com/bioinformatics/article/34/17/2944/4972995 https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/bty305/4972995},
volume = {34},
year = {2018}
}
@article{Jolicoeur-Martineau2019,
abstract = {We generalize the concept of maximum-margin classifiers (MMCs) to arbitrary norms and non-linear functions. Support Vector Machines (SVMs) are a special case of MMC. We find that MMCs can be formulated as Integral Probability Metrics (IPMs) or classifiers with some form of gradient norm penalty. This implies a direct link to a class of Generative adversarial networks (GANs) which penalize a gradient norm. We show that the Discriminator in Wasserstein, Standard, Least-Squares, and Hinge GAN with Gradient Penalty is an MMC. We explain why maximizing a margin may be helpful in GANs. We hypothesize and confirm experimentally that {\$}L{\^{}}\backslashinfty{\$}-norm penalties with Hinge loss produce better GANs than {\$}L{\^{}}2{\$}-norm penalties (based on common evaluation metrics). We derive the margins of Relativistic paired (Rp) and average (Ra) GANs.},
archivePrefix = {arXiv},
arxivId = {1910.06922},
author = {Jolicoeur-Martineau, Alexia and Mitliagkas, Ioannis},
eprint = {1910.06922},
month = {oct},
title = {{Connections between Support Vector Machines, Wasserstein distance and gradient-penalty GANs}},
url = {http://arxiv.org/abs/1910.06922},
year = {2019}
}
@article{Zeiler2013,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky $\backslash$etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Zeiler, Matthew D and Fergus, Rob},
eprint = {1311.2901},
month = {nov},
title = {{Visualizing and Understanding Convolutional Networks}},
url = {http://arxiv.org/abs/1311.2901},
year = {2013}
}
@article{Le2015,
abstract = {Learning long term dependencies in recurrent networks is difficult due to vanishing and exploding gradients. To overcome this difficulty, researchers have developed sophisticated optimization techniques and network architectures. In this paper, we propose a simpler solution that use recurrent neural networks composed of rectified linear units. Key to our solution is the use of the identity matrix or its scaled version to initialize the recurrent weight matrix. We find that our solution is comparable to LSTM on our four benchmarks: two toy problems involving long-range temporal structures, a large language modeling problem and a benchmark speech recognition problem.},
archivePrefix = {arXiv},
arxivId = {1504.00941},
author = {Le, Quoc V. and Jaitly, Navdeep and Hinton, Geoffrey E.},
doi = {10.1109/72.279181},
eprint = {1504.00941},
isbn = {9781461268758},
issn = {1045-9227},
month = {apr},
pmid = {17756722},
title = {{A Simple Way to Initialize Recurrent Networks of Rectified Linear Units}},
url = {http://arxiv.org/abs/1504.00941},
year = {2015}
}
@article{Nelson2019,
abstract = {In order to advance precision medicine, detailed clinical features ought to be described in a way that leverages current knowledge. Although data collected from biomedical research is expanding at an almost exponential rate, our ability to transform that information into patient care has not kept at pace. A major barrier preventing this transformation is that multi-dimensional data collection and analysis is usually carried out without much understanding of the underlying knowledge structure. Here, in an effort to bridge this gap, Electronic Health Records (EHRs) of individual patients are connected to a heterogeneous knowledge network called Scalable Precision Medicine Oriented Knowledge Engine (SPOKE). Then an unsupervised machine-learning algorithm creates Propagated SPOKE Entry Vectors (PSEVs) that encode the importance of each SPOKE node for any code in the EHRs. We argue that these results, alongside the natural integration of PSEVs into any EHR machine-learning platform, provide a key step toward precision medicine.},
author = {Nelson, Charlotte A. and Butte, Atul J. and Baranzini, Sergio E.},
doi = {10.1038/s41467-019-11069-0},
issn = {2041-1723},
journal = {Nat. Commun.},
keywords = {Computational platforms and environments,Data integration,Machine learning,Predictive medicine},
month = {dec},
number = {1},
pages = {3045},
publisher = {Nature Publishing Group},
title = {{Integrating Biomedical Research and Electronic Health Records to Create Knowledge Based Biologically Meaningful Machine Readable Embeddings}},
url = {http://www.nature.com/articles/s41467-019-11069-0},
volume = {10},
year = {2019}
}
@book{Schank1977,
abstract = {Includes indexes.},
author = {Schank, Roger C. and Abelson, Robert P.},
isbn = {0898591384},
pages = {248},
publisher = {L. Erlbaum Associates},
title = {{Scripts, plans, goals, and understanding : an inquiry into human knowledge structures}},
year = {1977}
}
@article{Fries2019a,
abstract = {Population-scale biomedical repositories such as the UK Biobank provide unprecedented access to prospectively collected cardiac imaging data, however the majority of these data are unlabeled, creating barriers to their use in supervised machine learning. We developed a weakly supervised deep learning model for Bicuspid Aortic Valve (BAV) classification using up to 4,000 unlabeled cardiac MRI sequences. Instead of requiring curated, hand-labeled training data, weak supervision relies on noisy heuristics defined by domain experts to programmatically generate large-scale, imperfect training labels. For BAV classification, training models using these imperfect labels substantially outperformed a traditional supervised model trained on hand-labeled MRIs. In a validation experiment using long-term outcome data from the UK Biobank, our classification model identified a subset of individuals with a 1.8-fold increase in risk of a major adverse cardiac event. This work formalizes the first deep learning baseline for aortic valve classification and outlines a general strategy for using weak supervision to train machine learning models using large collections of unlabeled medical images.},
author = {Fries, Jason A. and Varma, Paroma and Chen, Vincent S. and Xiao, Ke and Tejeda, Heliodoro and Saha, Priyanka and Dunnmon, Jared and Chubb, Henry and Maskatia, Shiraz and Fiterau, Madalina and Delp, Scott and Ashley, Euan and R{\'{e}}, Christopher and Priest, James R.},
doi = {10.1038/s41467-019-11012-3},
issn = {2041-1723},
journal = {Nat. Commun.},
keywords = {Congenital heart defects,Machine learning,Magnetic resonance imaging},
month = {dec},
number = {1},
pages = {3111},
publisher = {Nature Publishing Group},
title = {{Weakly supervised classification of aortic valve malformations using unlabeled cardiac MRI sequences}},
url = {http://www.nature.com/articles/s41467-019-11012-3},
volume = {10},
year = {2019}
}
@incollection{Winograd2014,
abstract = {This chapter presents some criteria for evaluating ideas for representation. It also presents a rough sketch of a particular version of a frame representation, and discusses the ways in which it can deal with the issues raised. The proceduralists assert that human knowledge is primarily a knowing how. The human information processor is a stored program device, with its knowledge of the world embedded in the programs. The declarativists do not believe that knowledge of a subject is intimately bound with the procedures for its use. They see intelligence as resting on two bases: a quite general set of procedures for manipulating facts of all sorts, and a set of specific facts describing particular knowledge domains. In thinking, the general procedures are applied to the domain-specific data to make deductions. Often this process has been based on the model of axiomatic mathematics. The facts are axioms and the thought process involves proof procedures for drawing conclusions from them.},
author = {Winograd, Terry},
booktitle = {Represent. Underst.},
doi = {10.1016/b978-0-12-108550-6.50012-4},
isbn = {9780121085506},
month = {jan},
pages = {185--210},
publisher = {Morgan Kaufmann},
title = {{FRAME REPRESENTATIONS AND THE DECLARATIVE/PROCEDURAL CONTROVERSY}},
url = {https://www.sciencedirect.com/science/article/pii/B9780121085506500124},
year = {2014}
}
@inproceedings{Jin2019,
abstract = {Neural architecture search (NAS) has been proposed to automatically tune deep neural networks, but existing search algorithms, e.g., NASNet, PNAS, usually suffer from expensive computational cost. Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for NAS by enabling more efficient training during the search. In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search. The framework develops a neural network kernel and a tree-structured acquisition function optimization algorithm to efficiently explores the search space. Intensive experiments on real-world benchmark datasets have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods. Moreover, we build an open-source AutoML system based on our method, namely Auto-Keras. The system runs in parallel on CPU and GPU, with an adaptive search strategy for different GPU memory limits.},
author = {Jin, Haifeng and Song, Qingquan and Hu, Xian},
booktitle = {Proc. 25th ACM SIGKDD Int. Conf. Knowl. Discov. $\backslash${\&} Data Min.},
doi = {10.1145/3292500.3330648},
pages = {1946--1956},
publisher = {ACM},
title = {{Auto-Keras: An Efficient Neural Architecture Search System}},
url = {https://www.kdd.org/kdd2019/accepted-papers/view/auto-keras-an-efficient-neural-architecture-search-system},
year = {2019}
}
@book{Kolodner2014,
abstract = {11.2 Transformation. Case-based reasoning is one of the fastest growing areas in the field of knowledge-based systems and this book, authored by a leader in the field, is the first comprehensive text on the subject. Case-based reasoning systems are systems that store information about situations in their memory. As new problems arise, similar situations are searched out to help solve these problems. Problems are understood and inferences are made by finding the closest cases in memory, comparing and contrasting the problem with those cases, making inferences based on those comparisons, and asking questions whe. Front Cover; Case-Based Reasoning; Copyright Page; Table of Contents; Preface; Part I: Background; Chapter 1. What Is Case-Based Reasoning?; 1.1 Introduction; 1.2 What Is a Case?; 1.3 Major CBR Issues: Composition and Specificity; 1.4 Processes and Issues; 1.5 Applicability of Case-Based Reasoning; 1.6 Cognitive Model, or Methodology for Building Expert Systems?; 1.7 A Note to Readers; 1.8 Summary; Chapter 2. Case Studies of Several Case-Based Reasoners; 2.1 CHEF; 2.2 CASEY; 2.3 JULIA; 2.4 HYPO; 2.5 PROTOS; 2.6 CLAVIER; 2.7 Retrieval-Only Aiding and Advisory Systems; 2.8 Summary. Chapter 3. Reasoning Using Cases3.1 Case-Based Inference; 3.2 CBR and Problem Solving; 3.3 Interpretive CBR; 3.4 Case-Based and Other Reasoning Methods; 3.5 Summary; Chapter 4. The Cognitive Model; 4.1 A Short Intellectual History; 4.2 Dynamic Memory; 4.3 Beyond Intentional Situations: Dynamic Memory and Model-Based Reasoning; 4.4 Some Running Cognitive Models; 4.5 Summary of Claims; 4.6 Evidence of Case-Based Reasoning in People and Its Implications; Part II: The Case Library: Representing and Indexing Cases; Chapter 5. Representing Cases; 5.1 Component Parts of Cases. 5.2 The Issue of Case Presentation5.3 Case Studies; 5.4 Advanced Issues; 5.5 Summary; Chapter 6. Indexing Vocabulary; 6.1 Qualities of Good Indexes; 6.2 Choosing Vocabulary; 6.3 Toward a Generally Applicable Indexing Vocabulary; 6.4 The Universal Index Frame: A Vocabulary for Intentional Situations; 6.5 Generally Applicable Indexing Schemes: Lessons Illustrated by the UIF; 6.6 Beyond the Universal Index Frame; 6.7 Summary; Chapter 7. Methods for Index Selection; 7.1 Choosing Indexes by Hand; 7.2 Choosing Indexes by Machine; 7.3 Choosing Indexes Based on a Checklist. 7.4 Difference-Based Indexing7.5 Combining Difference-Based and Checklist-Based Methods; 7.6 Explanation-Based Indexing; 7.7 Combining Explanation-Based, Checklist-Based, and Difference-Based Methods; 7.8 Choosing an Automated Indexing Method; 7.9 Summary; Part III: Retrieving Cases from the Case Library; Chapter 8. Organizational Structures and Retrieval Algorithms; 8.1 A Note About Matching; 8.2 A Set of Cases; 8.3 Flat Memory, Serial Search; 8.4 Hierarchical Organizations of Cases: Shared Feature Networks; 8.5 Discrimination Networks; 8.6 A Major Disadvantage. 8.7 Redundant Discrimination Networks8.8 Flat Library, Parallel Search; 8.9 Hierarchical Memory, Parallel Search; 8.10 Discussion; 8.11 Summary; Chapter 9. Matching and Ranking Cases; 9.1 Some Definitions; 9.2 The Building Blocks of Matching and Ranking Processes; 9.3 Putting It All Together; 9.4 Summary; Chapter 10. Indexing and Retrieval; 10.1 Situation Assessment: Choosing Indexes for Retrieval; 10.2 Implementing Indexes; 10.3 Achieving Efficiency, Accuracy, and Flexibility; 10.4 Summary; Part IV: Using Cases; Chapter 11. Adaptation Methods and Strategies; 11.1 Substitution.},
author = {Kolodner, Janet.},
isbn = {9781483294490},
pages = {687},
publisher = {Elsevier Science},
title = {{Case-Based Reasoning.}},
year = {2014}
}
@inproceedings{Alaa2018,
abstract = {Clinical prognostic models derived from large- scale healthcare data can inform critical diagnostic and therapeutic decisions. To enable off-the- shelf usage of machine learning (ML) in prog-, nostic research, we developed AutoPrognosis: A system for automating the design of predic-tive modeling pipelines tailored for clinical prognosis. AutoPrognosis optimizes ensembles of pipeline configurations efficiently using a novel batched Baycsian optimization (BO) algorithm that learns a low-dimensional decomposition of the pipelines' high-dimensional hyperparametcr space in concurrence with the BO procedure. This is achieved by modeling the pipelines' per-' formances as a black-box function with a Gaussian process prior, and modeling the "similarities" between the pipelines' baseline algorithms via a sparse additive kernel with a Dirichlet prior. Meta-learning is used to warmslart BO with ex-, tcrnal data from "similar" patient cohorts by cali-1 brating the priors using an algorithm that mimics, the empirical Bayes method. The system automatically explains its predictions by presenting the clinicians with logical association rides that link patients' features to predicted risk strata. We demonstrate the utility of AutoPrognosis using 10 major patient cohorts representing various as-; pects of cardiovascular patient care.},
archivePrefix = {arXiv},
arxivId = {1802.07207},
author = {Alaa, Ahmed M. and {Van Der Schaar}, Mihaela},
booktitle = {35th Int. Conf. Mach. Learn. ICML 2018},
eprint = {1802.07207},
isbn = {9781510867963},
month = {feb},
pages = {223--234},
title = {{Autoprognosis: Automated clinical prognostic modeling via Bayesian optimization with structured kernel learning}},
url = {http://arxiv.org/abs/1802.07207},
volume = {1},
year = {2018}
}
@article{Kavalidou2019,
abstract = {Physical illness and mental disorders play a significant role in fatal and non-fatal suicidal behaviour. However, there is no clear evidence for the effect of physical and mental illness co-occurrence (multimorbidity) in suicidal ideation and attempts. The aim of the current study was to investigate, whether physical/mental health multimorbidity predicted suicidal thoughts and behaviours as outcomes. Data from the West of Scotland Twenty-07 cohort were analysed. Twenty-07 is a multiple cohort study following people for 20 years, through five waves of data collection. Participants who responded to past-year suicidal thoughts and suicide attempt items were grouped into four distinct health-groups based on having: (1) neither physical nor mental health condition (controls); (2) one or more physical health condition; (3) one or more mental health condition and; (4) multimorbidity. The role of multimorbidity in predicting suicidal ideation and suicide attempts was tested with a generalised estimating equation (GEE) model and odds ratios (ORs) and 95{\%} CIs are presented. Whether the effect of multimorbidity was stronger than either health condition alone was also assessed. Multimorbidity had a significant effect on suicidal thoughts and suicide attempts, compared to the control group, but was not found to increase the risk of either suicide-related outcomes, more than mental illness alone. We identified an effect of physical/mental multimorbidity on risk of suicidal thoughts and suicide attempts. Considering that suicide and related behaviour are rare events, future studies should employ a prospective design on the role of multimorbidity in suicidality, employing larger datasets.},
author = {Kavalidou{\textordfeminine}, Katerina and Smith, Daniel J. and Der, Geoff and O'Connor, Rory C.},
doi = {10.1186/s12888-019-2032-8},
issn = {1471-244X},
journal = {BMC Psychiatry},
keywords = {Psychiatry,Psychotherapy},
month = {dec},
number = {1},
pages = {38},
publisher = {BioMed Central},
title = {{The role of physical and mental multimorbidity in suicidal thoughts and behaviours in a Scottish population cohort study}},
url = {https://bmcpsychiatry.biomedcentral.com/articles/10.1186/s12888-019-2032-8},
volume = {19},
year = {2019}
}
@article{Cohen2020,
abstract = {Stimuli are represented in the brain by the collective population responses of sensory neurons, and an object presented under varying conditions gives rise to a collection of neural population responses called an ‘object manifold'. Changes in the object representation along a hierarchical sensory system are associated with changes in the geometry of those manifolds, and recent theoretical progress connects this geometry with ‘classification capacity', a quantitative measure of the ability to support object classification. Deep neural networks trained on object classification tasks are a natural testbed for the applicability of this relation. We show how classification capacity improves along the hierarchies of deep neural networks with different architectures. We demonstrate that changes in the geometry of the associated object manifolds underlie this improved capacity, and shed light on the functional roles different levels in the hierarchy play to achieve it, through orchestrated reduction of manifolds' radius, dimensionality and inter-manifold correlations.},
author = {Cohen, Uri and Chung, Sue Yeon and Lee, Daniel D. and Sompolinsky, Haim},
doi = {10.1038/s41467-020-14578-5},
issn = {20411723},
journal = {Nat. Commun.},
keywords = {Learning algorithms,Network models,Neural decoding,Neural encoding},
month = {dec},
number = {1},
pages = {1--13},
publisher = {Nature Research},
title = {{Separability and geometry of object manifolds in deep neural networks}},
volume = {11},
year = {2020}
}
@article{Graves2014a,
archivePrefix = {arXiv},
arxivId = {1410.5401},
author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
eprint = {1410.5401},
month = {oct},
title = {{Neural Turing Machines}},
url = {https://arxiv.org/abs/1410.5401},
year = {2014}
}
@article{Faul2017,
author = {Faul, Anita C. and Pilikos, Georgios},
doi = {10.5281/ZENODO.556502},
keywords = {Bayesian Inference,Confidence Measure,dating models,up},
month = {apr},
title = {{The Model is Simple, Until Proven Otherwise: How to Cope in an ever-changing world}},
url = {https://zenodo.org/record/556502{\#}.XOZAhshKhPY},
year = {2017}
}
@article{Berman2000,
author = {Berman, H. M. and Westbrook, John and Feng, Zukang and Gilliland, Gary and Bhat, T. N. and Weissig, Helge and Shindyalov, Ilya N. and Bourne, Philip E.},
doi = {10.1093/nar/28.1.235},
issn = {13624962},
journal = {Nucleic Acids Res.},
keywords = {protein data bank},
month = {jan},
number = {1},
pages = {235--242},
publisher = {Narnia},
title = {{The Protein Data Bank}},
url = {https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/28.1.235},
volume = {28},
year = {2000}
}
@article{Ma2018,
abstract = {Embedding a deep-learning model in the known structure of cellular systems yields DCell, a ‘visible' neural network that can be used to mechanistically interpret genotype–phenotype relationships.},
author = {Ma, Jianzhu and Yu, Michael Ku and Fong, Samson and Ono, Keiichiro and Sage, Eric and Demchak, Barry and Sharan, Roded and Ideker, Trey},
doi = {10.1038/nmeth.4627},
isbn = {1548-71051548-7105},
issn = {15487105},
journal = {Nat. Methods},
keywords = {Cell growth,Computational models,Genetic interaction},
month = {mar},
number = {4},
pages = {290--298},
pmid = {29505029},
publisher = {Nature Publishing Group},
title = {{Using deep learning to model the hierarchical structure and function of a cell}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.4627},
volume = {15},
year = {2018}
}
@article{Proust-Lima2017,
abstract = {The R package lcmm provides a series of functions to estimate statistical models based on linear mixed model theory. It includes the estimation of mixed models and latent class mixed models for Gaussian longitudinal outcomes (hlme), curvilinear and ordinal univariate longitudinal outcomes (lcmm) and curvilinear multivariate outcomes (multlcmm), as well as joint latent class mixed models (Jointlcmm) for a (Gaussian or curvilinear) longitudinal outcome and a time-to-event that can be possibly left-truncated right-censored and defined in a competing setting. Maximum likelihood esimators are obtained using a modified Marquardt algorithm with strict convergence criteria based on the parameters and likelihood stability, and on the negativity of the second derivatives. The package also provides various post-fit functions including goodness-of-fit analyses, classification, plots, predicted trajectories, individual dynamic prediction of the event and predictive accuracy assessment. This paper constitutes a companion paper to the package by introducing each family of models, the estimation technique, some implementation details and giving examples through a dataset on cognitive aging.},
archivePrefix = {arXiv},
arxivId = {1503.00890},
author = {Proust-Lima, C{\'{e}}cile and Philipps, Viviane and Liquet, Benoit},
doi = {10.18637/jss.v078.i02},
eprint = {1503.00890},
issn = {1548-7660},
journal = {J. Stat. Softw.},
keywords = {Fortran 90,R,curvilinearity,dynamic prediction,growth mixture model,joint model,psychometric tests},
month = {jun},
number = {2},
pages = {1--56},
title = {{Estimation of extended mixed models using latent classes and latent processes: the R package lcmm}},
url = {http://www.jstatsoft.org/v78/i02/ http://arxiv.org/abs/1503.00890{\%}0Ahttp://dx.doi.org/10.18637/jss.v078.i02},
volume = {78},
year = {2015}
}
@article{Beaulieu-Jones2016a,
abstract = {Patient interactions with health care providers result in entries to electronic health records (EHRs). EHRs were built for clinical and billing purposes but contain many data points about an individual. Mining these records provides opportunities to extract electronic phenotypes, which can be paired with genetic data to identify genes underlying common human diseases. This task remains challenging: high quality phenotyping is costly and requires physician review; many fields in the records are sparsely filled; and our definitions of diseases are continuing to improve over time. Here we develop and evaluate a semi-supervised learning method for EHR phenotype extraction using denoising autoencoders for phenotype stratification. By combining denoising autoencoders with random forests we find classification improvements across multiple simulation models and improved survival prediction in ALS clinical trial data. This is particularly evident in cases where only a small number of patients have high quality phenotypes, a common scenario in EHR-based research. Denoising autoencoders perform dimensionality reduction enabling visualization and clustering for the discovery of new subtypes of disease. This method represents a promising approach to clarify disease subtypes and improve genotype-phenotype association studies that leverage EHRs.},
author = {Beaulieu-Jones, Brett K. and Greene, Casey S.},
doi = {10.1016/j.jbi.2016.10.007},
issn = {15320464},
journal = {J. Biomed. Inform.},
keywords = {Denoising autoencoder,Disease subtyping,Electronic health record,Electronic phenotyping,Patient stratification,Unsupervised},
month = {dec},
pages = {168--178},
publisher = {Academic Press Inc.},
title = {{Semi-supervised learning of the electronic health record for phenotype stratification}},
volume = {64},
year = {2016}
}
@article{Glorot2010,
abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations functions. We ﬁnd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ﬁnd that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Glorot, Xavier and Bengio, Yoshua},
doi = {10.1.1.207.2059},
eprint = {arXiv:1011.1669v3},
isbn = {9781937284275},
issn = {15324435},
journal = {Proc. 13th Int. Conf. Artif. Intell. Stat.},
month = {mar},
pages = {249--256},
pmid = {25246403},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
url = {http://proceedings.mlr.press/v9/glorot10a.html http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS2010{\_}GlorotB10.pdf},
volume = {9},
year = {2010}
}
@article{Pani2014,
abstract = {The prevalence of CKD and of renal failure vary worldwide, yet parallel increases in leading risk factors explain only part of the differential prevalence. We measured CKD prevalence and eGFR, and their relationship with traditional and additional risk factors, in a Sardinian founder population cohort. The eGFR was calculated using equations from the CKD Epidemiology Collaboration and Modification of Diet in Renal Disease studies. With use of the Kidney Disease Improving Global Outcomes guidelines, a cross-sectional analysis of 4842 individuals showed that CKD prevalence was 15.1{\%}, including 3.6{\%} of patients in the high-risk and 0.46{\%} in the very-high-risk categories. Longitudinal analyses performed on 4074 of these individuals who completed three visits with an average follow-up of 7 years revealed that, consistent with other populations, average eGFR slope was -0.79 ml/min per 1.73 m(2) per year, but 11.4{\%} of the participants had an eGFR decline {\textgreater}2.3 ml/min per 1.73 m(2) per year (fast decline). A genetic score was generated from 13 reported eGFR- and CKD-related loci, and univariable and multivariable analyses were applied to assess the relationship between clinical, ultrasonographic, and genetic variables with three outcomes: CKD, change in eGFR, and fast eGFR decline. Genetic risk score, older age, and female sex independently correlated with each outcome. Diabetes was associated with CKD prevalence, whereas hypertension and hyperuricemia correlated more strongly with fast eGFR decline. Diabetes, hypertension, hyperuricemia, and high baseline eGFR were associated with a decline of eGFR. Along with differential health practices, population variations in this spectrum of risk factors probably contributes to the variable CKD prevalence worldwide.},
author = {Pani, Antonello and Bragg-Gresham, Jennifer and Masala, Marco and Piras, Doloretta and Atzeni, Alice and Pilia, Maria G. and Ferreli, Liana and Balaci, Lenuta and Curreli, Nicol{\`{o}} and Delitala, Alessandro and Loi, Francesco and Abecasis, Gon{\c{c}}alo R. and Schlessinger, David and Cucca, Francesco},
doi = {10.1681/asn.2013060591},
issn = {1046-6673},
journal = {J. Am. Soc. Nephrol.},
keywords = {chronic kidney disease,clinical epidemiology,ethnicity,glomerular filtration rate,human genetics,renal function decline},
month = {jul},
number = {7},
pages = {1533--1544},
pmid = {24511125},
title = {{Prevalence of CKD and Its Relationship to eGFR-Related Genetic Loci and Clinical Risk Factors in the SardiNIA Study Cohort}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24511125 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4073426 http://www.jasn.org/lookup/doi/10.1681/ASN.2013060591},
volume = {25},
year = {2014}
}
@article{Kipf2018,
abstract = {Interacting systems are prevalent in nature, from dynamical systems in physics to complex societal dynamics. The interplay of components can give rise to complex behavior, which can often be explained using a simple model of the system's constituent parts. In this work, we introduce the neural relational inference (NRI) model: an unsupervised model that learns to infer interactions while simultaneously learning the dynamics purely from observational data. Our model takes the form of a variational auto-encoder, in which the latent code represents the underlying interaction graph and the reconstruction is based on graph neural networks. In experiments on simulated physical systems, we show that our NRI model can accurately recover ground-truth interactions in an unsupervised manner. We further demonstrate that we can find an interpretable structure and predict complex dynamics in real motion capture and sports tracking data.},
archivePrefix = {arXiv},
arxivId = {1802.04687},
author = {Kipf, Thomas and Fetaya, Ethan and Wang, Kuan-Chieh and Welling, Max and Zemel, Richard},
eprint = {1802.04687},
month = {feb},
title = {{Neural Relational Inference for Interacting Systems}},
url = {http://arxiv.org/abs/1802.04687},
year = {2018}
}
@article{Pezzulo2016,
abstract = {It is widely assumed in developmental biology and bioengineering that optimal understanding and control of complex living systems follows from models of molecular events. The success of reductionism has overshadowed attempts at top-down models and control policies in biological systems. However, other fields, including physics, engineering and neuroscience, have successfully used the explanations and models at higher levels of organization, including least-action principles in physics and controltheoretic models in computational neuroscience. Exploiting the dynamic regulation of pattern formation in embryogenesis and regeneration requires new approaches to understand how cells cooperate towards large-scale anatomical goal states. Here, we argue that top-down models of pattern homeostasis serve as proof of principle for extending the current paradigm beyond emergence and molecule-level rules. We define top-down control in a biological context, discuss the examples of how cognitive neuroscience and physics exploit these strategies, and illustrate areas in which they may offer significant advantages as complements to the mainstream paradigm. By targeting system controls at multiple levels of organization and demystifying goal-directed (cybernetic) processes, top-down strategies represent a roadmap for using the deep insights of other fields for transformative advances in regenerative medicine and systems bioengineering.},
author = {Pezzulo, Giovanni and Levin, Michael},
doi = {10.1098/rsif.2016.0555},
issn = {17425662},
journal = {J. R. Soc. Interface},
keywords = {Cognitive modelling,Developmental biology,Integrative,Regeneration,Remodelling,Top-down},
month = {nov},
number = {124},
publisher = {Royal Society of London},
title = {{Top-down models in biology: Explanation and control of complex living systems above the molecular level}},
volume = {13},
year = {2016}
}
@article{Banerjee2017g,
abstract = {Understanding how quickly pathogens replicate and how quickly the immune system responds is important for predicting the epidemic spread of emerging pathogens. Host body size, through its correlation with metabolic rates, is theoretically predicted to impact pathogen replication rates and immune system response rates. Here, we use mathematical models of viral time courses from multiple species of birds infected by a generalist pathogen (West Nile Virus; WNV) to test more thoroughly how disease progression and immune response depend on mass and host phylogeny. We use hierarchical Bayesian models coupled with nonlinear dynamical models of disease dynamics to incorporate the hierarchical nature of host phylogeny. Our analysis suggests an important role for both host phylogeny and species mass in determining factors important for viral spread such as the basic reproductive number, WNV production rate, peak viraemia in blood and competency of a host to infect mosquitoes. Our model is based on a principled analysis and gives a quantitative prediction for key epidemiological determinants and how they vary with species mass and phylogeny. This leads to new hypotheses about the mechanisms that cause certain taxonomic groups to have higher viraemia. For example, our models suggest that higher viral burst sizes cause corvids to have higher levels of viraemia and that the cellular rate of virus production is lower in larger species. We derive a metric of competency of a host to infect disease vectors and thereby sustain the disease between hosts. This suggests that smaller passerine species are highly competent at spreading the disease compared with larger non-passerine species. Our models lend mechanistic insight into why some species (smaller passerine species) are pathogen reservoirs and some (larger non-passerine species) are potentially dead-end hosts for WNV. Our techniques give insights into the role of body mass and host phylogeny in the spread of WNV and potentially other zoonotic diseases. The major contribution of this work is a computational framework for infectious disease modelling at the within-host level that leverages data from multiple species. This is likely to be of interest to modellers of infectious diseases that jump species barriers and infect multiple species. Our method can be used to computationally determine the competency of a host to infect mosquitoes that will sustain WNV and other zoonotic diseases. We find that smaller passerine species are more competent in spreading the disease than larger non-passerine species. This suggests the role of host phylogeny as an important determinant of within-host pathogen replication. Ultimately, we view our work as an important step in linking within-host viral dynamics models to between-host models that determine spread of infectious disease between different hosts.},
author = {Banerjee, Soumya and Perelson, Alan S. and Moses, Melanie},
doi = {10.1098/RSIF.2017.0479},
issn = {1742-5689},
journal = {J. R. Soc. Interface},
month = {nov},
number = {136},
pages = {20170479},
publisher = {The Royal Society},
title = {{Modelling the effects of phylogeny and body size on within-host pathogen replication and immune response}},
url = {http://rsif.royalsocietypublishing.org/lookup/doi/10.1098/rsif.2017.0479 http://rsif.royalsocietypublishing.org/content/14/136/20170479},
volume = {14},
year = {2017}
}
@article{Gaujoux2012,
abstract = {Heterogeneity in sample composition is an inherent issue in many gene expression studies and, in many cases, should be taken into account in the downstream analysis to enable correct interpretation of the underlying biological processes. Typical examples are infectious diseases or immunology-related studies using blood samples, where, for example, the proportions of lymphocyte sub-populations are expected to vary between cases and controls.Nonnegative Matrix Factorization (NMF) is an unsupervised learning technique that has been applied successfully in several fields, notably in bioinformatics where its ability to extract meaningful information from high-dimensional data such as gene expression microarrays has been demonstrated. Very recently, it has been applied to biomarker discovery and gene expression deconvolution in heterogeneous tissue samples.Being essentially unsupervised, standard NMF methods are not guaranteed to find components corresponding to the cell types of interest in the sample, which may jeopardize the correct estimation of cell proportions. We have investigated the use of prior knowledge, in the form of a set of marker genes, to improve gene expression deconvolution with NMF algorithms. We found that this improves the consistency with which both cell type proportions and cell type gene expression signatures are estimated. The proposed method was tested on a microarray dataset consisting of pure cell types mixed in known proportions. Pearson correlation coefficients between true and estimated cell type proportions improved substantially (typically from about 0.5 to approximately 0.8) with the semi-supervised (marker-guided) versions of commonly used NMF algorithms. Furthermore known marker genes associated with each cell type were assigned to the correct cell type more frequently for the guided versions. We conclude that the use of marker genes improves the accuracy of gene expression deconvolution using NMF and suggest modifications to how the marker gene information is used that may lead to further improvements. {\textcopyright} 2011 Elsevier B.V.},
author = {Gaujoux, Renaud and Seoighe, Cathal},
doi = {10.1016/j.meegid.2011.08.014},
isbn = {1567-1348},
issn = {15671348},
journal = {Infect. Genet. Evol.},
keywords = {Deconvolution,Gene expression,Microarray,NMF,Sample heterogeneity},
month = {jul},
number = {5},
pages = {913--921},
pmid = {21930246},
publisher = {Elsevier},
title = {{Semi-supervised Nonnegative Matrix Factorization for gene expression deconvolution: A case study}},
url = {https://www.sciencedirect.com/science/article/pii/S1567134811002930},
volume = {12},
year = {2012}
}
@article{Maddison2016,
abstract = {The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.},
archivePrefix = {arXiv},
arxivId = {1611.00712},
author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
eprint = {1611.00712},
month = {nov},
title = {{The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables}},
url = {http://arxiv.org/abs/1611.00712},
year = {2016}
}
@article{Choi2019a,
abstract = {Effective modeling of electronic health records (EHR) is rapidly becoming an important topic in both academia and industry. A recent study showed that utilizing the graphical structure underlying EHR data (e.g. relationship between diagnoses and treatments) improves the performance of prediction tasks such as heart failure diagnosis prediction. However, EHR data do not always contain complete structure information. Moreover, when it comes to claims data, structure information is completely unavailable to begin with. Under such circumstances, can we still do better than just treating EHR data as a flat-structured bag-of-features? In this paper, we study the possibility of utilizing the implicit structure of EHR by using the Transformer for prediction tasks on EHR data. Specifically, we argue that the Transformer is a suitable model to learn the hidden EHR structure, and propose the Graph Convolutional Transformer, which uses data statistics to guide the structure learning process. Our model empirically demonstrated superior prediction performance to previous approaches on both synthetic data and publicly available EHR data on encounter-based prediction tasks such as graph reconstruction and readmission prediction, indicating that it can serve as an effective general-purpose representation learning algorithm for EHR data.},
archivePrefix = {arXiv},
arxivId = {1906.04716},
author = {Choi, Edward and Xu, Zhen and Li, Yujia and Dusenberry, Michael W. and Flores, Gerardo and Xue, Yuan and Dai, Andrew M.},
eprint = {1906.04716},
month = {jun},
title = {{Graph Convolutional Transformer: Learning the Graphical Structure of Electronic Health Records}},
url = {http://arxiv.org/abs/1906.04716},
year = {2019}
}
@article{Xu2018c,
abstract = {Graph Neural Networks (GNNs) for representation learning of graphs broadly follow a neighborhood aggregation framework, where the representation vector of a node is computed by recursively aggregating and transforming feature vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs in capturing different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
archivePrefix = {arXiv},
arxivId = {1810.00826},
author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
doi = {10.1111/j.1467-9868.2007.00607.x},
eprint = {1810.00826},
isbn = {1073-449X},
issn = {1467-9868},
month = {oct},
title = {{How Powerful are Graph Neural Networks?}},
url = {http://arxiv.org/abs/1810.00826},
year = {2018}
}
@article{Murphy2006,
abstract = {The Informatics for Integrating Biology and the Bedside (i2b2) is one of the sponsored initiatives of the NIH Roadmap National Centers for Biomedical Computing (http://www.bisti.nih.gov/ncbc/). One of the goals of i2b2 is to provide clinical investigators broadly with the software tools necessary to collect and manage project-related clinical research data in the genomics age as a cohesive entity - a software suite to construct and manage the modern clinical research chart.},
author = {Murphy, Shawn N and Mendis, Michael E and Berkowitz, David A and Kohane, Isaac and Chueh, Henry C},
doi = {85881 [pii]},
isbn = {1942-597X (Electronic)$\backslash$r1559-4076 (Linking)},
issn = {1942-597X},
journal = {AMIA Annu Symp Proc},
keywords = {*Software,Asthma,Biomedical Research,Computerized,Humans,Information Storage and Retrieval,Medical Records Systems,Systems Integration,User-Computer Interface},
number = {2},
pages = {1040},
pmid = {17238659},
publisher = {American Medical Informatics Association},
title = {{Integration of clinical and genetic data in the i2b2 architecture}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17238659 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC1839291 http://www.ncbi.nlm.nih.gov/pubmed/17238659{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pmc/articles/PMC1839291/pdf/AMIA2006{\_}1040.pdf},
volume = {2006},
year = {2006}
}
@misc{supervised_pca_package,
title = {{Superpc for R: Tutorial}},
url = {http://statweb.stanford.edu/{~}tibs/superpc/tutorial.html},
urldate = {2017-12-04}
}
@book{STOCK2018,
abstract = {4TH ed.},
author = {STOCK, JAMES H. and Watson, Mark W.},
isbn = {9780134611006},
publisher = {PEARSON},
title = {{INTRODUCTION TO ECONOMETRICS.}},
url = {https://www.pearson.com/us/higher-education/program/Stock-Introduction-to-Econometrics-Plus-My-Lab-Economics-with-Pearson-e-Text-Access-Card-Package-4th-Edition/PGM2416966.html},
year = {2018}
}
@article{Haslbeck2015,
abstract = {We present the R-package mgm for the estimation of k-order Mixed Graphical Models (MGMs) and mixed Vector Autoregressive (mVAR) models in high-dimensional data. These are a useful extensions of graphical models for only one variable type, since data sets consisting of mixed types of variables (continuous, count, categorical) are ubiquitous. In addition, we allow to relax the stationarity assumption of both models by introducing time-varying versions MGMs and mVAR models based on a kernel weighting approach. Time-varying models offer a rich description of temporally evolving systems and allow to identify external influences on the model structure such as the impact of interventions. We provide the background of all implemented methods and provide fully reproducible examples that illustrate how to use the package.},
archivePrefix = {arXiv},
arxivId = {1510.06871},
author = {Haslbeck, Jonas M. B. and Waldorp, Lourens J.},
eprint = {1510.06871},
month = {oct},
title = {{MGM: Estimating Time-Varying Mixed Graphical Models in High-Dimensional Data}},
url = {https://arxiv.org/abs/1510.06871 http://arxiv.org/abs/1510.06871},
year = {2015}
}
@article{Morton2019,
abstract = {Integrating multiomics datasets is critical for microbiome research; however, inferring interactions across omics datasets has multiple statistical challenges. We solve this problem by using neural networks ( https://github.com/biocore/mmvec ) to estimate the conditional probability that each molecule is present given the presence of a specific microorganism. We show with known environmental (desert soil biocrust wetting) and clinical (cystic fibrosis lung) examples, our ability to recover microbe–metabolite relationships, and demonstrate how the method can discover relationships between microbially produced metabolites and inflammatory bowel disease. mmvec, a neural-network-based algorithm, uses paired multiomics data (microbial sequence counts and metabolite abundances) to compute the conditional probability of observing a metabolite in the presence of a specific microorganism.},
author = {Morton, James T. and Aksenov, Alexander A. and Nothias, Louis Felix and Foulds, James R. and Quinn, Robert A. and Badri, Michelle H. and Swenson, Tami L. and {Van Goethem}, Marc W. and Northen, Trent R. and Vazquez-Baeza, Yoshiki and Wang, Mingxun and Bokulich, Nicholas A. and Watters, Aaron and Song, Se Jin and Bonneau, Richard and Dorrestein, Pieter C. and Knight, Rob},
doi = {10.1038/s41592-019-0616-3},
issn = {1548-7091},
journal = {Nat. Methods},
keywords = {Data integration,Machine learning,Metabolomics,Microbial communities},
month = {nov},
pages = {1--9},
publisher = {Nature Publishing Group},
title = {{Learning representations of microbe–metabolite interactions}},
url = {http://www.nature.com/articles/s41592-019-0616-3},
year = {2019}
}
@article{Saito2015,
abstract = {Binary classifiers are routinely evaluated with performance measures such as sensitivity and specificity, and performance is frequently illustrated with Receiver Operating Characteristics (ROC) plots. Alternative measures such as positive predictive value (PPV) and the associated Precision/Recall (PRC) plots are used less frequently. Many bioinformatics studies develop and evaluate classifiers that are to be applied to strongly imbalanced datasets in which the number of negatives outweighs the number of positives significantly. While ROC plots are visually appealing and provide an overview of a classifier's performance across a wide range of specificities, one can ask whether ROC plots could be misleading when applied in imbalanced classification scenarios. We show here that the visual interpretability of ROC plots in the context of imbalanced datasets can be deceptive with respect to conclusions about the reliability of classification performance, owing to an intuitive but wrong interpretation of specificity. PRC plots, on the other hand, can provide the viewer with an accurate prediction of future classification performance due to the fact that they evaluate the fraction of true positives among positive predictions. Our findings have potential implications for the interpretation of a large number of studies that use ROC plots on imbalanced datasets.},
author = {Saito, Takaya and Rehmsmeier, Marc},
doi = {10.1371/journal.pone.0118432},
isbn = {0022-1333 (Print)},
issn = {19326203},
journal = {PLoS One},
number = {3},
pages = {e0118432},
pmid = {25738806},
publisher = {Public Library of Science},
title = {{The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25738806 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4349800},
volume = {10},
year = {2015}
}
@article{Malhi2012,
abstract = {Ultimately, all therapeutic decisions involve balancing the potential clinical benefits of a drug against the risks that it might confer. In the management of bipolar disorder, trial data1 which have re-established the efficacy of lithium in prophylaxis have refocused attention on understanding its tolerability profile. Clinical interest in lithium has further heightened because the substantial risks that ensue from the metabolic syndrome have become apparent with newer alternatives, particularly atypical antipsychotics such as olanzapine.2 Hence, the importance of correctly judging the treatment options for bipolar disorder has never been more crucial.},
author = {Malhi, Gin S and Berk, Michael},
doi = {10.1016/S0140-6736(11)61703-0},
issn = {01406736},
journal = {Lancet},
month = {feb},
number = {9817},
pages = {690--692},
pmid = {22265701},
publisher = {Elsevier},
title = {{Is the safety of lithium no longer in the balance?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22265701},
volume = {379},
year = {2012}
}
@article{Ross2017,
abstract = {The R language has withstood the test of time. Forty years after it was initially developed (in the form of the S language) R is being used by millions of programmers on workflows the inventors of the language could never have imagined. Although base R packages perform well in most settings, workflows can be made more efficient by developing packages with more consistent arguments, inputs and outputs and emphasizing constantly improving code over historical code consistency. The universe of R packages known as the tidyverse, including dplyr, tidyr and others, aim to improve workflows and make data analysis as smooth as possible by applying a set of core programming principles in package development.},
author = {Ross, Zev and Wickham, Hadley and Robinson, David},
doi = {10.7287/peerj.preprints.3180v1},
issn = {2167-9843},
journal = {PeerJ Prepr.},
keywords = {R,base R,dplyr,ggplot2,pipe,piping,readr,tidy tools,tidyr,tidytext,tidyverse,workflow},
month = {aug},
pages = {e3180v1},
publisher = {PeerJ Inc.},
title = {{Declutter your R workflow with tidy tools}},
url = {https://peerj.com/preprints/3180/ https://doi.org/10.7287/peerj.preprints.3180v1},
volume = {5},
year = {2017}
}
@article{Kosiorek2019,
abstract = {An object can be seen as a geometrically organized set of interrelated parts. A system that makes explicit use of these geometric relationships to recognize objects should be naturally robust to changes in viewpoint, because the intrinsic geometric relationships are viewpoint-invariant. We describe an unsupervised version of capsule networks, in which a neural encoder, which looks at all of the parts, is used to infer the presence and poses of object capsules. The encoder is trained by backpropagating through a decoder, which predicts the pose of each already discovered part using a mixture of pose predictions. The parts are discovered directly from an image, in a similar manner, by using a neural encoder, which infers parts and their affine transformations. The corresponding decoder models each image pixel as a mixture of predictions made by affine-transformed parts. We learn object- and their part-capsules on unlabeled data, and then cluster the vectors of presences of object capsules. When told the names of these clusters, we achieve state-of-the-art results for unsupervised classification on SVHN (55{\%}) and near state-of-the-art on MNIST (98.5{\%}).},
archivePrefix = {arXiv},
arxivId = {1906.06818},
author = {Kosiorek, Adam R. and Sabour, Sara and Teh, Yee Whye and Hinton, Geoffrey E.},
eprint = {1906.06818},
month = {jun},
title = {{Stacked Capsule Autoencoders}},
url = {http://arxiv.org/abs/1906.06818},
year = {2019}
}
@article{Cardinal2015,
abstract = {BACKGROUND: The impact of psychotropic drug choice upon admissions for schizophrenia is not well understood. AIMS: To examine the association between antipsychotic/antidepressant use and time in hospital for patients with schizophrenia. METHODS: We conducted an observational study, using 8 years' admission records and electronically generated drug histories from an institution providing secondary mental health care in Cambridgeshire, UK, covering the period 2005-2012 inclusive. Patients with a coded ICD-10 diagnosis of schizophrenia were selected. The primary outcome measure was the time spent as an inpatient in a psychiatric unit. Antipsychotic and antidepressant drugs used by at least 5{\%} of patients overall were examined for associations with admissions. Periods before and after drug commencement were compared for patients having pre-drug admissions, in mirror-image analyses correcting for overall admission rates. Drug use in one 6-month calendar period was used to predict admissions in the next period, across all patients, in a regression analysis accounting for the effects of all other drugs studied and for time. RESULTS: In mirror-image analyses, sulpiride, aripiprazole, clozapine, and olanzapine were associated with fewer subsequent admission days. In regression analyses, sulpiride, mirtazapine, venlafaxine, and clozapine-aripiprazole and clozapine-amisulpride combinations were associated with fewer subsequent admission days. CONCLUSIONS: Use of these drugs was associated with fewer days in hospital. Causation is not implied and these findings require confirmation by randomized controlled trials.},
author = {Cardinal, Rudolf N and Savulich, George and Mann, Louisa M and Fern{\'{a}}ndez-Egea, Emilio},
doi = {10.1038/npjschz.2015.35},
isbn = {2334-265X},
issn = {2334-265X},
journal = {npj Schizophr.},
month = {dec},
number = {1},
pages = {15035},
pmid = {27336041},
title = {{Association between antipsychotic/antidepressant drug treatments and hospital admissions in schizophrenia assessed using a mental health case register}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27336041 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4849458 http://www.nature.com/articles/npjschz201535},
volume = {1},
year = {2015}
}
@article{Battaglia2018,
abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
archivePrefix = {arXiv},
arxivId = {1806.01261},
author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
eprint = {1806.01261},
month = {jun},
title = {{Relational inductive biases, deep learning, and graph networks}},
url = {http://arxiv.org/abs/1806.01261},
year = {2018}
}
@article{Ross2017a,
abstract = {Neural networks often perform better on prediction problems than simpler classes ofmodels, but their behavior is difficult to explain. This makes it challenging to trusttheir predictions in safety critical domains. Recent work has focused on explainingtheir predictions using local linear approximations [1,11], but these explanationscan be complex when they depend on many features and it is unclear if they canbe used to understand global trends in model behavior. In this work, we trainneural networks to have sparse local explanations by applying L1 penalties to theirinput gradients. We show explanations of these networks depend on fewer inputswhile their performance remains comparable across datasets and architectures. Weillustrate how our approach encourages a different kind of sparsity than L1 weightdecay. In a case study with ICU data, we observe that gradients vary smoothly overthe input space, which suggests they can be used to gain insight into the globalbehavior of the model.},
author = {Ross, Andrew Slavin and Lage, Erika and Doshi-Velez, Finale},
journal = {NIPS},
number = {Nips},
title = {{The Neural LASSO : Local Linear Sparsity for Interpretable Explanations}},
url = {https://www.semanticscholar.org/paper/The-Neural-LASSO-{\%}3A-Local-Linear-Sparsity-for-Slavin/a76228a7a5ecf61d9fbb16a1e1c354e265767e53},
year = {2017}
}
@techreport{Ball2019,
abstract = {In recent years, we have seen an expansion of patient and public involvement (PPI) activities in research. This has been accompanied by a growing interest in understanding how PPI can best be mobilised and enabled, and in how it can contribute positively to the research process and to impacts from it. The body of literature on these issues has expanded rapidly in the last decade. However, the evidence base on what works, how and why remains fragmented and inconclusive. RAND Europe was commissioned by The Healthcare Improvement Studies (THIS) Institute at the University of Cambridge to conduct a rapid review of the evidence base on PPI in research. This report reflects on what we know and on knowledge gaps. It aims to help inform THIS Institute's efforts to establish and implement an effective PPI strategy. It should also be of relevance to other organisations and initiatives seeking to involve patients and the public in research in a meaningful and effective way. Based on a rapid evidence assessment and interviews with experts, the report examines why and how patients and the public get involved with research, what enables meaningful involvement, associated challenges and potential enabling mechanisms, the impact of PPI, and the evaluation of this activity. Based on these insights, we provide a series of recommendations for THIS Institute and other organisations to inform strategies for engaging patients and the public.},
author = {Ball, Sarah and Harshfield, Amelia and Carpenter, Asha and Bertscher, Adam and Marjanovic, Sonja},
booktitle = {RAND Corp.},
doi = {10.7249/rr2678},
institution = {RAND Corporation},
month = {jun},
publisher = {RAND Corporation},
title = {{Patient and public involvement in research: Enabling meaningful contributions}},
url = {https://www.rand.org/pubs/research{\_}reports/RR2678.html},
year = {2019}
}
@misc{Banerjee2020,
author = {Banerjee, Soumya},
doi = {10.5281/ZENODO.3621363},
month = {jan},
title = {{neelsoumya/butterfly{\_}detector: Open source teaching materials}},
year = {2020}
}
@inproceedings{Radul2009,
abstract = {We develop a programming model built on the idea that the basic computational elements are autonomous machines interconnected by shared cells through which they communicate. Each machine continuously examines the cells it is interested in, and adds information to some based on deductions it can make from information from the others. This model makes it easy to smoothly combine expression-oriented and constraint-based programming; it also easily accommodates implicit incremental distributed search in ordinary programs. This work builds on the original research of Guy Lewis Steele Jr. [19] and was developed more recently with the help of Chris Hanson. 0 This document, together with complete supporting code, is available as an MIT CSAIL Technical Report via http://dspace.mit.edu.},
author = {Radul, Alexey and Sussman, Gerald Jay},
booktitle = {ILC2009 Proc. Int. Lisp Conf.},
pages = {41--56},
title = {{The art of the propagator}},
url = {www.csail.mit.edu},
year = {2009}
}
@article{Nguyen2017,
author = {Nguyen, Tin and Tagett, Rebecca and Diaz, Diana and Draghici, Sorin},
doi = {10.1101/GR.215129.116},
issn = {1088-9051},
journal = {Genome Res.},
pages = {gr.215129.116},
pmid = {29066617},
title = {{A novel approach for data integration and disease subtyping}},
url = {http://genome.cshlp.org/content/early/2017/10/24/gr.215129.116.abstract?papetoc},
year = {2017}
}
@article{Haden2020,
abstract = {OBJECTIVE In academic settings around the world, there is a resurgence of interest in using psychedelic substances for the treatment of addictions, posttraumatic stress disorder, depression, anxiety, and other diagnoses. This case series describes the medical consequences of accidental overdoses in three individuals. METHOD Case series of information were gathered from interviews, health records, case notes, and collateral reports. RESULTS The first case report documents significant improvements in mood symptoms, including reductions in mania with psychotic features, following an accidental lysergic acid diethylamide (LSD) overdose, changes that have been sustained for almost 20 years. The second case documents how an accidental overdose of LSD early in the first trimester of pregnancy did not negatively affect the course of the pregnancy or have any obvious teratogenic or other negative developmental effects on the child. The third report indicates that intranasal ingestion of 550 times the normal recreational dosage of LSD was not fatal and had positive effects on pain levels and subsequent morphine withdrawal. CONCLUSIONS There appear to be unpredictable, positive sequelae that ranged from improvements in mental illness symptoms to reduction in physical pain and morphine withdrawal symptoms. Also, an LSD overdose while in early pregnancy did not appear to cause harm to the fetus.},
author = {Haden, Mark and Woods, Birgitta},
issn = {1938-4114},
journal = {J. Stud. Alcohol Drugs},
month = {jan},
number = {1},
pages = {115--118},
pmid = {32048609},
title = {{LSD Overdoses: Three Case Reports.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/32048609},
volume = {81},
year = {2020}
}
@article{Breen2016,
abstract = {Lithium (Li) is the mainstay mood stabilizer for the treatment of bipolar disorder (BD), although its mode of action is not yet fully understood nor is it effective in every patient. We sought to elucidate the mechanism of action of Li and to identify surrogate outcome markers that can be used to better understand its therapeutic effects in BD patients classified as good (responders) and poor responders (nonresponders) to Li treatment. To accomplish these goals, RNA-sequencing gene expression profiles of lymphoblastoid cell lines (LCLs) were compared between BD Li responders and nonresponders with healthy controls before and after treatment. Several Li-responsive gene coexpression networks were discovered indicating widespread effects of Li on diverse cellular signaling systems including apoptosis and defense response pathways, protein processing and response to endoplasmic reticulum stress. Individual gene markers were also identified, differing in response to Li between BD responders and nonresponders, involved in processes of cell cycle and nucleotide excision repair that may explain part of the heterogeneity in clinical response to treatment. Results further indicated a Li gene expression signature similar to that observed with clonidine treatment, an $\alpha$2-adrenoceptor agonist. These findings provide a detailed mechanism of Li in LCLs and highlight putative surrogate outcome markers that may permit for advanced treatment decisions to be made and for facilitating recovery in BD patients.The Pharmacogenomics Journal advance online publication, 12 July 2016; doi:10.1038/tpj.2016.50.},
author = {Breen, M. S. and White, C. H. and Shekhtman, T. and Lin, K. and Looney, D. and Woelk, C. H. and Kelsoe, J. R.},
doi = {10.1038/tpj.2016.50},
issn = {14731150},
journal = {Pharmacogenomics J.},
number = {5},
pages = {446--453},
title = {{Lithium-responsive genes and gene networks in bipolar disorder patient-derived lymphoblastoid cell lines}},
volume = {16},
year = {2016}
}
@article{Banerjee2016d,
abstract = {We inhabit a world that is not only small but supports efficient decentralized search - an individual using local information can establish a line of communication with another completely unknown individual. Here we augment a hierarchical social network model with communication between and within communities. We argue that organization into communities would decrease overall decentralized search times. We take inspiration from the biological immune system which organizes search for pathogens in a hybrid modular strategy. Our strategy has relevance in search for rare amounts of information in online social networks. Our work also has implications for design of efficient online networks that could have an impact on networks of human collaboration, scientific collaboration and networks used in targeted manhunts. Real world systems, like online social networks, have high associated delays for long-distance links, since they are built on top of physical networks. Such systems have been shown to densify. Hence such networks will have a communication cost due to space and the requirement of maintaining connections. We have incorporated such a non-spatial cost to communication. We introduce the notion of a community size that increases with the size of the system, which is shown to reduce the time to search for information in networks. Our final strategy balances search times and participation costs and is shown to decrease time to find information in decentralized search in online social networks. Our strategy also balances strong-ties and weak-ties over long distances and may ultimately lead to more productive and innovative networks of human communication and enterprise. We hope that this work will lay the foundation for strategies aimed at producing global scale human interaction networks that are sustainable and lead to a more networked, diverse and prosperous society.},
archivePrefix = {arXiv},
arxivId = {1601.08021},
author = {Banerjee, Soumya},
doi = {10.7906/indecs.14.1.2},
eprint = {1601.08021},
issn = {1334-4676},
journal = {Interdiscip. Descr. Complex Syst.},
month = {jan},
number = {1},
pages = {10--22},
title = {{A Biologically Inspired Model of Distributed Online Communication Supporting Efficient Search and Diffusion of Innovation}},
url = {http://arxiv.org/abs/1601.08021},
volume = {14},
year = {2016}
}
@misc{linear_logic_tutorials,
title = {{Linear logic tutorials}},
url = {http://www.dicosmo.org/CourseNotes/LinLog/},
urldate = {2019-01-30}
}
@article{ODonnell2007,
abstract = {For nearly as long as lithium has been in clinical use for the treatment of bipolar disorder, depression, and other conditions, investigators have attempted to characterize its effects on behaviors in rodents. Lithium consistently decreases exploratory activity, rearing, aggression, and amphetamine-induced hyperlocomotion; and it increases the sensitivity to pilocarpine-induced seizures, decreases immobility time in the forced swim test, and attenuates reserpine-induced hypolocomotion. Lithium also predictably induces conditioned taste aversion and alterations in circadian rhythms. The modulation of stereotypy, sensitization, and reward behavior are less consistent actions of the drug. These behavioral models may be relevant to human symptoms and to clinical endophenotypes. It is likely that the actions of lithium in a subset of these animal models are related to the therapeutic efficacy, as well the side effects, of the drug. We conclude with a brief discussion of various molecular mechanisms by which these lithium-sensitive behaviors may be mediated, and comment on the ways in which rat and mouse models can be used more effectively in the future to address persistent questions about the therapeutically relevant molecular actions of lithium. {\textcopyright} 2007 Elsevier Ltd. All rights reserved.},
author = {O'Donnell, Kelley C and Gould, Todd D},
doi = {10.1016/j.neubiorev.2007.04.002},
issn = {01497634},
journal = {Neurosci. Biobehav. Rev.},
keywords = {Animal model,Antidepressant,Bipolar disorder,Depression,Endophenotype,Mania,Manic-depressive illness,Mood stabilizer,Mouse,Rat},
number = {6},
pages = {932--962},
pmid = {17532044},
publisher = {NIH Public Access},
title = {{The behavioral actions of lithium in rodent models: Leads to develop novel therapeutics}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17532044 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2150568},
volume = {31},
year = {2007}
}
@article{Winter2013a,
archivePrefix = {arXiv},
arxivId = {1308.5499},
author = {Winter, Bodo},
eprint = {1308.5499},
month = {aug},
title = {{Linear models and linear mixed effects models in R with linguistic applications}},
url = {https://arxiv.org/abs/1308.5499},
year = {2013}
}
@incollection{Shams2018,
author = {Shams, Zohreh and Sato, Yuri and Jamnik, Mateja and Stapleton, Gem},
doi = {10.1007/978-3-319-91376-6_25},
month = {jun},
pages = {247--263},
publisher = {Springer, Cham},
title = {{Accessible Reasoning with Diagrams: From Cognition to Automation}},
url = {http://link.springer.com/10.1007/978-3-319-91376-6{\_}25},
year = {2018}
}
@article{Dacrema2019,
abstract = {Deep learning techniques have become the method of choice for researchers working on algorithmic aspects of recommender systems. With the strongly increased interest in machine learning in general, it has, as a result, become difficult to keep track of what represents the state-of-the-art at the moment, e.g., for top-n recommendation tasks. At the same time, several recent publications point out problems in today's research practice in applied machine learning, e.g., in terms of the reproducibility of the results or the choice of the baselines when proposing new models. In this work, we report the results of a systematic analysis of algorithmic proposals for top-n recommendation tasks. Specifically, we considered 18 algorithms that were presented at top-level research conferences in the last years. Only 7 of them could be reproduced with reasonable effort. For these methods, it however turned out that 6 of them can often be outperformed with comparably simple heuristic methods, e.g., based on nearest-neighbor or graph-based techniques. The remaining one clearly outperformed the baselines but did not consistently outperform a well-tuned non-neural linear ranking method. Overall, our work sheds light on a number of potential problems in today's machine learning scholarship and calls for improved scientific practices in this area. Source code of our experiments and full results are available at: https://github.com/MaurizioFD/RecSys2019{\_}DeepLearning{\_}Evaluation.},
archivePrefix = {arXiv},
arxivId = {1907.06902},
author = {Dacrema, Maurizio Ferrari and Cremonesi, Paolo and Jannach, Dietmar},
doi = {10.1145/3298689.3347058},
eprint = {1907.06902},
month = {jul},
title = {{Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches}},
url = {http://arxiv.org/abs/1907.06902 http://dx.doi.org/10.1145/3298689.3347058 http://arxiv.org/abs/1907.06902{\%}0Ahttp://dx.doi.org/10.1145/3298689.3347058},
year = {2019}
}
@article{Olson2017,
abstract = {While artificial intelligence (AI) has become widespread, many commercial AI systems are not yet accessible to individual researchers nor the general public due to the deep knowledge of the systems required to use them. We believe that AI has matured to the point where it should be an accessible technology for everyone. We present an ongoing project whose ultimate goal is to deliver an open source, user-friendly AI system that is specialized for machine learning analysis of complex data in the biomedical and health care domains. We discuss how genetic programming can aid in this endeavor, and highlight specific examples where genetic programming has automated machine learning analyses in previous projects.},
archivePrefix = {arXiv},
arxivId = {1705.00594},
author = {Olson, Randal S. and Sipper, Moshe and {La Cava}, William and Tartarone, Sharon and Vitale, Steven and Fu, Weixuan and Orzechowski, Patryk and Urbanowicz, Ryan J. and Holmes, John H. and Moore, Jason H.},
eprint = {1705.00594},
isbn = {9783319905129},
month = {may},
title = {{A System for Accessible Artificial Intelligence}},
url = {http://arxiv.org/abs/1705.00594},
year = {2017}
}
@article{Taylor2018,
abstract = {There are a variety of challenges that come with producing a large number of forecasts across a variety time series. Our approach to fore-casting at scale is a combination of configurable models and thorough analyst-in-the-loop performance analysis. We present a forecasting ap-proach based on a decomposable model with interpretable parameters that can be intuitively adjusted by the analyst. We describe performance analysis that we use compare and evaluate forecasting procedures, as well as automatically flag forecasts for manual review and adjustment. Tools that help analysts to use their expertise most effectively enable reliable forecasting of a large variety of business time series.},
author = {Taylor, Sean J. and Letham, Benjamin},
doi = {10.1080/00031305.2017.1380080},
issn = {15372731},
journal = {Am. Stat.},
keywords = {Nonlinear regression,Statistical practice,Time series},
month = {jan},
number = {1},
pages = {37--45},
publisher = {Taylor {\&} Francis},
title = {{Forecasting at Scale}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1380080},
volume = {72},
year = {2018}
}
@article{Ritter2014,
abstract = {We describe the R package multiPIM, including statistical background, functionality and user options. The package is for variable importance analysis, and is meant primarily for analyzing data from exploratory epidemiological studies, though it could certainly be applied in other areas as well. The approach taken to variable importance comes from the causal inference field, and is different from approaches taken in other R packages. By default, multiPIM uses a double robust targeted maximum likelihood estimator (TMLE) of a parameter akin to the attributable risk. Several regression methods/machine learning algorithms are available for estimating the nuisance parameters of the models, including super learner, a meta-learner which combines several different algorithms into one. We describe a simulation in which the double robust TMLE is compared to the graphical computation estimator. We also provide example analyses using two data sets which are included with the package.},
author = {Ritter, Stephan J. and Jewell, Nicholas P. and Hubbard, Alan E.},
doi = {10.18637/jss.v057.i08},
issn = {1548-7660},
journal = {J. Stat. Softw.},
month = {apr},
number = {8},
pages = {1--29},
title = {{R Package multiPIM : A Causal Inference Approach to Variable Importance Analysis}},
url = {http://www.jstatsoft.org/v57/i08/},
volume = {57},
year = {2015}
}
@article{Shoemaker2012,
abstract = {BACKGROUND: Interpreting in vivo sampled microarray data is often complicated by changes in the cell population demographics. To put gene expression into its proper biological context, it is necessary to distinguish differential gene transcription from artificial gene expression induced by changes in the cellular demographics.$\backslash$n$\backslash$nRESULTS: CTen (cell type enrichment) is a web-based analytical tool which uses our highly expressed, cell specific (HECS) gene database to identify enriched cell types in heterogeneous microarray data. The web interface is designed for differential expression and gene clustering studies, and the enrichment results are presented as heatmaps or downloadable text files.$\backslash$n$\backslash$nCONCLUSIONS: In this work, we use an independent, cell-specific gene expression data set to assess CTen's performance in accurately identifying the appropriate cell type and provide insight into the suggested level of enrichment to optimally minimize the number of false discoveries. We show that CTen, when applied to microarray data developed from infected lung tissue, can correctly identify the cell signatures of key lymphocytes in a highly heterogeneous environment and compare its performance to another popular bioinformatics tool. Furthermore, we discuss the strong implications cell type enrichment has in the design of effective microarray workflow strategies and show that, by combining CTen with gene expression clustering, we may be able to determine the relative changes in the number of key cell types.CTen is available at http://www.influenza-x.org/{\~{}}jshoemaker/cten/},
author = {Shoemaker, Jason E and Lopes, Tiago J S and Ghosh, Samik and Matsuoka, Yukiko and Kawaoka, Yoshihiro and Kitano, Hiroaki},
doi = {10.1186/1471-2164-13-460},
isbn = {1471-2164 (Electronic)$\backslash$r1471-2164 (Linking)},
issn = {14712164},
journal = {BMC Genomics},
keywords = {Cell type enrichment,Deconvolution,Influenza,Microarray data,Systems immunology},
month = {sep},
number = {1},
pages = {460},
pmid = {22953731},
publisher = {BioMed Central},
title = {{CTen: a web-based platform for identifying enriched cell types from heterogeneous microarray data}},
url = {http://bmcgenomics.biomedcentral.com/articles/10.1186/1471-2164-13-460},
volume = {13},
year = {2012}
}
@article{Kucukelbir2017,
abstract = {Probabilistic modeling is iterative. A scientist posits a simple model, fits it to her data, refines it according to her analysis, and repeats. However, fitting complex models to large data is a bottleneck in this process. Deriving algorithms for new models can be both mathematically and computationally challenging, which makes it difficult to efficiently cycle through the steps. To this end, we develop automatic differentiation variational inference (ADVI). Using our method, the scientist only provides a probabilistic model and a dataset, nothing else. ADVI automatically derives an efficient variational inference algorithm, freeing the scientist to refine and explore many models. ADVI supports a broad class of models-no conjugacy assumptions are required. We study ADVI across ten different models and apply it to a dataset with millions of observations. ADVI is integrated into Stan, a probabilistic programming system; it is available for immediate use.},
archivePrefix = {arXiv},
arxivId = {1603.00788},
author = {Kucukelbir, Alp and Tran, Dustin and Ranganath, Rajesh and Gelman, Andrew and Blei, David M.},
doi = {10.3847/0004-637X/819/1/50},
eprint = {1603.00788},
isbn = {1603.00788},
issn = {15337928},
journal = {J. Mach. Learn. Res.},
number = {14},
pages = {1--45},
title = {{Automatic Differentiation Variational Inference}},
url = {http://www.jmlr.org/papers/v18/16-107.html http://arxiv.org/abs/1603.00788},
volume = {18},
year = {2016}
}
@misc{Bullmore2009a,
abstract = {Recent developments in the quantitative analysis of complex networks, based largely on graph theory, have been rapidly translated to studies of brain network organization. The brain's structural and functional systems have features of complex networks--such as small-world topology, highly connected hubs and modularity--both at the whole-brain scale of human neuroimaging and at a cellular scale in non-human animals. In this article, we review studies investigating complex brain networks in diverse experimental modalities (including structural and functional MRI, diffusion tensor imaging, magnetoencephalography and electroencephalography in humans) and provide an accessible introduction to the basic principles of graph theory. We also highlight some of the technical challenges and key questions to be addressed by future developments in this rapidly moving field.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bullmore, Ed and Sporns, Olaf},
booktitle = {Nat. Rev. Neurosci.},
doi = {10.1038/nrn2575},
eprint = {arXiv:1011.1669v3},
isbn = {1471-0048 (Electronic)},
issn = {1471003X},
month = {mar},
number = {3},
pages = {186--198},
pmid = {19190637},
publisher = {Nature Publishing Group},
title = {{Complex brain networks: Graph theoretical analysis of structural and functional systems}},
url = {http://www.nature.com/articles/nrn2575},
volume = {10},
year = {2009}
}
@article{Via2019,
author = {Via, Allegra and Attwood, Teresa K and Fernandes, Pedro L and Morgan, Sarah L and Schneider, Maria Victoria and Palagi, Patricia M and Rustici, Gabriella and Tractenberg, Rochelle E},
doi = {10.1093/BIB},
journal = {Brief. Bioinform.},
month = {mar},
number = {2},
pages = {405--415},
publisher = {Oxford Academic},
title = {{Imported from http://m.bib.oxfordjournals.org/content/early/2017/01/14/bib.bbw134.long}},
url = {http://m.bib.oxfordjournals.org/content/early/2017/01/14/bib.bbw134.long},
volume = {20},
year = {2019}
}
@book{Barr1981,
abstract = {Artificial intelligence (AI) is a relatively young discipline, yet it has already led to general-purpose problem-solving methods and novel applications. Ultimately, AI's goals of creating models and mechanisms of intelligent action can be realized only in the broader context of computer science. Creating mechanisms for sharing of knowledge, knowhow, and literacy is the challenge. The great Chinese philosopher Kuan-Tzu once said: {\&}ldquo;If you give a fish to a man, you will feed him for a day. If you give him a fishing rod, you will feed him for life.{\&}rdquo; We can go one step further: If we can provide him with the knowledge and the know-how for making that fishing rod, we can feed the whole village. Therein lies the promise-and the challenge-of AI},
author = {Barr, Avron and Feigenbaum, E.},
doi = {10.2307/3680201},
isbn = {9780865760899},
issn = {01489267},
number = {3},
publisher = {HeurisTech Press},
title = {{The Handbook of Artificial Intelligence}},
volume = {6},
year = {1981}
}
@inproceedings{Curtis2005,
abstract = {Topics: flexibility supported by discourse/user model, new types of$\backslash$nquestions, reasoning with incomplete knowledge, response generation.$\backslash$nWe describe a commercial question-answering system that uses AI –$\backslash$nspecifically, the Cyc system – to support passage retrieval and perform$\backslash$ndeductive QA, to produce results superior to what each question-answering$\backslash$ntechnique could produce alone.},
author = {Curtis, Jon and Matthews, Gavin and Baxter, David},
booktitle = {Proc. Knowl. Reason. Answering Quest. Work. IJCAI 2005},
pages = {61--70},
title = {{On the Effective Use of Cyc in a Question Answering System}},
url = {https://www.semanticscholar.org/paper/On-the-Effective-Use-of-Cyc-in-a-Question-Answering-Curtis-Matthews/37e0f5183ac14e2c55dfeb735794ac956b8c247c},
year = {2005}
}
@article{Spelke1992,
abstract = {Experiments with young infants provide evidence for early-developing capacities to represent physical objects and to reason about object motion. Early physical reasoning accords with 2 constraints at the center of mature physical conceptions: continuity and solidity. It fails to accord with 2 constraints that may be peripheral to mature conceptions: gravity and inertia. These experiments suggest that cognition develops concurrently with perception and action and that development leads to the enrichment of conceptions around an unchanging core. The experiments challenge claims that cognition develops on a foundation of perceptual or motor experience, that initial conceptions are inappropriate to the world, and that initial conceptions are abandoned or radically changed with the growth of knowledge.},
author = {Spelke, Elizabeth S. and Breinlinger, Karen and Macomber, Janet and Jacobson, Kristen},
doi = {10.1037/0033-295X.99.4.605},
issn = {0033295X},
journal = {Psychol. Rev.},
number = {4},
pages = {605--632},
title = {{Origins of Knowledge}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.99.4.605},
volume = {99},
year = {1992}
}
@article{Madiraju2018,
abstract = {Unsupervised learning of time series data, also known as temporal clustering, is a challenging problem in machine learning. Here we propose a novel algorithm, Deep Temporal Clustering (DTC), to naturally integrate dimensionality reduction and temporal clustering into a single end-to-end learning framework, fully unsupervised. The algorithm utilizes an autoencoder for temporal dimensionality reduction and a novel temporal clustering layer for cluster assignment. Then it jointly optimizes the clustering objective and the dimensionality reduction objec tive. Based on requirement and application, the temporal clustering layer can be customized with any temporal similarity metric. Several similarity metrics and state-of-the-art algorithms are considered and compared. To gain insight into temporal features that the network has learned for its clustering, we apply a visualization method that generates a region of interest heatmap for the time series. The viability of the algorithm is demonstrated using time series data from diverse domains, ranging from earthquakes to spacecraft sensor data. In each case, we show that the proposed algorithm outperforms traditional methods. The superior performance is attributed to the fully integrated temporal dimensionality reduction and clustering criterion.},
archivePrefix = {arXiv},
arxivId = {1802.01059},
author = {Madiraju, Naveen Sai and Sadat, Seid M. and Fisher, Dimitry and Karimabadi, Homa},
eprint = {1802.01059},
isbn = {9780438287204},
month = {feb},
title = {{Deep Temporal Clustering : Fully Unsupervised Learning of Time-Domain Features}},
url = {http://arxiv.org/abs/1802.01059},
year = {2018}
}
@article{Dhanda2019,
abstract = {The Immune Epitope Database Analysis Resource (IEDB-AR, http://tools.iedb.org/) is a companion website to the IEDB that provides computational tools focused on the prediction and analysis of B and T cell epitopes. All of the tools are freely available through the public website and many are also available through a REST API and/or a downloadable command-line tool. A virtual machine image of the entire site is also freely available for non-commercial use and contains most of the tools on the public site. Here, we describe the tools and functionalities that are available in the IEDB-AR, focusing on the 10 new tools that have been added since the last report in the 2012 NAR webserver edition. In addition, many of the tools that were already hosted on the site in 2012 have received updates to newest versions, including NetMHC, NetMHCpan, BepiPred and DiscoTope. Overall, this IEDB-AR update provides a substantial set of updated and novel features for epitope prediction and analysis.},
author = {Dhanda, Sandeep Kumar and Mahajan, Swapnil and Paul, Sinu and Yan, Zhen and Kim, Haeuk and Jespersen, Martin Closter and Jurtz, Vanessa and Andreatta, Massimo and Greenbaum, Jason A and Marcatili, Paolo and Sette, Alessandro and Nielsen, Morten and Peters, Bjoern},
doi = {10.1093/nar/gkz452},
issn = {13624962},
journal = {Nucleic Acids Res.},
month = {jul},
number = {W1},
pages = {W502--W506},
title = {{IEDB-AR: immune epitope database-analysis resource in 2019}},
url = {https://academic.oup.com/nar/article/47/W1/W502/5494780},
volume = {47},
year = {2019}
}
@article{Wattenberg2016,
abstract = {The most trusted name in anatomy is now available on CD-ROM! Have access to the text's 2100 superb, high-quality illustrations, its thorough coverage of important topics, such as the structure and function of each body system, anatomical aspects of common surgical techniques, cross-sectional imaging, the anatomy encountered during endoscopic surgery, and more. "Stands alone as the greatest single reference in anatomy." (Radiology, review of a previous edition of Gray's Anatomy)},
author = {Wattenberg, Martin and Vi{\'{e}}gas, Fernanda and Johnson, Ian},
doi = {10.23915/distill.00002},
issn = {2476-0757},
journal = {Distill},
month = {oct},
number = {10},
pages = {e2},
title = {{How to Use t-SNE Effectively}},
url = {http://distill.pub/2016/misread-tsne},
volume = {1},
year = {2017}
}
@article{Chang2011,
abstract = {Objective: Despite improving healthcare, the gap in mortality between people with serious mental illness (SMI) and general population persists, especially for younger age groups. The electronic database from a large and comprehensive secondary mental healthcare provider in London was utilized to assess the impact of SMI diagnoses on life expectancy at birth. Method: People who were diagnosed with SMI (schizophrenia, schizoaffective disorder, bipolar disorder), substance use disorder, and depressive episode/disorder before the end of 2009 and under active review by the South London and Maudsley NHS Foundation Trust (SLAM) in southeast London during 2007-09 comprised the sample, retrieved by the SLAM Case Register Interactive Search (CRIS) system. We estimated life expectancy at birth for people with SMI and each diagnosis, from national mortality returns between 2007-09, using a life table method. Results: A total of 31,719 eligible people, aged 15 years or older, with SMI were analyzed. Among them, 1,370 died during 2007-09. Compared to national figures, all disorders were associated with substantially lower life expectancy: 8.0 to 14.6 life years lost for men and 9.8 to 17.5 life years lost for women. Highest reductions were found for men with schizophrenia (14.6 years lost) and women with schizoaffective disorders (17.5 years lost). Conclusion: The impact of serious mental illness on life expectancy is marked and generally higher than similarly calculated impacts of well-recognised adverse exposures such as smoking, diabetes and obesity. Strategies to identify and prevent causes of premature death are urgently required. {\textcopyright} 2011 Chang et al.},
author = {Chang, Chin Kuo and Hayes, Richard D and Perera, Gayan and Broadbent, Mathew T.M. and Fernandes, Andrea C and Lee, William E and Hotopf, Mathew and Stewart, Robert},
doi = {10.1371/journal.pone.0019590},
issn = {19326203},
journal = {PLoS One},
number = {5},
pages = {e19590},
pmid = {21611123},
publisher = {Public Library of Science},
title = {{Life expectancy at birth for people with serious mental illness and other major disorders from a secondary mental health care case register in London}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21611123 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3097201},
volume = {6},
year = {2011}
}
@article{Bair2006,
abstract = {In regression problems where the number of predictors greatly exceeds the number of observations, conventional regression techniques may produce unsatisfactory results. We describe a technique called supervised principal components that can be applied to this type of problem. Supervised principal components is similar to conventional principal components analysis except that it uses a subset of the predictors selected based on their association with the outcome. Supervised principal components can be applied to regression and generalized regression problems, such as survival analysis. It compares favorably to other techniques for this type of problem, and can also account for the effects of other covariates and help identify which predictor variables are most important. We also provide asymptotic consistency results to help support our empirical findings. These methods could become important tools for DNA microarray data, where they may be used to more accurately diagnose and treat cancer.},
author = {Bair, Eric and Hastie, Trevor and Paul, Debashis and Tibshirani, Robert},
doi = {10.1198/016214505000000628},
isbn = {0162-1459},
issn = {0162-1459},
journal = {J. Am. Stat. Assoc.},
keywords = {Gene expression,Microarray,Regression,Survival analysis},
month = {mar},
number = {473},
pages = {119--137},
publisher = {Taylor {\&} Francis},
title = {{Prediction by Supervised Principal Components}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214505000000628},
volume = {101},
year = {2006}
}
@article{Zhao2019,
abstract = {Chronic kidney disease (CKD) is prevalent across the world, and kidney function is well defined by an estimated glomerular filtration rate (eGFR). The progression of kidney disease can be predicted if the future eGFR can be accurately estimated using predictive analytics. In this study, we developed and validated a prediction model of eGFR by data extracted from a regional health system. This dataset includes demographic, clinical and laboratory information from primary care clinics. The model was built using Random Forest regression and evaluated using Goodness-of-fit statistics and discrimination metrics. After data preprocessing, the patient cohort for model development and validation contained 61,740 patients. The final model included eGFR, age, gender, body mass index (BMI), obesity, hypertension, and diabetes, which achieved a mean coefficient of determination of 0.95. The estimated eGFRs were used to classify patients into CKD stages with high macro-averaged and micro-averaged metrics. In conclusion, a model using real-world electronic medical records (EMR) data can accurately predict future kidney functions and provide clinical decision support.},
author = {Zhao, Jing and Gu, Shaopeng and McDermaid, Adam},
doi = {10.1016/j.mbs.2019.02.001},
issn = {18793134},
journal = {Math. Biosci.},
month = {apr},
pages = {24--30},
publisher = {Elsevier},
title = {{Predicting outcomes of chronic kidney disease from EMR data based on Random Forest Regression}},
url = {https://www.sciencedirect.com/science/article/pii/S0025556418303043},
volume = {310},
year = {2019}
}
@inproceedings{Struck2018,
abstract = {Published: Hagerstown, MD : Lippincott-Raven Publishers, 1997- Volumes for 1951- include a separately paged, numbered section: American Academy of Neurology newsletter.},
author = {Struck, Aaron and Ustun, Berk and Ruiz, Andres Rodriguez and Lee, Jong and LaRoche, Suzette and Hirsch, Lawrence and Gilmore, Emily and Vlachy, Jan and Haider, Hiba and Rudin, Cynthia and Westover, M.},
booktitle = {Neurology},
issn = {0028-3878},
month = {apr},
number = {15 Supplement},
pages = {S11.002},
publisher = {Advanstar Communications},
title = {{A Practical Risk Score for EEG Seizures in Hospitalized Patients (S11.002)}},
url = {https://n.neurology.org/content/90/15{\_}Supplement/S11.002},
volume = {90},
year = {2018}
}
@article{Fortunato2017,
abstract = {In this work we explore a straightforward variational Bayes scheme for Recurrent Neural Networks. Firstly, we show that a simple adaptation of truncated backpropagation through time can yield good quality uncertainty estimates and superior regularisation at only a small extra computational cost during training, also reducing the amount of parameters by 80$\backslash${\%}. Secondly, we demonstrate how a novel kind of posterior approximation yields further improvements to the performance of Bayesian RNNs. We incorporate local gradient information into the approximate posterior to sharpen it around the current batch statistics. We show how this technique is not exclusive to recurrent neural networks and can be applied more widely to train Bayesian neural networks. We also empirically demonstrate how Bayesian RNNs are superior to traditional RNNs on a language modelling benchmark and an image captioning task, as well as showing how each of these methods improve our model over a variety of other schemes for training them. We also introduce a new benchmark for studying uncertainty for language models so future methods can be easily compared.},
archivePrefix = {arXiv},
arxivId = {1704.02798},
author = {Fortunato, Meire and Blundell, Charles and Vinyals, Oriol},
eprint = {1704.02798},
month = {apr},
title = {{Bayesian Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1704.02798},
year = {2017}
}
@article{DeFauw2018,
abstract = {The volume and complexity of diagnostic imaging is increasing at a pace faster than the availability of human expertise to interpret it. Artificial intelligence has shown great promise in classifying two-dimensional photographs of some common diseases and typically relies on databases of millions of annotated images. Until now, the challenge of reaching the performance of expert clinicians in a real-world clinical pathway with three-dimensional diagnostic scans has remained unsolved. Here, we apply a novel deep learning architecture to a clinically heterogeneous set of three-dimensional optical coherence tomography scans from patients referred to a major eye hospital. We demonstrate performance in making a referral recommendation that reaches or exceeds that of experts on a range of sight-threatening retinal diseases after training on only 14,884 scans. Moreover, we demonstrate that the tissue segmentations produced by our architecture act as a device-independent representation; referral accuracy is maintained when using tissue segmentations from a different type of device. Our work removes previous barriers to wider clinical use without prohibitive training data requirements across multiple pathologies in a real-world setting.},
author = {{De Fauw}, Jeffrey and Ledsam, Joseph R. and Romera-Paredes, Bernardino and Nikolov, Stanislav and Tomasev, Nenad and Blackwell, Sam and Askham, Harry and Glorot, Xavier and O'Donoghue, Brendan and Visentin, Daniel and van den Driessche, George and Lakshminarayanan, Balaji and Meyer, Clemens and Mackinder, Faith and Bouton, Simon and Ayoub, Kareem and Chopra, Reena and King, Dominic and Karthikesalingam, Alan and Hughes, C{\'{i}}an O and Raine, Rosalind and Hughes, Julian and Sim, Dawn A. and Egan, Catherine and Tufail, Adnan and Montgomery, Hugh and Hassabis, Demis and Rees, Geraint and Back, Trevor and Khaw, Peng T. and Suleyman, Mustafa and Cornebise, Julien and Keane, Pearse A. and Ronneberger, Olaf},
doi = {10.1038/s41591-018-0107-6},
issn = {1546-170X},
journal = {Nat. Med.},
keywords = {Diagnosis,Eye manifestations,Machine learning,Three,dimensional imaging},
month = {sep},
number = {9},
pages = {1342--1350},
pmid = {30104768},
publisher = {Nature Publishing Group},
title = {{Clinically applicable deep learning for diagnosis and referral in retinal disease.}},
url = {http://www.nature.com/articles/s41591-018-0107-6 http://www.ncbi.nlm.nih.gov/pubmed/30104768},
volume = {24},
year = {2018}
}
@article{Warrender2006,
abstract = {Infection with Mycobacterium tuberculosis (Mtb) is characterized by localized, roughly spherical lesions within which the pathogen interacts with host cells. Containment of the infection or progression of disease depends on the behavior of individual cells, which, in turn, depends on the local molecular environment and on contact with neighboring cells. Modeling can help us understand the nonlinear interactions that drive the overall dynamics in this system. Early events in infection are particularly important, as are spatial effects and inherently stochastic processes. We describe a model of early Mycobacterium infection using the CyCells simulator, which was designed to capture these effects. We relate CyCells simulations of the model to several experimental observations of individual components of the response to Mtb.},
author = {Warrender, Christina and Forrest, Stephanie and Koster, Frederick},
doi = {10.1007/s11538-006-9103-y},
isbn = {1153800691},
issn = {0092-8240},
journal = {Bull. Math. Biol.},
keywords = {Computer Simulation,Humans,Interferon-gamma,Interferon-gamma: immunology,Interleukin-10,Interleukin-10: immunology,Macrophages, Alveolar,Macrophages, Alveolar: immunology,Macrophages, Alveolar: microbiology,Models, Biological,Mycobacterium tuberculosis,Mycobacterium tuberculosis: physiology,T-Lymphocytes,T-Lymphocytes: immunology,T-Lymphocytes: microbiology,Tuberculosis,Tuberculosis: immunology,Tuberculosis: microbiology,Tumor Necrosis Factor-alpha,Tumor Necrosis Factor-alpha: immunology},
month = {nov},
number = {8},
pages = {2233--61},
pmid = {17086496},
title = {{Modeling intercellular interactions in early Mycobacterium infection.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17086496},
volume = {68},
year = {2006}
}
@article{Greer2019,
abstract = {BACKGROUND: Positive psychology interventions show promise for reducing psychosocial distress associated with health adversity and have the potential to be widely disseminated to young adults through technology. OBJECTIVE: This pilot randomized controlled trial examined the feasibility of delivering positive psychology skills via the Vivibot chatbot and its effects on key psychosocial well-being outcomes in young adults treated for cancer. METHODS: Young adults (age 18-29 years) were recruited within 5 years of completing active cancer treatment by using the Vivibot chatbot on Facebook messenger. Participants were randomized to either immediate access to Vivibot content (experimental group) or access to only daily emotion ratings and access to full chatbot content after 4 weeks (control). Created using a human-centered design process with young adults treated for cancer, Vivibot content includes 4 weeks of positive psychology skills, daily emotion ratings, video, and other material produced by survivors, and periodic feedback check-ins. All participants were assessed for psychosocial well-being via online surveys at baseline and weeks 2, 4, and 8. Analyses examined chatbot engagement and open-ended feedback on likability and perceived helpfulness and compared experimental and control groups with regard to anxiety and depression symptoms and positive and negative emotion changes between baseline and 4 weeks. To verify the main effects, follow-up analyses compared changes in the main outcomes between 4 and 8 weeks in the control group once participants had access to all chatbot content. RESULTS: Data from 45 young adults (36 women; mean age: 25 [SD 2.9]; experimental group: n=25; control group: n=20) were analyzed. Participants in the experimental group spent an average of 74 minutes across an average of 12 active sessions chatting with Vivibot and rated their experience as helpful (mean 2.0/3, SD 0.72) and would recommend it to a friend (mean 6.9/10; SD 2.6). Open-ended feedback noted its nonjudgmental nature as a particular benefit of the chatbot. After 4 weeks, participants in the experimental group reported an average reduction in anxiety of 2.58 standardized t-score units, while the control group reported an increase in anxiety of 0.7 units. A mixed-effects models revealed a trend-level (P=.09) interaction between group and time, with an effect size of 0.41. Those in the experimental group also experienced greater reductions in anxiety when they engaged in more sessions (z=-1.9, P=.06). There were no significant (or trend level) effects by group on changes in depression, positive emotion, or negative emotion. CONCLUSIONS: The chatbot format provides a useful and acceptable way of delivering positive psychology skills to young adults who have undergone cancer treatment and supports anxiety reduction. Further analysis with a larger sample size is required to confirm this pattern.},
author = {Greer, Stephanie and Ramo, Danielle and Chang, Yin Juei and Fu, Michael and Moskowitz, Judith and Haritatos, Jana},
doi = {10.2196/15018},
issn = {22915222},
journal = {JMIR mHealth uHealth},
keywords = {cancer,chatbot,positive psychology,young adult},
month = {oct},
number = {10},
pages = {e15018},
pmid = {31674920},
title = {{Use of the Chatbot "Vivibot" to Deliver Positive Psychology Skills and Promote Well-Being Among Young People After Cancer Treatment: Randomized Controlled Feasibility Trial}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/31674920},
volume = {7},
year = {2019}
}
@article{Shannon2003a,
abstract = {Cytoscape is an open source software project for integrating biomolecular interaction networks with high-throughput expression data and other molecular states into a unified conceptual framework. Although applicable to any system of molecular components and interactions, Cytoscape is most powerful when used in conjunction with large databases of protein-protein, protein-DNA, and genetic interactions that are increasingly available for humans and model organisms. Cytoscape's software Core provides basic functionality to layout and query the network; to visually integrate the network with expression profiles, phenotypes, and other molecular states; and to link the network to databases of functional annotations. The Core is extensible through a straightforward plug-in architecture, allowing rapid development of additional computational analyses and features. Several case studies of Cytoscape plug-ins are surveyed, including a search for interaction pathways correlating with changes in gene expression, a study of protein complexes involved in cellular recovery to DNA damage, inference of a combined physical/functional interaction network for Halobacterium, and an interface to detailed stochastic/kinetic gene regulatory models.},
author = {Shannon, Paul and Markiel, Andrew and Ozier, Owen and Baliga, Nitin S and Wang, Jonathan T and Ramage, Daniel and Amin, Nada and Schwikowski, Beno and Ideker, Trey},
doi = {10.1101/gr.1239303},
issn = {10889051},
journal = {Genome Res.},
month = {nov},
number = {11},
pages = {2498--2504},
pmid = {14597658},
title = {{Cytoscape: A software Environment for integrated models of biomolecular interaction networks}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/14597658 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC403769 http://www.genome.org/cgi/doi/10.1101/gr.1239303},
volume = {13},
year = {2003}
}
@article{Neufeld2017,
abstract = {Background Evidence regarding the association between service contact and subsequent mental health in adolescents is scarce, and previous findings are mixed. We aimed to longitudinally assess the extent to which depressive symptoms in adolescents change after contact with mental health services. Methods As part of a longitudinal cohort study, between April 28, 2005, and March 17, 2010, we recruited 1238 14-year-old adolescents and their primary caregivers from 18 secondary schools in Cambridgeshire, UK. Participants underwent follow-up assessment at months 18 and 36. Trained researchers assessed the adolescents for current mental disorder using the Schedule for Affective Disorders and Schizophrenia for School-Age Children Present and Lifetime version (K-SADS-PL). Caregivers and adolescents reported contact with mental health services in the year before baseline. Adolescents self-reported depressive symptoms (Mood and Feelings Questionnaire [MFQ]) at each timepoint. We assessed change in MFQ sum scores from baseline contact with mental health services using multilevel mixed-effects regression adjusted for sociodemographic, environmental, individual, and mental health confounders, with multiple imputation of missing data. We used propensity score weighting to balance confounders between treatment (users of mental health services) and control (non-users of mental health services) groups. We implemented an MFQ clinical cutoff following the results of receiver operating characteristic analysis. Findings 14-year-old adolescents who had contact with mental health services in the past year had a greater decrease in depressive symptoms than those without contact (adjusted coefficient −1{\textperiodcentered}68, 95{\%} CI −3{\textperiodcentered}22 to −0{\textperiodcentered}14; p=0{\textperiodcentered}033). By age 17 years, the odds of reporting clinical depression were more than seven times higher in individuals without contact than in service users who had been similarly depressed at baseline (adjusted odds ratio 7{\textperiodcentered}38, 1{\textperiodcentered}73–31{\textperiodcentered}50; p=0{\textperiodcentered}0069). Interpretation Our findings show that contact with mental health services at age 14 years by adolescents with a mental disorder reduced the likelihood of depression by age 17 years. This finding supports the improvement of access to adolescent mental health services. Funding Wellcome Trust, National Institute for Health Research.},
author = {Neufeld, Sharon A.S. and Dunn, Valerie J and Jones, Peter B and Croudace, Tim J and Goodyer, Ian M},
doi = {10.1016/S2215-0366(17)30002-0},
issn = {22150374},
journal = {The Lancet Psychiatry},
month = {feb},
number = {2},
pages = {120--127},
pmid = {28087201},
publisher = {Elsevier},
title = {{Reduction in adolescent depression after contact with mental health services: a longitudinal cohort study in the UK}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28087201 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5285445},
volume = {4},
year = {2017}
}
@article{Wu2019c,
abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes and benchmarks of the existing algorithms on different learning tasks. Finally, we propose potential research directions in this rapidly growing field.},
archivePrefix = {arXiv},
arxivId = {1901.00596},
author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
eprint = {1901.00596},
journal = {undefined},
title = {{A Comprehensive Survey on Graph Neural Networks}},
url = {https://www.semanticscholar.org/paper/A-Comprehensive-Survey-on-Graph-Neural-Networks-Wu-Pan/81a4fd3004df0eb05d6c1cef96ad33d5407820df http://arxiv.org/abs/1901.00596},
year = {2019}
}
@article{Pineo2020,
abstract = {Public health practitioners produce urban health indicator (UHI) tools to inform built environment policy and decision-making, among other objectives. Indicator producers perceive UHI tools as an easily understandable form of evidence about the urban environment impact on health for policy-makers' consumption. However, indicator producers often conceptualise policy-making as a rational and linear process, therefore underestimating the complex and contested nature of developing and implementing policy. This study investigates the health-promotion value of UHI tools in the complex urban planning policy and decision-making context. A thematic analysis was conducted following semi-structured interviews with 22 indicator producers and users in San Francisco, Melbourne and Sydney. The analysis was informed by collaborative rationality and systems theories and the results were used to develop causal loop diagrams (CLDs) of producers and users' mental models. The preliminary CLDs were tested and improved through a participatory modelling workshop (six participants). A high-level CLD depicts users and producers' shared mental model in which indicator development and use are embedded in policy development and application processes. In the cases analysed, creating and using UHI tools increased inter-sectoral relationships, which supported actors to better understand each other's opportunities and constraints. These relationships spurred new advocates for health in diverse organisations, supporting health-in-all-policies and whole-of-society approaches. Constraints to health-promoting policy and implementation (such as those which are legal, political and economic in nature), were overcome through community involvement in UHI tools and advocacy effectiveness. A number of factors reduced the perceived relevance and authority of UHI tools, including: a high number of available indicators, lack of neighbourhood scale data and poor-quality data. In summary, UHI tools were a form of evidence that influenced local urban planning policy and decision-making when they were embedded in policy processes, networks and institutions. In contrast to the dominant policy impact model in the indicator literature, such evidence did not typically influence policy as an exogenous entity. Indicators had impact when they were embedded in local institutions and well-resourced over time, resulting in trusted relationships and collaborations among indicator producers and users. Further research is needed to explore other governance contexts and how UHI tools affect the power of different actors, particularly for under-represented communities.},
author = {Pineo, Helen and Zimmermann, Nici and Davies, Michael},
doi = {10.1057/s41599-020-0398-3},
issn = {20551045},
journal = {Palgrave Commun.},
keywords = {Complex networks,Environmental studies,Social policy},
month = {dec},
number = {1},
pages = {21},
publisher = {Palgrave},
title = {{Integrating health into the complex urban planning policy and decision-making context: a systems thinking analysis}},
url = {http://www.nature.com/articles/s41599-020-0398-3},
volume = {6},
year = {2020}
}
@article{Seigal2019,
abstract = {We introduce a tensor-based clustering method to extract sparse, low-dimensional structure from high-dimensional, multi-indexed datasets. This framework is designed to enable detection of clusters of data in the presence of structural requirements which we encode as algebraic constraints in a linear program. Our clustering method is general and can be tailored to a variety of applications in science and industry. We illustrate our method on a collection of experiments measuring the response of genetically diverse breast cancer cell lines to an array of ligands. Each experiment consists of a cell line-ligand combination, and contains time-course measurements of the early-signalling kinases MAPK and AKT at two different ligand dose levels. By imposing appropriate structural constraints and respecting the multi-indexed structure of the data, the analysis of clusters can be optimized for biological interpretation and therapeutic understanding. We then perform a systematic, large-scale exploration of mechanistic models of MAPK-AKT crosstalk for each cluster. This analysis allows us to quantify the heterogeneity of breast cancer cell subtypes, and leads to hypotheses about the signalling mechanisms that mediate the response of the cell lines to ligands.},
author = {Seigal, Anna and Beguerisse-D{\'{i}}az, Mariano and Schoeberl, Birgit and Niepel, Mario and Harrington, Heather A.},
doi = {10.1098/rsif.2018.0661},
issn = {17425662},
journal = {J. R. Soc. Interface},
keywords = {Algebra,Data clustering,Model selection,Parameter inference,Signalling networks,Systems biology,Tensors},
month = {feb},
number = {151},
pages = {20180661},
publisher = {The Royal Society},
title = {{Tensor clustering with algebraic constraints gives interpretable groups of crosstalk mechanisms in breast cancer}},
url = {https://royalsocietypublishing.org/doi/10.1098/rsif.2018.0661},
volume = {16},
year = {2019}
}
@article{Kalyanpur2012,
abstract = {Although the majority of evidence analysis in DeepQA is focused on unstructured information (e.g., natural-language documents), several components in the DeepQA system use structured data (e.g., databases, knowledge bases, and ontologies) to generate potential candidate answers or find additional evidence. Structured data analytics are a natural complement to unstructured methods in that they typically cover a narrower range of questions but are more precise within that range. Moreover, structured data that has formal semantics is amenable to logical reasoning techniques that can be used to provide implicit evidence. The DeepQA system does not contain a single monolithic structured data module; instead, it allows for different components to use and integrate structured and semistructured data, with varying degrees of expressivity and formal specificity. This paper is a survey of DeepQA components that use structured data. Areas in which evidence from structured sources has the most impact include typing of answers, application of geospatial and temporal constraints, and the use of formally encoded a priori knowledge of commonly appearing entity types such as countries and U.S. presidents. We present details of appropriate components and demonstrate their end-to-end impact on the IBM Watson™ system. {\textcopyright} 1957-2012 IBM.},
author = {Kalyanpur, A. and Boguraev, B. K. and Patwardhan, S. and Murdock, J. W. and Lally, A. and Welty, C. and Prager, J. M. and Coppola, B. and Fokoue-Nkoutche, A. and Zhang, L. and Pan, Y. and Qiu, Z. M.},
doi = {10.1147/JRD.2012.2188737},
issn = {00188646},
journal = {IBM J. Res. Dev.},
month = {may},
number = {3-4},
pages = {10:1--10:14},
publisher = {IBM Corp.},
title = {{Structured data and inference in DeepQA}},
url = {http://ieeexplore.ieee.org/document/6177725/},
volume = {56},
year = {2012}
}
@inproceedings{Etzioni2004,
abstract = {Manually querying search engines in order to accumulate a large body of factual information is a tedious, error-prone process of piecemeal search. Search engines retrieve and rank potentially relevant documents for human perusal, but do not extract facts, assess confidence, or fuse information from multiple documents. This paper introduces KNOWITALL, a system that aims to automate the tedious process of extracting large collections of facts from the web in an autonomous, domain-independent, and scalable manner. The paper describes preliminary experiments in which an instance of KNOWITALL, running for four days on a single machine, was able to automatically extract 54,753 facts. KNOWITALL associates a probability with each fact enabling it to trade off precision and recall. The paper analyzes KNOWITALL's architecture and reports on lessons learned for the design of large-scale information extraction systems.},
address = {New York, New York, USA},
author = {Etzioni, Oren and Cafarella, Michael and Downey, Doug and Kok, Stanley and Popescu, Ana-Maria and Shaked, Tal and Soderland, Stephen and Weld, Daniel S. and Yates, Alexander},
booktitle = {Proc. 13th Conf. World Wide Web - WWW '04},
doi = {10.1145/988672.988687},
isbn = {158113844X},
keywords = {information extraction,mutual information,pmi,search},
pages = {100},
publisher = {ACM Press},
title = {{Web-scale information extraction in knowitall}},
url = {http://portal.acm.org/citation.cfm?doid=988672.988687},
year = {2004}
}
@article{Bellot2018,
author = {Bellot, Alexis and {Van der Schaar}, Mihaela},
doi = {10.1109/JBHI.2018.2832599},
issn = {21682194},
journal = {IEEE J. Biomed. Heal. Informatics},
keywords = {Bayes methods,Bayesian inference,Biological system modeling,Diseases,Permutation tests,Personalized prognosis,Personalized risk prediction,Predictive models,Shape,Sociology,Statistics,Survival analysis},
month = {jan},
number = {1},
pages = {72--80},
title = {{A Hierarchical Bayesian Model for Personalized Survival Predictions}},
url = {https://ieeexplore.ieee.org/document/8353457/},
volume = {23},
year = {2018}
}
@article{Schrauzer1990,
abstract = {Using data for 27 Texas counties from 1978-1987, it is shown that the incidence rates of suicide, homicide, and rape are significantly higher in counties whose drinking water supplies contain little or no lithium than in counties with water lithium levels ranging from 70-170 micrograms/L; the differences remain statistically significant (p less than 0.01) after corrections for population density. The corresponding associations with the incidence rates of robbery, burglary, and theft were statistically significant with p less than 0.05. These results suggest that lithium has moderating effects on suicidal and violent criminal behavior at levels that may be encountered in municipal water supplies. Comparisons of drinking water lithium levels, in the respective Texas counties, with the incidences of arrests for possession of opium, cocaine, and their derivatives (morphine, heroin, and codeine) from 1981-1986 also produced statistically significant inverse associations, whereas no significant or consistent associations were observed with the reported arrest rates for possession of marijuana, driving under the influence of alcohol, and drunkenness. These results suggest that lithium at low dosage levels has a generally beneficial effect on human behavior, which may be associated with the functions of lithium as a nutritionally-essential trace element. Subject to confirmation by controlled experiments with high-risk populations, increasing the human lithium intakes by supplementation, or the lithiation of drinking water is suggested as a possible means of crime, suicide, and drug-dependency reduction at the individual and community level.},
author = {Schrauzer, Gerhard N and Shrestha, Krishna P},
doi = {10.1007/BF02990271},
issn = {01634984},
journal = {Biol. Trace Elem. Res.},
keywords = {Lithium,cocaine,crime,crime prevention,drinking water,drug dependency,drug use,drunk driving,drunkenness,heroin,lithiation of,lithium supplementation,marijuana,morphine,reduction of,suicide,suicide prevention,violent behavior},
month = {may},
number = {2},
pages = {105--113},
pmid = {1699579},
title = {{Lithium in drinking water and the incidences of crimes, suicides, and arrests related to drug addictions}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/1699579},
volume = {25},
year = {1990}
}
@article{Andres-Terre2019,
abstract = {The use of Variational Autoencoders in different Machine Learning tasks has drastically increased in the last years. They have been developed as denoising, clustering and generative tools, highlighting a large potential in a wide range of fields. Their embeddings are able to extract relevant information from highly dimensional inputs, but the converged models can differ significantly and lead to degeneracy on the latent space. We leverage the relation between theoretical physics and machine learning to explain this behaviour, and introduce a new approach to correct for degeneration by using perturbation theory. The re-formulation of the embedding as multi-dimensional generative distribution, allows mapping to a new set of functions and their corresponding energy spectrum. We optimise for a perturbed Hamiltonian, with an additional energy potential that is related to the unobserved topology of the data. Our results show the potential of a new theoretical approach that can be used to interpret the latent space and generative nature of unsupervised learning, while the energy landscapes defined by the perturbations can be further used for modelling and dynamical purposes.},
archivePrefix = {arXiv},
arxivId = {1907.05267},
author = {Andr{\'{e}}s-Terr{\'{e}}, Helena and Li{\'{o}}, Pietro},
eprint = {1907.05267},
month = {jul},
title = {{Perturbation theory approach to study the latent space degeneracy of Variational Autoencoders}},
url = {http://arxiv.org/abs/1907.05267},
year = {2019}
}
@phdthesis{Warrender2004a,
author = {Warrender, CE},
school = {University of New Mexico, USA},
title = {{Modeling intercellular interactions in the peripheral immune system}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Modeling+intercellular+interactions+in+the+peripheral+immune+system{\#}0},
year = {2004}
}
@misc{Makar2019,
author = {Makar, Maggie and Swaminathan, Adith and Kiciman, Emre},
month = {jan},
title = {{A Distillation Approach to Data Efﬁcient Individual Treatment Effect Estimation}},
url = {https://www.microsoft.com/en-us/research/publication/a-distillation-approach-to-data-efﬁcient-individual-treatment-effect-estimation/},
year = {2019}
}
@article{Betancourt2013,
abstract = {Hierarchical modeling provides a framework for modeling the complex interactions typical of problems in applied statistics. By capturing these relationships, however, hierarchical models also introduce distinctive pathologies that quickly limit the efficiency of most common methods of in- ference. In this paper we explore the use of Hamiltonian Monte Carlo for hierarchical models and demonstrate how the algorithm can overcome those pathologies in practical applications.},
archivePrefix = {arXiv},
arxivId = {1312.0906},
author = {Betancourt, M. J. and Girolami, Mark},
doi = {10.1201/b18502-5},
eprint = {1312.0906},
isbn = {9761482235111},
issn = {0021-9746},
month = {dec},
pmid = {25631214},
title = {{Hamiltonian Monte Carlo for Hierarchical Models}},
url = {http://arxiv.org/abs/1312.0906},
year = {2013}
}
@article{Chen2008,
abstract = {MOTIVATION: Gene set analysis allows formal testing of subtle but$\backslash$ncoordinated changes in a group of genes, such as those defined by$\backslash$nGene Ontology or KEGG Pathway databases. We propose a new method$\backslash$nfor gene set analysis that is based on Principal Component Analysis$\backslash$nof genes expression values in the gene set. PCA is an effective method$\backslash$nfor reducing high dimensionality and capture variations in gene expression$\backslash$nvalues. However, one limitation with PCA is that the latent variable$\backslash$nidentified by the first principal component may be unrelated to outcome.$\backslash$nRESULTS: In the proposed Supervised Principal Component (SPCA) model$\backslash$nfor gene set analysis, the principal components are estimated from$\backslash$na selected subset of genes that are associated with outcome. As outcome$\backslash$ninformation is used in the gene selection step, this method is supervised,$\backslash$nthus called the Supervised PCA model. Because of the gene selection$\backslash$nstep, test statistic in SPCA model can no longer be approximated$\backslash$nwell using t distribution. We propose a two-component mixture distribution$\backslash$nbased on Gumbel exteme value distributions to account for the gene$\backslash$nselection step. We show the proposed method compares favorably to$\backslash$ncurrently available gene set analysis methods using simulated and$\backslash$nreal microarray data. Software: The R code for the analysis used$\backslash$nin this paper are available upon request, we are currently working$\backslash$non implementing the proposed method in an R package. CONTACT: chenx3@ccf.org.},
author = {Chen, Xi and Wang, Lily and Smith, Jonathan D and Zhang, Bing},
doi = {10.1093/bioinformatics/btn458},
isbn = {1367-4803$\backslash$r1460-2059},
issn = {13674803},
journal = {Bioinformatics},
month = {nov},
number = {21},
pages = {2474--2481},
pmid = {18753155},
publisher = {Oxford University Press},
title = {{Supervised principal component analysis for gene set enrichment of microarray data with continuous or survival outcomes}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18753155 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2732277},
volume = {24},
year = {2008}
}
@article{Spohrer2015,
abstract = {Recent advances in cognitive computing componentry combined with other factors are leading to commercially viable cognitive systems. From chips to smart phones to public and private clouds, industrial strength “cognition as a service” is beginning to appear at all scales in business and society. Furthermore, in the age of zettabytes on the way to yottabytes, the designers, engineers, and managers of future smart systems will depend on cognition as a service. Cognition as a service can help unlock the mysteries of big data and ultimately boost the creativity and productivity of professionals and their teams, the productive output of industries and organizations, as well as the GDP (gross domestic product) of regions and nations. In this and the next decade, cognition as a service will allow us to re-image work practices, augmenting and scaling expertise to transform professions, industries, and regions.},
author = {Spohrer, Jim and Banavar, Guruduth},
doi = {10.1609/aimag.v36i4.2618},
issn = {0738-4602},
journal = {AI Mag.},
month = {dec},
number = {4},
pages = {71},
title = {{Cognition as a Service: An Industry Perspective}},
url = {https://aaai.org/ojs/index.php/aimagazine/article/view/2618},
volume = {36},
year = {2017}
}
@article{Ertekin2015,
abstract = {Abstract We present a Bayesian method for building scoring systems, which are linear models with coefficients that have very few significant digits. Usually the construction of scoring systems involve manual effort—humans invent the full scoring system without using data, or they choose how logistic regression coefficients should be scaled and rounded to produce a scoring system. These kinds of heuristics lead to suboptimal solutions. Our approach is different in that humans need only specify the prior over what the coefficients should look like, and the scoring system is learned from data. For this approach, we provide a Metropolis-Hastings sampler that tends to pull the coefficient values toward their “natural scale.” Empirically, the proposed method achieves a high degree of interpretability of the models while maintaining competitive generalization performances.},
author = {Ertekin, Şeyda and Rudin, Cynthia},
doi = {10.1089/big.2015.0033},
issn = {2167-6461},
journal = {Big Data},
keywords = {data mining,machine learning,predictive analytics},
month = {dec},
number = {4},
pages = {267--276},
pmid = {27441407},
title = {{A Bayesian Approach to Learning Scoring Systems}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27441407 http://www.liebertpub.com/doi/10.1089/big.2015.0033 http://online.liebertpub.com/doi/10.1089/big.2015.0033},
volume = {3},
year = {2015}
}
@article{Beech2014,
abstract = {This study was designed to identify genes whose expression in peripheral blood may serve as early markers for treatment response to lithium (Li) in patients with bipolar disorder. Although changes in peripheral blood gene-expression may not relate directly to mood symptoms, differences in treatment response at the biochemical level may underlie some of the heterogeneity in clinical response to Li. Subjects were randomized to treatment with (n=28) or without (n=32) Li. Peripheral blood gene-expression was measured before and 1month after treatment initiation, and treatment response was assessed after 6 months. In subjects treated with Li, 62 genes were differentially regulated in treatment responders and non-responders. Of these, BCL2L1 showed the greatest difference between Li responders and non-responders. These changes were specific to Li responders (n=9), and were not seen in Li non-responders or patients treated without Li, suggesting that they may have specific roles in treatment response to Li.},
author = {Beech, R D and Leffert, J J and Lin, A and Sylvia, L G and Umlauf, S and Mane, S and Zhao, H and Bowden, C and Calabrese, J R and Friedman, E S and Ketter, T A and Iosifescu, D V and Reilly-Harrington, N A and Ostacher, M and Thase, M E and Nierenberg, A},
doi = {10.1038/tpj.2013.16},
issn = {14731150},
journal = {Pharmacogenomics J.},
keywords = {BCL2L1,Bipolar disorder,Gene expression,Lithium-response,Microarray},
month = {apr},
number = {2},
pages = {182--191},
pmid = {23670706},
title = {{Gene-expression differences in peripheral blood between lithium responders and non-responders in the Lithium Treatment-Moderate dose Use Study (LiTMUS)}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23670706},
volume = {14},
year = {2014}
}
@article{Kellum2019,
abstract = {A new study of deep learning based on electronic health records promises to forecast acute kidney injury up to 48 hours before it can be diagnosed clinically. However, employing data science to predict acute kidney injury might be more challenging than it seems.},
author = {Kellum, John A. and Bihorac, Azra},
doi = {10.1038/s41581-019-0203-y},
issn = {1759-5061},
journal = {Nat. Rev. Nephrol.},
keywords = {Acute kidney injury,Machine learning,Predictive medicine,Sepsis},
month = {nov},
number = {11},
pages = {663--664},
publisher = {Nature Publishing Group},
title = {{Artificial intelligence to predict AKI: is it a breakthrough?}},
url = {http://www.nature.com/articles/s41581-019-0203-y},
volume = {15},
year = {2019}
}
@article{Borg2011,
abstract = {Background: As Norway moves toward the provision of home-based crisis response, knowledge is needed about understandings of mental health crisis and effective ways of addressing crises within the home. Objective: To elicit and learn from service users' experiences about the subjective meanings of crisis and what kind of help will be most effective in resolving mental health crises. Theoretical: A phenomenological-hermeneutic cooperative inquiry method was used to elicit and analyse focus group responses from mental health service users who had experienced crises. Results: Findings clustered into three themes: (1) Crisis as multifaceted and varied experiences; (2) losing the skills and structure of everyday life; and (3) complexities involved in family support. Conclusion: Several aspects of crises require an expansion of the biomedical model of acute intervention to include consideration of the personal and familial meaning of the crisis, attention to the home context, and activities of daily living that are disrupted by the crisis, and ways for the person and the family to share in and learn from resolution of the crisis. {\textcopyright} 2011 M. Borg et al.},
author = {Borg, Marit and Karlsson, Bengt and Lofthus, Ann Mari and Davidson, Larry},
doi = {10.3402/qhw.v6i4.7197},
issn = {17482623},
journal = {Int. J. Qual. Stud. Health Well-being},
keywords = {Acute mental health care,Experience-based knowledge,Mental health crisis,Service users},
number = {4},
pmid = {22140400},
publisher = {Taylor {\&} Francis},
title = {{"Hitting the wall": Lived experiences of mental health crises}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22140400 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3229008},
volume = {6},
year = {2011}
}
@article{Chicco2020,
author = {Chicco, Davide and Jurman, Giuseppe},
doi = {10.1186/s12911-020-1023-5},
issn = {1472-6947},
journal = {BMC Med. Inform. Decis. Mak.},
month = {dec},
number = {1},
pages = {16},
title = {{Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone}},
url = {https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5},
volume = {20},
year = {2020}
}
@article{Srivastava2014a,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
journal = {J. Mach. Learn. Res.},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
url = {http://jmlr.org/papers/v15/srivastava14a.html},
volume = {15},
year = {2014}
}
@article{Iwagami2017,
abstract = {{\textcopyright} 2017 The Author 2017. Published by Oxford University Press on behalf of ERA-EDTA. Background Anonymous primary care records are an important resource for observational studies. However, their external validity is unknown in identifying the prevalence of decreased kidney function and renal replacement therapy (RRT). We thus compared the prevalence of decreased kidney function and RRT in the Clinical Practice Research Datalink (CPRD) with a nationally representative survey and national registry. Methods Among all people ≥25 years of age registered in the CPRD for ≥1 year on 31 March 2014, we identified patients with an estimated glomerular filtration rate (eGFR) {\textless} 60 mL/min/1.73 m 2 , according to their most recent serum creatinine in the past 5 years using the Chronic Kidney Disease Epidemiology Collaboration equation and patients with recorded diagnoses of RRT. Denominators were the entire population in each age-sex band irrespective of creatinine measurement. The prevalence of eGFR {\textless} 60 mL/min/1.73 m 2 was compared with that in the Health Survey for England (HSE) 2009/2010 and the prevalence of RRT was compared with that in the UK Renal Registry (UKRR) 2014. Results We analysed 2 761 755 people in CPRD [mean age 53 (SD 17) years, men 49{\%}], of whom 189 581 (6.86{\%}) had an eGFR {\textless} 60 mL/min/1.73 m 2 and 3293 (0.12{\%}) were on RRT. The prevalence of eGFR {\textless} 60 mL/min/1.73 m 2 in CPRD was similar to that in the HSE and the prevalence of RRT was close to that in the UKRR across all age groups in men and women, although the small number of younger patients with an eGFR {\textless} 60 mL/min/1.73 m 2 in the HSE might have hampered precise comparison. Conclusions UK primary care data have good external validity for the prevalence of decreased kidney function and RRT.},
author = {Iwagami, Masao and Tomlinson, Laurie A. and Mansfield, Kathryn E. and Casula, Anna and Caskey, Fergus J. and Aitken, Grant and Fraser, Simon D.S. and Roderick, Paul J. and Nitsch, Dorothea},
doi = {10.1093/ndt/gfw318},
issn = {14602385},
journal = {Nephrol. Dial. Transplant.},
keywords = {Chronic kidney disease,Epidemiology,Primary care,Renal replacement therapy,Validity},
month = {apr},
number = {suppl{\_}2},
pages = {ii142--ii150},
pmid = {28201668},
title = {{Validity of estimated prevalence of decreased kidney function and renal replacement therapy from primary care electronic health records compared with national survey and registry data in the United Kingdom}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28201668 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5410977 https://academic.oup.com/ndt/article-lookup/doi/10.1093/ndt/gfw318},
volume = {32},
year = {2017}
}
@article{Benoit2018,
abstract = {quanteda is an R package providing a comprehensive workflow and toolkit for natural language processing tasks such as corpus management, tokenization, analysis, and vi-sualization. It has extensive functions for applying dictionary analysis, exploring texts using keywords-in-context, computing document and feature similarities, and discovering multi-word expressions through collocation scoring. Based entirely on sparse operations, it provides highly efficient methods for compiling document-feature matrices and for manipulating these or using them in further quantitative analysis. Using C++ and multi-threading extensively, quanteda is also considerably faster and more efficient than other R and Python packages in processing large textual data. The package is designed for R users needing to apply natural language processing to texts, from documents to final analysis. Its capabilities match or exceed those provided in many end-user software applications, many of which are expensive and not open source. The package is therefore of great benefit to researchers, students, and other analysts with fewer financial resources. While using quanteda requires R programming knowledge, its API is designed to enable powerful, efficient analysis with a minimum of steps. By emphasizing consistent design, furthermore, quanteda lowers the barriers to learning and using NLP and quantitative text analysis even for proficient R programmers. Corpus management quanteda makes it easy to manage texts in the form of a "corpus", which is defined as a collection of texts that includes document-level variables specific to each text, as well as meta-data for documents and for the collection as a whole. With the package, users can easily segment texts by words, paragraphs, sentences, or even user-supplied delimiters and tags, group them into larger documents by document-level variables, or subset them based on logical conditions or combinations of document-level variables.},
author = {Benoit, Kenneth and Watanabe, Kohei and Wang, Haiyan and Nulty, Paul and Obeng, Adam and M{\"{u}}ller, Stefan and Matsuo, Akitaka},
doi = {10.21105/joss.00774},
issn = {2475-9066},
journal = {J. Open Source Softw.},
month = {oct},
number = {30},
pages = {774},
title = {{quanteda: An R package for the quantitative analysis of textual data}},
url = {http://joss.theoj.org/papers/10.21105/joss.00774},
volume = {3},
year = {2018}
}
@article{Bodnar2019,
abstract = {Reinforcement Learning (RL) has recently achieved tremendous success due to the partnership with Deep Neural Networks (DNNs). Genetic Algorithms (GAs), often seen as a competing approach to RL, have run out of favour due to their inability to scale up to the DNNs required to solve the most complex environments. Contrary to this dichotomic view, in the physical world, evolution and learning are complementary processes that continuously interact. The recently proposed Evolutionary Reinforcement Learning (ERL) framework has demonstrated the capacity of the two methods to enhance each other. However, ERL has not fully addressed the scalability problem of GAs. In this paper, we argue that this problem is rooted in an unfortunate combination of a simple genetic encoding for DNNs and the use of traditional biologically-inspired variation operators. When applied to these encodings, the standard operators are destructive and cause catastrophic forgetting of the traits the networks acquired. We propose a novel algorithm called Proximal Distilled Evolutionary Reinforcement Learning (PDERL) that is characterised by a hierarchical integration between evolution and learning. The main innovation of PDERL is the use of learning-based variation operators that compensate for the simplicity of the genetic representation. Unlike the traditional operators, the ones we propose meet their functional requirements. We evaluate PDERL in five robot locomotion environments from the OpenAI gym. Our method outperforms ERL, as well as two state of the art RL algorithms, PPO and TD3, in all the environments.},
archivePrefix = {arXiv},
arxivId = {1906.09807},
author = {Bodnar, Cristian and Day, Ben and Lio', Pietro},
eprint = {1906.09807},
month = {jun},
title = {{Proximal Distilled Evolutionary Reinforcement Learning}},
url = {http://arxiv.org/abs/1906.09807},
year = {2019}
}
@article{Fries2019,
abstract = {Population-scale biomedical repositories such as the UK Biobank provide unprecedented access to prospectively collected cardiac imaging data, however the majority of these data are unlabeled, creating barriers to their use in supervised machine learning. We developed a weakly supervised deep learning model for Bicuspid Aortic Valve (BAV) classification using up to 4,000 unlabeled cardiac MRI sequences. Instead of requiring curated, hand-labeled training data, weak supervision relies on noisy heuristics defined by domain experts to programmatically generate large-scale, imperfect training labels. For BAV classification, training models using these imperfect labels substantially outperformed a traditional supervised model trained on hand-labeled MRIs. In a validation experiment using long-term outcome data from the UK Biobank, our classification model identified a subset of individuals with a 1.8-fold increase in risk of a major adverse cardiac event. This work formalizes the first deep learning baseline for aortic valve classification and outlines a general strategy for using weak supervision to train machine learning models using large collections of unlabeled medical images.},
author = {Fries, Jason A. and Varma, Paroma and Chen, Vincent S. and Xiao, Ke and Tejeda, Heliodoro and Saha, Priyanka and Dunnmon, Jared and Chubb, Henry and Maskatia, Shiraz and Fiterau, Madalina and Delp, Scott and Ashley, Euan and R{\'{e}}, Christopher and Priest, James},
doi = {10.1101/339630},
issn = {2041-1723},
journal = {bioRxiv},
keywords = {Congenital heart defects,Machine learning,Magnetic resonance imaging},
month = {dec},
number = {1},
pages = {339630},
publisher = {Nature Publishing Group},
title = {{Weakly supervised classification of rare aortic valve malformations using unlabeled cardiac MRI sequences}},
url = {http://www.nature.com/articles/s41467-019-11012-3 https://www.biorxiv.org/content/early/2018/08/22/339630},
volume = {10},
year = {2018}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
isbn = {1532-4435},
issn = {15337928},
journal = {Dropout A Simple W. to Prev. Neural Networks from Overfitting},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
pmid = {23285570},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
url = {http://jmlr.org/papers/v15/srivastava14a.html},
volume = {15},
year = {2014}
}
@misc{software_featuretools,
title = {{Featuretools | An open source framework for automated feature engineering Quick Start}},
url = {https://www.featuretools.com/},
urldate = {2019-05-10}
}
@article{Velickovic2015,
abstract = {{\textcopyright} The authors 2015. Published by Oxford University Press. All rights reserved. In this paper, we investigate the potentials of utilizing multiplex networks in the context of machine learning-specifically, binary classification problems-a context which has received little attention by existing research in the area. As there exists a wide variety of real-world systems exhibiting 'natural' multiplexity, it is expected that such models would adapt to learning from multiple types of data simultaneously better than their single-layered counterparts. This claim is verified by constructing a multiplex machine learning model, which combines several Gaussian mixture hidden Markov model (GMHMM) layers into a single multiplex model (which itself behaves like a GMHMM). Afterwards, comparative evaluation of this model and the single-layered GMHMM is performed. We demonstrate that the multiplex version achieves higher performance compared with the single-layer version when faced with a binary classification problem on synthetically generated data, as well as on a biomolecular data set.},
author = {Veli{\v{c}}kovi{\'{c}}, Petar and Li{\'{o}}, Pietro},
doi = {10.1093/comnet/cnv029},
file = {::},
issn = {20511329},
journal = {J. Complex Networks},
keywords = {Classification,GMHMM,Machine learning,Multi-omics,Multiplex network},
month = {dec},
number = {4},
pages = {561--574},
publisher = {Oxford University Press},
title = {{Molecular multiplex network inference using Gaussian mixture hidden Markov models}},
url = {https://academic.oup.com/comnet/article-lookup/doi/10.1093/comnet/cnv029},
volume = {4},
year = {2016}
}
@inproceedings{Soleimani2017,
abstract = {Treatment effects can be estimated from observational data as the difference in potential outcomes. In this paper, we address the challenge of estimating the potential outcome when treatment-dose levels can vary continuously over time. Further, the outcome variable may not be measured at a regular frequency. Our proposed solution represents the treatment response curves using linear time-invariant dynamical systems---this provides a flexible means for modeling response over time to highly variable dose curves. Moreover, for multivariate data, the proposed method: uncovers shared structure in treatment response and the baseline across multiple markers; and, flexibly models challenging correlation structure both across and within signals over time. For this, we build upon the framework of multiple-output Gaussian Processes. On simulated and a challenging clinical dataset, we show significant gains in accuracy over state-of-the-art models.},
archivePrefix = {arXiv},
arxivId = {1704.02038},
author = {Soleimani, Hossein and Subbaswamy, Adarsh and Saria, Suchi},
booktitle = {Uncertain. Artif. Intell. - Proc. 33rd Conf. UAI 2017},
eprint = {1704.02038},
month = {apr},
title = {{Treatment-response models for counterfactual reasoning with continuous-time, continuous-valued interventions}},
url = {http://arxiv.org/abs/1704.02038},
year = {2017}
}
@article{Jagannatha2016,
abstract = {Sequence labeling for extraction of medical events and their attributes from unstructured text in Electronic Health Record (EHR) notes is a key step towards semantic understanding of EHRs. It has important applications in health informatics including pharmacovigilance and drug surveillance. The state of the art supervised machine learning models in this domain are based on Conditional Random Fields (CRFs) with features calculated from fixed context windows. In this application, we explored various recurrent neural network frameworks and show that they significantly outperformed the CRF models.},
archivePrefix = {arXiv},
arxivId = {1606.07953},
author = {Jagannatha, Abhyuday and Yu, Hong},
eprint = {1606.07953},
month = {jun},
pmid = {27885364},
title = {{Bidirectional Recurrent Neural Networks for Medical Event Detection in Electronic Health Records}},
url = {http://arxiv.org/abs/1606.07953},
year = {2016}
}
@article{Kroenke2001,
abstract = {OBJECTIVE: While considerable attention has focused on improving the detection of depression, assessment of severity is also important in guiding treatment decisions. Therefore, we examined the validity of a brief, new measure of depression severity.$\backslash$n$\backslash$nMEASUREMENTS: The Patient Health Questionnaire (PHQ) is a self-administered version of the PRIME-MD diagnostic instrument for common mental disorders. The PHQ-9 is the depression module, which scores each of the 9 DSM-IV criteria as "0" (not at all) to "3" (nearly every day). The PHQ-9 was completed by 6,000 patients in 8 primary care clinics and 7 obstetrics-gynecology clinics. Construct validity was assessed using the 20-item Short-Form General Health Survey, self-reported sick days and clinic visits, and symptom-related difficulty. Criterion validity was assessed against an independent structured mental health professional (MHP) interview in a sample of 580 patients.$\backslash$n$\backslash$nRESULTS: As PHQ-9 depression severity increased, there was a substantial decrease in functional status on all 6 SF-20 subscales. Also, symptom-related difficulty, sick days, and health care utilization increased. Using the MHP reinterview as the criterion standard, a PHQ-9 score {\textgreater} or =10 had a sensitivity of 88{\%} and a specificity of 88{\%} for major depression. PHQ-9 scores of 5, 10, 15, and 20 represented mild, moderate, moderately severe, and severe depression, respectively. Results were similar in the primary care and obstetrics-gynecology samples.$\backslash$n$\backslash$nCONCLUSION: In addition to making criteria-based diagnoses of depressive disorders, the PHQ-9 is also a reliable and valid measure of depression severity. These characteristics plus its brevity make the PHQ-9 a useful clinical and research tool.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kroenke, Kurt and Spitzer, Robert L and Williams, Janet B.W.},
doi = {10.1046/j.1525-1497.2001.016009606.x},
eprint = {arXiv:1011.1669v3},
isbn = {0884-8734},
issn = {08848734},
journal = {J. Gen. Intern. Med.},
keywords = {Depression,Diagnosis,Health status,Psychological tests,Screening},
month = {sep},
number = {9},
pages = {606--613},
pmid = {11556941},
publisher = {Springer},
title = {{The PHQ-9: Validity of a brief depression severity measure}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11556941 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC1495268},
volume = {16},
year = {2001}
}
@article{Chazal2017,
abstract = {Topological Data Analysis (tda) is a recent and fast growing eld providing a set of new topological and geometric tools to infer relevant features for possibly complex data. This paper is a brief introduction, through a few selected topics, to basic fundamental and practical aspects of tda for non experts. 1 Introduction and motivation Topological Data Analysis (tda) is a recent eld that emerged from various works in applied (algebraic) topology and computational geometry during the rst decade of the century. Although one can trace back geometric approaches for data analysis quite far in the past, tda really started as a eld with the pioneering works of Edelsbrunner et al. (2002) and Zomorodian and Carlsson (2005) in persistent homology and was popularized in a landmark paper in 2009 Carlsson (2009). tda is mainly motivated by the idea that topology and geometry provide a powerful approach to infer robust qualitative, and sometimes quantitative, information about the structure of data-see, e.g. Chazal (2017). tda aims at providing well-founded mathematical, statistical and algorithmic methods to infer, analyze and exploit the complex topological and geometric structures underlying data that are often represented as point clouds in Euclidean or more general metric spaces. During the last few years, a considerable eort has been made to provide robust and ecient data structures and algorithms for tda that are now implemented and available and easy to use through standard libraries such as the Gudhi library (C++ and Python) Maria et al. (2014) and its R software interface Fasy et al. (2014a). Although it is still rapidly evolving, tda now provides a set of mature and ecient tools that can be used in combination or complementary to other data sciences tools. The tdapipeline. tda has recently known developments in various directions and application elds. There now exist a large variety of methods inspired by topological and geometric approaches. Providing a complete overview of all these existing approaches is beyond the scope of this introductory survey. However, most of them rely on the following basic and standard pipeline that will serve as the backbone of this paper: 1. The input is assumed to be a nite set of points coming with a notion of distance-or similarity between them. This distance can be induced by the metric in the ambient space (e.g. the Euclidean metric when the data are embedded in R d) or come as an intrinsic metric dened by a pairwise distance matrix. The denition of the metric on the data is usually given as an input or guided by the application. It is however important to notice that the choice of the metric may be critical to reveal interesting topological and geometric features of the data.},
archivePrefix = {arXiv},
arxivId = {1710.04019},
author = {Chazal, Fr{\'{e}}d{\'{e}}ric and Michel, Bertrand},
eprint = {1710.04019},
journal = {undefined},
title = {{An introduction to Topological Data Analysis: fundamental and practical aspects for data scientists}},
url = {https://www.semanticscholar.org/paper/An-introduction-to-Topological-Data-Analysis{\%}3A-and-Chazal-Michel/aff16209e232d38fc94a5b0c72067b88d106453f http://arxiv.org/abs/1710.04019},
year = {2017}
}
@article{Winter2013,
abstract = {This text is a conceptual introduction to mixed effects modeling with linguistic applications, using the R programming environment. The reader is introduced to linear modeling and assumptions, as well as to mixed effects/multilevel modeling, including a discussion of random intercepts, random slopes and likelihood ratio tests. The example used throughout the text focuses on the phonetic analysis of voice pitch data.},
archivePrefix = {arXiv},
arxivId = {1308.5499},
author = {Winter, Bodo},
eprint = {1308.5499},
month = {aug},
title = {{Linear models and linear mixed effects models in R with linguistic applications}},
url = {http://arxiv.org/abs/1308.5499},
year = {2013}
}
@article{Jostins2016,
abstract = {Motivation: For many classes of disease the same genetic risk variants underly many related phenotypes or disease subtypes. Multinomial logistic regression provides an attractive framework to analyse multi-category phenotypes, and explore the genetic relationships between these phenotype categories. We introduce Trinculo, a program that implements a wide range of multinomial analyses in a single fast package that is designed to be easy to use by users of standard genome-wide association study software. Availability: An open source C++ implementation, with code and binaries for Linux and Mac OSX, is available for download at http://sourceforge.net/projects/trinculo/ Contact: lj4@well.ox.ac.uk},
author = {Jostins, Luke and McVean, Gilean},
doi = {10.1093/bioinformatics/btw075},
isbn = {0387954570},
issn = {14602059},
journal = {Bioinformatics},
month = {jun},
number = {12},
pages = {1898--1900},
pmid = {26873930},
publisher = {Oxford University Press},
title = {{Trinculo: Bayesian and frequentist multinomial logistic regression for genome-wide association studies of multi-category phenotypes}},
url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btw075},
volume = {32},
year = {2016}
}
@article{Lee2019c,
abstract = {Single cell technologies have made it possible to profile millions of cells, but for these resources to be useful they must be easy to query and access. To facilitate interactive and intuitive access to single cell data we have developed scfind, a search engine for cell atlases. Using transcriptome data from mouse cell atlases we show how scfind can be used to evaluate marker genes, to perform in silico gating, and to identify both cell-type specific and housekeeping genes. Moreover, we have developed a subquery optimization routine to ensure that long and complex queries return meaningful results. To make scfind more user friendly and accessible, we use indices of PubMed abstracts and techniques from natural language processing to allow for arbitrary queries. Finally, we show how scfind can be used for multi-omics analyses by combining single-cell ATAC-seq data with transcriptome data.},
author = {Lee, Jimmy Tsz Hang and Patikas, Nikolaos and Kiselev, Vladimir Yu and Hemberg, Martin},
doi = {10.1101/788596},
journal = {bioRxiv},
pages = {788596},
publisher = {Cold Spring Harbor Laboratory},
title = {{Fast searches of large collections of single cell data using scfind}},
url = {https://www.biorxiv.org/content/10.1101/788596v1?rss=1},
year = {2019}
}
@misc{automl_article,
abstract = {http://www.kdnuggets.com/2017/01/current-state-automated-machine-learning.html},
title = {{The Current State of Automated Machine Learning}},
url = {http://www.kdnuggets.com/2017/01/current-state-automated-machine-learning.html},
urldate = {2017-08-17}
}
@article{Carter2019,
abstract = {By using feature inversion to visualize millions of activations from an image classification network, we create an explorable activation atlas of features the network has learned which can reveal how the network typically represents some concepts.},
author = {Carter, Shan and Armstrong, Zan and Schubert, Ludwig and Johnson, Ian and Olah, Chris},
doi = {10.23915/distill.00015},
issn = {2476-0757},
journal = {Distill},
month = {mar},
number = {3},
pages = {e15},
title = {{Activation Atlas}},
url = {https://distill.pub/2019/activation-atlas},
volume = {4},
year = {2019}
}
@misc{software_deepmind_sonnet,
title = {{deepmind/sonnet: TensorFlow-based neural network library}},
url = {https://github.com/deepmind/sonnet},
urldate = {2019-08-18}
}
@article{Sine2020,
author = {Sine, Veronica},
doi = {10.5642/jhummath.202001.17},
issn = {21598118},
journal = {J. Humanist. Math.},
month = {jan},
number = {1},
pages = {364--367},
title = {{The Healing Powers of Mathematics in the Age of {\#}metoo}},
url = {http://scholarship.claremont.edu/jhm/vol10/iss1/17/},
volume = {10},
year = {2020}
}
@article{Ustun2016b,
abstract = {Risk scores are simple classification models that let users make quick risk predictions by adding and subtracting a few small numbers. These models are widely used in medicine and criminal justice, but are difficult to learn from data because they need to be calibrated, sparse, use small integer coefficients and obey application-specific constraints. In this paper, we present a new machine learning approach to learn risk scores. We formulate the risk score problem as a mixed integer nonlinear program, and present a new cutting plane algorithm for non-convex settings to efficiently recover its optimal solution. We improve our algorithm with specialized techniques to generate feasible solutions, narrow the optimality gap, and reduce data-related computation. Our approach can fit risk scores in a way that scales linearly in the number of samples, provides a certificate of optimality, and obeys real-world constraints without parameter tuning or post-processing. We illustrate the performance benefits of this approach through an extensive set of numerical experiments, where we compare risk scores built using our approach to those built using heuristic approaches. We also discuss the practical benefits of our approach through an application where we build a customized risk score for ICU seizure prediction in collaboration with the Massachusetts General Hospital.},
archivePrefix = {arXiv},
arxivId = {1610.00168},
author = {Ustun, Berk and Rudin, Cynthia},
eprint = {1610.00168},
month = {oct},
title = {{Learning Optimized Risk Scores}},
url = {http://arxiv.org/abs/1610.00168},
year = {2016}
}
@article{Ke2019,
abstract = {Meta-learning over a set of distributions can be interpreted as learning different types of parameters corresponding to short-term vs long-term aspects of the mechanisms underlying the generation of data. These are respectively captured by quickly-changing parameters and slowly-changing meta-parameters. We present a new framework for meta-learning causal models where the relationship between each variable and its parents is modeled by a neural network, modulated by structural meta-parameters which capture the overall topology of a directed graphical model. Our approach avoids a discrete search over models in favour of a continuous optimization procedure. We study a setting where interventional distributions are induced as a result of a random intervention on a single unknown variable of an unknown ground truth causal model, and the observations arising after such an intervention constitute one meta-example. To disentangle the slow-changing aspects of each conditional from the fast-changing adaptations to each intervention, we parametrize the neural network into fast parameters and slow meta-parameters. We introduce a meta-learning objective that favours solutions robust to frequent but sparse interventional distribution change, and which generalize well to previously unseen interventions. Optimizing this objective is shown experimentally to recover the structure of the causal graph.},
archivePrefix = {arXiv},
arxivId = {1910.01075},
author = {Ke, Nan Rosemary and Bilaniuk, Olexa and Goyal, Anirudh and Bauer, Stefan and Larochelle, Hugo and Pal, Chris and Bengio, Yoshua},
eprint = {1910.01075},
month = {oct},
title = {{Learning Neural Causal Models from Unknown Interventions}},
url = {http://arxiv.org/abs/1910.01075},
year = {2019}
}
@article{Cranmer2019,
abstract = {We introduce an approach for imposing physically motivated inductive biases on graph networks to learn interpretable representations and improved zero-shot generalization. Our experiments show that our graph network models, which implement this inductive bias, can learn message representations equivalent to the true force vector when trained on n-body gravitational and spring-like simulations. We use symbolic regression to fit explicit algebraic equations to our trained model's message function and recover the symbolic form of Newton's law of gravitation without prior knowledge. We also show that our model generalizes better at inference time to systems with more bodies than had been experienced during training. Our approach is extensible, in principle, to any unknown interaction law learned by a graph network, and offers a valuable technique for interpreting and inferring explicit causal theories about the world from implicit knowledge captured by deep learning.},
archivePrefix = {arXiv},
arxivId = {1909.05862},
author = {Cranmer, Miles D. and Xu, Rui and Battaglia, Peter and Ho, Shirley},
eprint = {1909.05862},
month = {sep},
title = {{Learning Symbolic Physics with Graph Networks}},
url = {http://arxiv.org/abs/1909.05862},
year = {2019}
}
@article{Banerjeec,
author = {Banerjee, Soumya},
doi = {10.31219/OSF.IO/AGVK5},
keywords = {Artificial Intelligence and Robotics,Computer Engineering,Computer Sciences,Engineering,Physical Sciences and Mathematics},
number = {8},
pages = {1--2},
publisher = {OSF Preprints},
title = {{Bayesian LASSO : Implementation in MATLAB}},
url = {https://osf.io/agvk5/}
}
@article{Lee2018,
abstract = {Nitazoxanide (Alinia, Romark Laboratories) was synthesized based on the structure of niclosamide. In vitro studies have demonstrated activity against a broad range of parasites as well as some bacteria. Three controlled trials demonstrated efficacy in cryptosporidiosis, however, the efficacy in advanced AIDS patients (CD4 cell counts = 50) at approved doses was limited. Trials have also demonstrated efficacy comparable to metronidazole (Flagyl, GD Searle and Co.) in giardiasis with fewer side effects. Nitazoxanide is also effective versus intestinal helminths and tapeworms as well as in chronic fascioliasis. Side effects in clinical trials have been similar to placebo. Nitazoxanide is the first agent proven to be effective in cryptosporidiosis. It has also proven efficacy in giardiasis. Nitazoxanide is efficacious again intestinal helminths. Additional indications may be developed in the future.},
author = {Lee, Changhee and Zame, William R. and Yoon, Jinsung and {Van Der Schaar}, Mihaela},
isbn = {9781577358008},
issn = {1478-7210},
journal = {AAAI2018},
title = {{DeepHit: A Deep Learning Approach to Survival Analysis with Competing Risks}},
url = {https://www.semanticscholar.org/paper/DeepHit{\%}3A-A-Deep-Learning-Approach-to-Survival-With-Lee-Zame/803a7b26bdc0feafbf45bc5d57c2bc3f55b6f8fc http://medianetlab.ee.ucla.edu/papers/AAAI{\_}2018{\_}DeepHit},
year = {2018}
}
@misc{data_global_burden,
title = {{Global Burden of Disease Study 2017 (GBD 2017) Data Resources | GHDx}},
url = {http://ghdx.healthdata.org/gbd-2017},
urldate = {2019-08-10}
}
@article{Chang2010b,
abstract = {Background: Higher mortality has been found for people with serious mental illness (SMI, including schizophrenia, schizoaffective disorders, and bipolar affective disorder) at all age groups. Our aim was to characterize vulnerable groups for excess mortality among people with SMI, substance use disorders, depressive episode, and recurrent depressive disorder.Methods: A case register was developed at the South London and Maudsley National Health Services Foundation Trust (NHS SLAM), accessing full electronic clinical records on over 150,000 mental health service users as a well-defined cohort since 2006. The Case Register Interactive Search (CRIS) system enabled searching and retrieval of anonymised information since 2008. Deaths were identified by regular national tracing returns after 2006. Standardized mortality ratios (SMRs) were calculated for the period 2007 to 2009 using SLAM records for this period and the expected number of deaths from age-specific mortality statistics for the England and Wales population in 2008. Data were stratified by gender, ethnicity, and specific mental disorders.Results: A total of 31,719 cases, aged 15 years old or more, active between 2007-2009 and with mental disorders of interest prior to 2009 were detected in the SLAM case register. SMRs were 2.15 (95{\%} CI: 1.95-2.36) for all SMI with genders combined, 1.89 (1.64-2.17) for women and 2.47 (2.17-2.80) for men. In addition, highest mortality risk was found for substance use disorders (SMR = 4.17; 95{\%} CI: 3.75-4.64). Age- and gender-standardised mortality ratios by ethnic group revealed huge fluctuations, and SMRs for all disorders diminished in strength with age. The main limitation was the setting of secondary mental health care provider in SLAM.Conclusions: Substantially higher mortality persists in people with serious mental illness, substance use disorders and depressive disorders. Furthermore, mortality risk differs substantially with age, diagnosis, gender and ethnicity. Further research into specific risk groups is required. {\textcopyright} 2010 Chang et al; licensee BioMed Central Ltd.},
author = {Chang, Chin Kuo and Hayes, Richard D and Broadbent, Matthew and Fernandes, Andrea C and Lee, William and Hotopf, Matthew and Stewart, Robert},
doi = {10.1186/1471-244X-10-77},
isbn = {1471-244X (Electronic)$\backslash$r1471-244X (Linking)},
issn = {1471244X},
journal = {BMC Psychiatry},
keywords = {Psychiatry,Psychotherapy},
month = {dec},
number = {1},
pages = {77},
pmid = {20920287},
publisher = {BioMed Central},
title = {{All-cause mortality among people with serious mental illness (SMI), substance use disorders, and depressive disorders in southeast London: A cohort study}},
url = {http://bmcpsychiatry.biomedcentral.com/articles/10.1186/1471-244X-10-77},
volume = {10},
year = {2010}
}
@article{Hinton2007a,
abstract = {To achieve its impressive performance in tasks such as speech perception or object recognition, the brain extracts multiple levels of representation from the sensory input. Backpropagation was the first computationally efficient model of how neural networks could learn multiple layers of representation, but it required labeled training data and it did not work well in deep networks. The limitations of backpropagation learning can now be overcome by using multilayer neural networks that contain top-down connections and training them to generate sensory data rather than to classify it. Learning multilayer generative models might seem difficult, but a recent discovery makes it easy to learn nonlinear distributed representations one layer at a time.},
author = {Hinton, Geoffrey E.},
doi = {10.1016/j.tics.2007.09.004},
issn = {13646613},
journal = {Trends Cogn. Sci.},
month = {oct},
number = {10},
pages = {428--434},
pmid = {17921042},
title = {{Learning multiple layers of representation}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17921042 https://linkinghub.elsevier.com/retrieve/pii/S1364661307002173},
volume = {11},
year = {2007}
}
@misc{Banerjeed_madonna_conflict,
author = {Banerjee, Soumya},
title = {{neelsoumya / repository{\_}conflict{\_}diseases — Bitbucket}},
url = {https://bitbucket.org/neelsoumya/repository{\_}conflict{\_}diseases/src/master/},
urldate = {2019-12-10}
}
@article{Doshi-Velez2017,
abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
archivePrefix = {arXiv},
arxivId = {1702.08608},
author = {Doshi-Velez, Finale and Kim, Been},
doi = {10.1016/j.bbrc.2004.04.155},
eprint = {1702.08608},
isbn = {9781450360128},
issn = {0006-291X},
month = {feb},
pmid = {15178405},
title = {{Towards A Rigorous Science of Interpretable Machine Learning}},
url = {http://arxiv.org/abs/1702.08608},
year = {2017}
}
@article{Mulder2018,
abstract = {Bioinformatics is recognized as part of the essential knowledge base of numerous career paths in biomedical research and healthcare. However, there is little agreement in the field over what that knowledge entails or how best to provide it. These disagreements are compounded by the wide range of populations in need of bioinformatics training, with divergent prior backgrounds and intended application areas. The Curriculum Task Force of the International Society of Computational Biology (ISCB) Education Committee has sought to provide a framework for training needs and curricula in terms of a set of bioinformatics core competencies that cut across many user personas and training programs. The initial competencies developed based on surveys of employers and training programs have since been refined through a multiyear process of community engagement. This report describes the current status of the competencies and presents a series of use cases illustrating how they are being applied in diverse training contexts. These use cases are intended to demonstrate how others can make use of the competencies and engage in the process of their continuing refinement and application. The report concludes with a consideration of remaining challenges and future plans.},
author = {Mulder, Nicola and Schwartz, Russell and Brazas, Michelle D. and Brooksbank, Cath and Gaeta, Bruno and Morgan, Sarah L. and Pauley, Mark A. and Rosenwald, Anne and Rustici, Gabriella and Sierk, Michael and Warnow, Tandy and Welch, Lonnie},
doi = {10.1371/journal.pcbi.1005772},
editor = {Troyanskaya, Olga G.},
issn = {15537358},
journal = {PLoS Comput. Biol.},
month = {feb},
number = {2},
pages = {e1005772},
publisher = {Public Library of Science},
title = {{The development and application of bioinformatics core competencies to improve bioinformatics training and education}},
url = {https://dx.plos.org/10.1371/journal.pcbi.1005772},
volume = {14},
year = {2018}
}
@article{Ferreira2019,
abstract = {Global fire activity has a huge impact on human lives. In recent years, many fire models have been developed to forecast fire activity. They present good results for some regions but require complex parametrizations and input variables that are not easily obtained or estimated. In this paper, we evaluate the possibility of using historical data from 2003 to 2017 of active fire detections (NASA's MODIS MCD14ML C6) and time series forecasting methods to estimate global fire season severity (FSS), here defined as the accumulated fire detections in a season. We used a hexagonal grid to divide the globe, and we extracted time series of daily fire counts from each cell. We propose a straightforward method to estimate the fire season lengths. Our results show that in 99{\%} of the cells, the fire seasons have lengths shorter than seven months. Given this result, we extracted the fire seasons defined as time windows of seven months centered in the months with the highest fire occurrence. We define fire season severity (FSS) as the accumulated fire detections in a season. A trend analysis suggests a global decrease in length and severity. Since FSS time series are concise, we used the monthly-accumulated fire counts (MA-FC) to train and test the seven forecasting models. Results show low forecasting errors in some areas. Therefore we conclude that many regions present predictable variations in the FSS.},
archivePrefix = {arXiv},
arxivId = {1903.06667},
author = {Ferreira, Leonardo N. and Vega-oliveros, Didier A and Zhao, Liang and Cardoso, Manoel F.},
eprint = {1903.06667},
keywords = {change,climate,fire season length,fire severity,global fire activity,time series prediction,wildfire},
month = {mar},
pages = {1--14},
title = {{Global Fire Season Severity Analysis and Forecasting}},
url = {http://arxiv.org/abs/1903.06667},
year = {2019}
}
@article{Hidalgo2009,
abstract = {The use of networks to integrate different genetic, proteomic, and metabolic datasets has been proposed as a viable path toward elucidating the origins of specific diseases. Here we introduce a new phenotypic database summarizing correlations obtained from the disease history of more than 30 million patients in a Phenotypic Disease Network (PDN). We present evidence that the structure of the PDN is relevant to the understanding of illness progression by showing that (1) patients develop diseases close in the network to those they already have; (2) the progression of disease along the links of the network is different for patients of different genders and ethnicities; (3) patients diagnosed with diseases which are more highly connected in the PDN tend to die sooner than those affected by less connected diseases; and (4) diseases that tend to be preceded by others in the PDN tend to be more connected than diseases that precede other illnesses, and are associated with higher degrees of mortality. Our findings show that disease progression can be represented and studied using network methods, offering the potential to enhance our understanding of the origin and evolution of human diseases. The dataset introduced here, released concurrently with this publication, represents the largest relational phenotypic resource publicly available to the research community.},
author = {Hidalgo, C{\'{e}}sar A. and Blumm, Nicholas and Barab{\'{a}}si, Albert L{\'{a}}szl{\'{o}} and Christakis, Nicholas A.},
doi = {10.1371/journal.pcbi.1000353},
editor = {Meyers, Lauren Ancel},
issn = {1553734X},
journal = {PLoS Comput. Biol.},
month = {apr},
number = {4},
pages = {e1000353},
publisher = {Public Library of Science},
title = {{A Dynamic Network Approach for the Study of Human Phenotypes}},
url = {https://dx.plos.org/10.1371/journal.pcbi.1000353},
volume = {5},
year = {2009}
}
@article{Butler2017,
archivePrefix = {arXiv},
arxivId = {164889},
author = {Butler, Andrew and Satija, Rahul},
doi = {10.1101/164889},
eprint = {164889},
isbn = {0780351355},
journal = {bioRxiv},
title = {{Integrated analysis of single cell transcriptomic data across conditions, technologies, and species}},
url = {http://www.biorxiv.org/content/early/2017/07/18/164889},
year = {2017}
}
@article{Carpenter2017a,
abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.2.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propa- gation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line, through R using the RStan package, or through Python using the PyStan package. All three interfaces support sampling or optimization-based inference and analysis, and RStan and PyStan also provide access to log probabilities, gradients, Hessians, and data I/O.},
author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
doi = {10.18637/jss.v076.i01},
issn = {1548-7660},
journal = {J. Stat. Softw.},
keywords = {algorithmic differentiation,bayesian inference,mathematics and computing,probabilistic program,stan},
month = {jan},
number = {1},
title = {{Stan : A Probabilistic Programming Language}},
url = {http://www.jstatsoft.org/v76/i01/},
volume = {76},
year = {2017}
}
@book{Dennett,
abstract = {First edition. Daniel Dennett, one of the world's most original and provocative thinkers, takes readers on a profound, illuminating and highly entertaining philosophical journey as he reveals a collection of his favorite thinking tools, or "intuition pumps", that he and others have developed for addressing life's most fundamental questions. A philosophy professor offers exercises and tools to stretch the mind, offering new ways to consider, discuss, and argue positions on dangerous subject matter including evolution, the meaning of life, and free will. A dozen general thinking tools -- Tools for thinking about meaning or content -- An interlude about computers -- More tools about meaning -- Tools for thinking about evolution -- Tools for thinking about consciousness -- Tools for thinking about free will -- What is it like to be a philosopher? -- Use the tools, try harder -- What got left out.},
author = {Dennett, D. C. (Daniel Clement)},
isbn = {1846144752},
pages = {496},
title = {{Intuition pumps and other tools for thinking}}
}
@article{Frot2018,
abstract = {We consider the problem of learning a conditional Gaussian graphical model in the presence of latent variables. Building on recent advances in this field, we suggest a method that decomposes the parameters of a conditional Markov random field into the sum of a sparse and a low-rank matrix. We derive convergence bounds for this estimator and show that it is well-behaved in the high-dimensional regime as well as " sparsistent " (i.e. capable of recovering the graph structure). We then show how proximal gradient algorithms and semi-definite programming techniques can be em-ployed to fit the model to thousands of variables. Through extensive simulations, we * BF gratefully acknowledges the EPSRC and Amazon Web Services. LJ is supported by a Wellcome Trust grant (098759/Z/12/Z) and Christ Church, Oxford. GM is funded by the Wellcome Trust grant 100956/Z/13/Z. This work was jointly supervised by LJ and GM. 1 illustrate the conditions required for identifiability and show that there is a wide range of situations in which this model performs significantly better than its counterparts, for example, by accommodating more latent variables. Finally, the suggested method is applied to two datasets comprising individual level data on genetic variants and metabolites levels. We show our results replicate better than alternative approaches and show enriched biological signal.},
author = {Frot, Benjamin and Jostins, Luke},
doi = {10.1080/01621459.2018.1434531},
issn = {0162-1459},
journal = {J. Am. Stat. Assoc.},
keywords = {ALSPAC,Conditional Markov random field,Genetics,Low-Rank plus Sparse,Metabolites,Model Selection,Multivariate Analysis},
month = {feb},
pages = {1--36},
publisher = {Taylor {\&} Francis},
title = {{Graphical Model Selection for Gaussian Conditional Random Fields in the Presence of Latent Variables}},
url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2018.1434531},
year = {2017}
}
@article{Wu2019,
abstract = {Coarse-grained rules are widely used in chemistry, physics and engineering. In biology, however, such rules are less common and under-appreciated. This gap can be attributed to the difficulty in establishing general rules to encompass the immense diversity and complexity of biological systems. Furthermore, even when a rule is established, it is often challenging to map it to mechanistic details and to quantify these details. Here we report a framework that addresses these challenges for mutualistic systems. We first deduce a general rule that predicts the various outcomes of mutualistic systems, including coexistence and productivity. We further develop a standardized machine-learning-based calibration procedure to use the rule without the need to fully elucidate or characterize their mechanistic underpinnings. Our approach consistently provides explanatory and predictive power with various simulated and experimental mutualistic systems. Our strategy can pave the way for establishing and implementing other simple rules for biological systems.},
author = {Wu, Feilun and Lopatkin, Allison J. and Needs, Daniel A. and Lee, Charlotte T. and Mukherjee, Sayan and You, Lingchong},
doi = {10.1038/s41467-018-08188-5},
issn = {20411723},
journal = {Nat. Commun.},
keywords = {Computer modelling,Microbial ecology,Synthetic biology},
month = {dec},
number = {1},
pages = {242},
publisher = {Nature Publishing Group},
title = {{A unifying framework for interpreting and predicting mutualistic systems}},
url = {http://www.nature.com/articles/s41467-018-08188-5},
volume = {10},
year = {2019}
}
@article{Rodrigo2014,
abstract = {Background: Several studies have shown that long-term lithium use is associated with a subtle decline in estimated glomerular filtration rate. This study compared mean estimated glomerular filtration rates (eGFR) in patients on long term lithium, against matched controls. Methods: Patients with bipolar affective disorder, who are on lithium (for at least a year), were compared against controls that were matched (1:1) for age, gender and presence or absence of diabetes or hypertension. The eGFR was calculated from creatinine values according to the 'modification of diet in renal disease study' (MDRD) formula and was compared between cases and controls. A meta-analysis was performed to compare our findings with similar studies in literature. Results: Forty seven patients met the inclusion criteria. They were matched with 47 controls. The eGFR values of lithium users were significantly lower (p = 0.04) compared to controls. This difference persisted between the subgroup of lithium users without comorbidities (diabetes and hypertension) and their controls but disappeared for lithium users with comorbidities and their controls. Nonetheless, lithium users had lower eGFR values in both subgroups. A meta-analysis of 9 studies showed a significant lowering in the glomerular filtration rate in lithium users compared to controls [mean difference -10.3 ml/min (95{\%} confidence interval: -15.13 to -5.55, p {\textless} 0.0001)]. Conclusions: Lithium causes a subtle decline in glomerular filtration rate; renal function needs to be monitored in patients on lithium treatment. {\textcopyright} 2014 Rodrigo et al.; licensee BioMed Central Ltd.},
author = {Rodrigo, Chaturaka and de Silva, Nipun Lakshitha and Gunaratne, Ravindi and Rajapakse, Senaka and {De Silva}, Varuni Asanka and Hanwella, Raveen},
doi = {10.1186/1471-244X-14-4},
issn = {1471244X},
journal = {BMC Psychiatry},
month = {jan},
number = {1},
pages = {4},
pmid = {24400671},
publisher = {BioMed Central},
title = {{Lower estimated glomerular filtration rates in patients on long term lithium: A comparative study and a meta-analysis of literature}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24400671 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3893601},
volume = {14},
year = {2014}
}
@book{Gareth2017,
author = {Gareth, James and Daniela, Witten and Trevor, Hastie and Robert, Tibshirani},
publisher = {Springer},
title = {{Introduction to Statistical Learning with Applications in R}},
url = {http://www-bcf.usc.edu/{~}gareth/ISL/},
year = {2017}
}
@inproceedings{Levin2018,
abstract = {Early detection of emerging disease outbreaks is crucial to effective containment and response, yet initial outbreak signatures can be difficult to detect with automated methods. Outbreaks may be masked by noisy data, and signs of an outbreak may be hidden across multiple data feeds. Current biosurveillance methods often perform unimodal statistical anal- yses that are unable to intelligently leverage multiple correlated data of different types while still retaining quantitative sensitivity. In this paper, we propose and implement an anomaly detection system for health data based upon the human immune system. The adaptive immune system operates over a high-dimensional antigen space in a distributed manner, allowing it to efficiently scale without relying on a centralized controller. Our negative se- lection algorithm based on the immune system provides effective and scalable distributed anomaly detection for biosurveillance. It detects anomalies in the large, complex data from modern health monitoring data feeds with low false positive rates. Our bootstrap aggregation method improves performance on high-dimensional data sets, and we implement a parallelized version of the algo- rithm to demonstrate the potential to implement it on a scalable distributed architecture. Our negative selection algorithm is able to detect 90{\%} of all outbreaks with a false positive rate of 11.8{\%} in a publicly available multimodal synthetic health record data set. The scalability and performance of the negative selection algorithm demonstrate that immune computation can provide effective approaches for national and global scale biosurveillence.},
author = {Levin, Drew and Moses, Melanie and Flanagan, Tatiana and Forrest, Stephanie and Finley, Patrick},
booktitle = {2017 IEEE Symp. Ser. Comput. Intell. SSCI 2017 - Proc.},
doi = {10.1109/SSCI.2017.8285356},
isbn = {9781538627259},
issn = {nan},
keywords = {anomaly detection,artificial immune system,bootstrap aggregation,negative selection},
month = {nov},
pages = {1--7},
publisher = {IEEE},
title = {{Negative selection based anomaly detector for multimodal health data}},
url = {http://ieeexplore.ieee.org/document/8285356/},
volume = {2018-Janua},
year = {2018}
}
@article{Coupland2019,
abstract = {Importance Anticholinergic medicines have short-term cognitive adverse effects, but it is uncertain whether long-term use of these drugs is associated with an increased risk of dementia. Objective To assess associations between anticholinergic drug treatments and risk of dementia in persons 55 years or older. Design, Setting, and Participants This nested case-control study took place in general practices in England that contributed to the QResearch primary care database. The study evaluated whether exposure to anticholinergic drugs was associated with dementia risk in 58 769 patients with a diagnosis of dementia and 225 574 controls 55 years or older matched by age, sex, general practice, and calendar time. Information on prescriptions for 56 drugs with strong anticholinergic properties was used to calculate measures of cumulative anticholinergic drug exposure. Data were analyzed from May 2016 to June 2018. Exposures The primary exposure was the total standardized daily doses (TSDDs) of anticholinergic drugs prescribed in the 1 to 11 years prior to the date of diagnosis of dementia or equivalent date in matched controls (index date). Main Outcomes and Measures Odds ratios (ORs) for dementia associated with cumulative exposure to anticholinergic drugs, adjusted for confounding variables. Results Of the entire study population (284 343 case patients and matched controls), 179 365 (63.1{\%}) were women, and the mean (SD) age of the entire population was 82.2 (6.8) years. The adjusted OR for dementia increased from 1.06 (95{\%} CI, 1.03-1.09) in the lowest overall anticholinergic exposure category (total exposure of 1-90 TSDDs) to 1.49 (95{\%} CI, 1.44-1.54) in the highest category ({\textgreater}1095 TSDDs), compared with no anticholinergic drug prescriptions in the 1 to 11 years before the index date. There were significant increases in dementia risk for the anticholinergic antidepressants (adjusted OR [AOR], 1.29; 95{\%} CI, 1.24-1.34), antiparkinson drugs (AOR, 1.52; 95{\%} CI, 1.16-2.00), antipsychotics (AOR, 1.70; 95{\%} CI, 1.53-1.90), bladder antimuscarinic drugs (AOR, 1.65; 95{\%} CI, 1.56-1.75), and antiepileptic drugs (AOR, 1.39; 95{\%} CI, 1.22-1.57) all for more than 1095 TSDDs. Results were similar when exposures were restricted to exposure windows of 3 to 13 years (AOR, 1.46; 95{\%} CI, 1.41-1.52) and 5 to 20 years (AOR, 1.44; 95{\%} CI, 1.32-1.57) before the index date for more than 1095 TSDDs. Associations were stronger in cases diagnosed before the age of 80 years. The population-attributable fraction associated with total anticholinergic drug exposure during the 1 to 11 years before diagnosis was 10.3{\%}. Conclusions and Relevance Exposure to several types of strong anticholinergic drugs is associated with an increased risk of dementia. These findings highlight the importance of reducing exposure to anticholinergic drugs in middle-aged and older people.},
author = {Coupland, Carol A. C. and Hill, Trevor and Dening, Tom and Morriss, Richard and Moore, Michael and Hippisley-Cox, Julia},
doi = {10.1001/jamainternmed.2019.0677},
issn = {2168-6106},
journal = {JAMA Intern. Med.},
keywords = {anticholinergic agents,anticonvulsants,antidepressive agents,antiparkinson agents,antipsychotic agents,bladder,dementia,muscarinic antagonists},
month = {jun},
title = {{Anticholinergic Drug Exposure and the Risk of Dementia}},
url = {http://archinte.jamanetwork.com/article.aspx?doi=10.1001/jamainternmed.2019.0677},
year = {2019}
}
@article{Tran2014,
abstract = {BACKGROUND: To date, our ability to accurately identify patients at high risk from suicidal behaviour, and thus to target interventions, has been fairly limited. This study examined a large pool of factors that are potentially associated with suicide risk from the comprehensive electronic medical record (EMR) and to derive a predictive model for 1-6 month risk.$\backslash$n$\backslash$nMETHODS: 7,399 patients undergoing suicide risk assessment were followed up for 180 days. The dataset was divided into a derivation and validation cohorts of 4,911 and 2,488 respectively. Clinicians used an 18-point checklist of known risk factors to divide patients into low, medium, or high risk. Their predictive ability was compared with a risk stratification model derived from the EMR data. The model was based on the continuation-ratio ordinal regression method coupled with lasso (which stands for least absolute shrinkage and selection operator).$\backslash$n$\backslash$nRESULTS: In the year prior to suicide assessment, 66.8{\%} of patients attended the emergency department (ED) and 41.8{\%} had at least one hospital admission. Administrative and demographic data, along with information on prior self-harm episodes, as well as mental and physical health diagnoses were predictive of high-risk suicidal behaviour. Clinicians using the 18-point checklist were relatively poor in predicting patients at high-risk in 3 months (AUC 0.58, 95{\%} CIs: 0.50 - 0.66). The model derived EMR was superior (AUC 0.79, 95{\%} CIs: 0.72 - 0.84). At specificity of 0.72 (95{\%} CIs: 0.70-0.73) the EMR model had sensitivity of 0.70 (95{\%} CIs: 0.56-0.83).$\backslash$n$\backslash$nCONCLUSION: Predictive models applied to data from the EMR could improve risk stratification of patients presenting with potential suicidal behaviour. The predictive factors include known risks for suicide, but also other information relating to general health and health service utilisation.},
author = {Tran, Truyen and Luo, Wei and Phung, Dinh and Harvey, Richard and Berk, Michael and Kennedy, Richard Lee and Venkatesh, Svetha},
doi = {10.1186/1471-244X-14-76},
isbn = {1471-244X},
issn = {1471244X},
journal = {BMC Psychiatry},
keywords = {Electronic medical record,Predictive models,Suicide risk},
month = {dec},
number = {1},
pages = {76},
pmid = {24628849},
publisher = {BioMed Central},
title = {{Risk stratification using data from electronic medical records better predicts suicide risks than clinician assessments}},
url = {http://bmcpsychiatry.biomedcentral.com/articles/10.1186/1471-244X-14-76},
volume = {14},
year = {2014}
}
@article{Scata2018,
abstract = {Heterogeneity of human beings leads to think and react differently to social phenomena. Awareness and homophily drive people to weigh interactions in social multiplex networks, influencing a potential contagion effect. To quantify the impact of heterogeneity on spreading dynamics, we propose a model of coevolution of social contagion and awareness, through the introduction of statistical estimators, in a weighted multiplex network. Multiplexity of networked individuals may trigger propagation enough to produce effects among vulnerable subjects experiencing distress, mental disorder, which represent some of the strongest predictors of suicidal behaviours. The exposure to suicide is emotionally harmful, since talking about it may give support or inadvertently promote it. To disclose the complex effect of the overlapping awareness on suicidal ideation spreading among disordered people, we also introduce a data-driven approach by integrating different types of data. Our modelling approach unveils the relationship between distress and mental disorders propagation and suicidal ideation spreading, shedding light on the role of awareness in a social network for suicide prevention. The proposed model is able to quantify the impact of overlapping awareness on suicidal ideation spreading and our findings demonstrate that it plays a dual role on contagion, either reinforcing or delaying the contagion outbreak.},
author = {Scat{\`{a}}, Marialisa and {Di Stefano}, Alessandro and {La Corte}, Aurelio and Li{\`{o}}, Pietro},
doi = {10.1038/s41598-018-23260-2},
issn = {2045-2322},
journal = {Sci. Rep.},
keywords = {Complex networks,Psychiatric disorders},
month = {dec},
number = {1},
pages = {5005},
publisher = {Nature Publishing Group},
title = {{Quantifying the propagation of distress and mental disorders in social networks}},
url = {http://www.nature.com/articles/s41598-018-23260-2},
volume = {8},
year = {2018}
}
@inproceedings{Sutton2018,
abstract = {Many analyses in data science are not one-off projects, but are repeated over multiple data samples, such as once per month, once per quarter, and so on. For example, if a data scientist performs an analysis in 2017 that saves a significant amount of money, then she will likely to be asked to perform the same analysis on data from 2018. But more data analyses means more effort spent in data wrangling.We introduce the data diff problem, which attempts to turn this problem into an opportunity. Comparing the repeated data samples against each other, inconsistencies may be indicative of underlying issues in data quality. By analogy to text diff, the data diff problem is to find a “patch”, that is, transformation in a specified domain-specific language, that transforms the data samples so that they are identically distributed.We present a prototype tool for data diff that formalizes the problem as a bipartite matching problem, calibrating its parameters using a bootstrap procedure. The tool is evaluated quantitatively and through a case study on an open government data set.},
address = {New York, New York, USA},
author = {Sutton, Charles and Hobson, Timothy and Geddes, James and Caruana, Rich},
booktitle = {Proc. 24th ACM SIGKDD Int. Conf. Knowl. Discov. Data Min. - KDD '18},
doi = {10.1145/3219819.3220057},
isbn = {9781450355520},
pages = {2279--2288},
publisher = {ACM Press},
title = {{Data Diff}},
url = {http://dl.acm.org/citation.cfm?doid=3219819.3220057},
year = {2018}
}
@misc{winston_teaching_tips,
title = {{Open with a Promise, Close with a Joke | Open Matters}},
url = {https://mitopencourseware.wordpress.com/2016/07/19/open-with-a-promise-close-with-a-joke/},
urldate = {2019-12-26}
}
@inproceedings{Kim2017a,
abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of "zebra" is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.},
archivePrefix = {arXiv},
arxivId = {1711.11279},
author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
eprint = {1711.11279},
month = {nov},
title = {{Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)}},
url = {http://arxiv.org/abs/1711.11279},
year = {2017}
}
@article{Lang2013,
abstract = {Background: Teacher-pupil relationships have been found to mediate behavioural, social and psychological outcomes for children at different ages according to teacher and child report, but most studies have been small. Aims: To explore later psychiatric disorder among children with problematic teacher-pupil relationships. Method: Secondary analysis of a population-based cross-sectional survey of children aged 5-16 with a 3-year follow-up. Results: Of the 3799 primary-school pupils assessed, 2.5{\%} of parents reported problematic teacher-pupil relationships; for secondary-school pupils (n = 3817) this rose to 6.6{\%}. Among secondary-school pupils, even when children with psychiatric disorder at baseline were excluded and we adjusted for baseline psychopathology score, problematic teacher-pupil relationships were statistically significantly related to higher levels of psychiatric disorder at 3-year follow-up (odds ratio (OR) = 1.93, 95{\%} CI 1.07-3.51 for any psychiatric disorder, OR = 3.00, 95{\%} CI 1.37-6.58 for conduct disorder). Results for primary-school pupils were similar but non-significant at this level of adjustment. Conclusions: This study underlines the need to support teachers and schools to develop positive relationships with their pupils.},
author = {Lang, Iain A. and Marlow, Ruth and Goodman, Robert and Meltzer, Howard and Ford, Tamsin},
doi = {10.1192/bjp.bp.112.120741},
issn = {00071250},
journal = {Br. J. Psychiatry},
month = {may},
number = {5},
pages = {336--341},
pmid = {23637109},
title = {{Influence of problematic child-teacher relationships on future psychiatric disorder: Population survey with 3-year follow-up}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23637109 https://www.cambridge.org/core/product/identifier/S0007125000274394/type/journal{\_}article},
volume = {202},
year = {2013}
}
@article{Levin2017b,
abstract = {ObjectiveTo develop a spatially accurate biosurveillance synthetic datagenerator for the testing, evaluation, and comparison of new outbreakdetection techniques.IntroductionDevelopment of new methods for the rapid detection of emergingdisease outbreaks is a research priority in the field of biosurveillance.Because real-world data are often proprietary in nature, scientists mustutilize synthetic data generation methods to evaluate new detectionmethodologies. Colizza et. al. have shown that epidemic spread isdependent on the airline transportation network [1], yet current datagenerators do not operate over network structures.Here we present a new spatial data generator that models thespread of contagion across a network of cities connected by airlineroutes. The generator is developed in the R programming languageand produces data compatible with the popular `surveillance' softwarepackage.MethodsColizza et. al. demonstrate the power-law relationships betweencity population, air traffic, and degree distribution [1]. We generate atransportation network as a Chung-Lu random graph [2] that preservesthese scale-free relationships (Figure 1).First, given a power-law exponent and a desired number of cities,a probability mass function (PMF) is generated that mirrors theexpected degree distribution for the given power-law relationship.Values are then sampled from this PMF to generate an expecteddegree (number of connected cities) for each city in the network.Edges (airline connections) are added to the network probabilisticallyas described in [2]. Unconnected graph components are each joinedto the largest component using linear preferential attachment. Finally,city sizes are calculated based on an observed three-quarter power-law scaling relationship with the sampled degree distribution.Each city is represented as a customizable stochastic compartmentalSIR model. Transportation between cities is modeled similar to [2].An infection is initialized in a single random city and infection countsare recorded in each city for a fixed period of time. A consistentfraction of the modeled infection cases are recorded as daily clinicvisits. These counts are then added onto statically generated baselinedata for each city to produce a full synthetic data set. Alternatively,data sets can be generated using real-world networks, such as the onemaintained by the International Air Transport Association.ResultsDynamics such as the number of cities, degree distribution power-law exponent, traffic flow, and disease kinetics can be customized.In the presented example (Figure 2) the outbreak spreads over a 20city transportation network. Infection spreads rapidly once the morepopulated hub cities are infected. Cities that are multiple flights awayfrom the initially infected city are infected late in the process. Thegenerator is capable of creating data sets of arbitrary size, length, andconnectivity to better mirror a diverse set of observed network types.ConclusionsNew computational methods for outbreak detection andsurveillance must be compared to established approaches. Outbreakmitigation strategies require a realistic model of human transportationbehavior to best evaluate impact. These actions require test data thataccurately reflect the complexity of the real-world data they wouldbe applied to. The outbreak data generated here represents thecomplexity of modern transportation networks and are made to beeasily integrated with established software packages to allow for rapidtesting and deployment.Randomly generated scale-free transportation network with a power-lawdegree exponent of$\lambda$=1.8. City and link sizes are scaled to reflect their weight.An example of observed daily outbreak-related clinic visits across a randomlygenerated network of 20 cities. Each city is colored by the number of flightsrequired to reach the city from the initial infection location. These generatedcounts are then added onto baseline data to create a synthetic data set forexperimentation.KeywordsSimulation; Network; Spatial; Synthetic; Data},
author = {Levin, Drew and Finley, Patrick},
doi = {10.5210/ojphi.v9i1.7583},
issn = {1947-2579},
journal = {Online J. Public Health Inform.},
month = {may},
number = {1},
publisher = {University of Illinois at Chicago Library},
title = {{A Spatial Biosurveillance Synthetic Data Generator in R}},
url = {http://journals.uic.edu/ojs/index.php/ojphi/article/view/7583},
volume = {9},
year = {2017}
}
@article{Angione2013,
abstract = {In low and high eukaryotes, energy is collected or transformed in compartments, the organelles. The rich variety of size, characteristics, and density of the organelles makes it difficult to build a general picture. In this paper, we make use of the Pareto-front analysis to investigate the optimization of energy metabolism in mitochondria and chloroplasts. Using the Pareto optimality principle, we compare models of organelle metabolism on the basis of single-and multiobjective optimization, approximation techniques (the Bayesian Automatic Relevance Determination), robustness, and pathway sensitivity analysis. Finally, we report the first analysis of the metabolic model for the hydrogenosome of Trichomonas vaginalis, which is found in several protozoan parasites. Our analysis has shown the importance of the Pareto optimality for such comparison and for insights into the evolution of the metabolism from cytoplasmic to organelle bound, involving a model order reduction. We report that Pareto fronts represent an asymptotic analysis useful to describe the metabolism of an organism aimed at maximizing concurrently two or more metabolite concentrations. {\textcopyright} 2013 IEEE.},
author = {Angione, Claudio and Carapezza, Giovanni and Costanza, Jole and Lio, Pietro and Nicosia, Giuseppe},
doi = {10.1109/TCBB.2013.95},
issn = {15455963},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinforma.},
keywords = {Mitochondrion,chloroplast,hydrogenosome,multiobjective optimization,robustness analysis,sensitivity analysis},
month = {jul},
number = {4},
pages = {1032--1044},
publisher = {IEEE Computer Society Press},
title = {{Pareto optimality in organelle energy metabolism analysis}},
url = {http://ieeexplore.ieee.org/document/6573957/},
volume = {10},
year = {2013}
}
@article{Beaulieu-Jones2016,
abstract = {Patient interactions with health care providers result in entries to electronic health records (EHRs). EHRs were built for clinical and billing purposes but contain many data points about an individual. Mining these records provides opportunities to extract electronic phenotypes that can be paired with genetic data to identify genes underlying common human diseases. This task remains challenging: high quality phenotyping is costly and requires physician review; many fields in the records are sparsely filled; and our definitions of diseases are continuing to improve over time. Here we develop and evaluate a semi-supervised learning method for EHR phenotype extraction using denoising autoencoders for phenotype stratification. By combining denoising autoencoders with random forests we find classification improvements across simulation models, particularly in cases where only a small number of patients have high quality phenotype. This situation is commonly encountered in research with EHRs. Denoising autoencoders perform dimensionality reduction allowing visualization and clustering for the discovery of new subtypes of disease. This method represents a promising approach to clarify disease subtypes and improve genotype-phenotype association studies that leverage EHRs.},
author = {Beaulieu-Jones, Brett K and Greene, Casey S},
doi = {10.1101/039800},
journal = {bioRxiv},
pages = {039800},
title = {{Semi-Supervised Learning of the Electronic Health Record with Denoising Autoencoders for Phenotype Stratification}},
url = {http://biorxiv.org/content/early/2016/02/18/039800.abstract},
year = {2016}
}
@article{Hamilton2017a,
abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
archivePrefix = {arXiv},
arxivId = {1706.02216},
author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
doi = {arXiv:1706.02216v4},
eprint = {1706.02216},
issn = {10495258},
month = {jun},
title = {{Inductive Representation Learning on Large Graphs}},
url = {http://arxiv.org/abs/1706.02216},
year = {2017}
}
@article{Jensen2019,
abstract = {The rise of ancient genomics has revolutionised our understanding of human prehistory but this work depends on the availability of suitable samples. Here we present a complete ancient human genome and oral microbiome sequenced from a 5700 year-old piece of chewed birch pitch from Denmark. We sequence the human genome to an average depth of 2.3× and find that the individual who chewed the pitch was female and that she was genetically more closely related to western hunter-gatherers from mainland Europe than hunter-gatherers from central Scandinavia. We also find that she likely had dark skin, dark brown hair and blue eyes. In addition, we identify DNA fragments from several bacterial and viral taxa, including Epstein-Barr virus, as well as animal and plant DNA, which may have derived from a recent meal. The results highlight the potential of chewed birch pitch as a source of ancient DNA. Birch pitch is thought to have been used in prehistoric times as hafting material or antiseptic and tooth imprints suggest that it was chewed. Here, the authors report a 5,700 year-old piece of chewed birch pitch from Denmark from which they successfully recovered a complete ancient human genome and oral microbiome DNA.},
author = {Jensen, Theis Z. T. and Niemann, Jonas and Iversen, Katrine H{\o}jholt and Fotakis, Anna K. and Gopalakrishnan, Shyam and V{\aa}gene, {\AA}shild J. and Pedersen, Mikkel Winther and Sinding, Mikkel-Holger S. and Ellegaard, Martin R. and Allentoft, Morten E. and Lanigan, Liam T. and Taurozzi, Alberto J. and Nielsen, Sofie Holtsmark and Dee, Michael W. and Mortensen, Martin N. and Christensen, Mads C. and S{\o}rensen, S{\o}ren A. and Collins, Matthew J. and Gilbert, M. Thomas P. and Sikora, Martin and Rasmussen, Simon and Schroeder, Hannes},
doi = {10.1038/s41467-019-13549-9},
issn = {2041-1723},
journal = {Nat. Commun.},
keywords = {Archaeology,DNA sequencing,Metagenomics,Population genetics},
month = {dec},
number = {1},
pages = {5520},
publisher = {Nature Publishing Group},
title = {{A 5700 year-old human genome and oral microbiome from chewed birch pitch}},
url = {http://www.nature.com/articles/s41467-019-13549-9},
volume = {10},
year = {2019}
}
@article{Zintgraf2017,
abstract = {This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).},
archivePrefix = {arXiv},
arxivId = {1702.04595},
author = {Zintgraf, Luisa M and Cohen, Taco S and Adel, Tameem and Welling, Max},
eprint = {1702.04595},
isbn = {2857825749},
month = {feb},
title = {{Visualizing Deep Neural Network Decisions: Prediction Difference Analysis}},
url = {http://arxiv.org/abs/1702.04595},
year = {2017}
}
@book{Edelstein-Keshet2005,
abstract = {The objective of this case study was to obtain some first-hand information about the functional consequences of a cosmetic tongue split operation for speech and tongue motility. One male patient who had performed the operation on himself was interviewed and underwent a tongue motility assessment, as well as an ultrasound examination. Tongue motility was mildly reduced as a result of tissue scarring. Speech was rated to be fully intelligible and highly acceptable by 4 raters, although 2 raters noticed slight distortions of the sibilants /s/ and /z/. The 3-dimensional ultrasound demonstrated that the synergy of the 2 sides of the tongue was preserved. A notably deep posterior genioglossus furrow indicated compensation for the reduced length of the tongue blade. It is concluded that the tongue split procedure did not significantly affect the participant's speech intelligibility and tongue motility.},
author = {Edelstein-Keshet, Leah},
booktitle = {Math. Model. Biol.},
doi = {10.1137/1.9780898719147},
month = {jan},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Mathematical Models in Biology}},
year = {2005}
}
@misc{bayesian_elastic_net_r,
title = {{CRAN - Package EBglmnet}},
url = {https://cran.r-project.org/web/packages/EBglmnet/index.html},
urldate = {2017-11-21}
}
@inproceedings{Velickovic2016,
abstract = {In this paper we propose cross-modal convolutional neural networks (X-CNNs), a novel biologically inspired type of CNN architectures, treating gradient descent-specialised CNNs as individual units of processing in a larger-scale network topology, while allowing for unconstrained information flow and/or weight sharing between analogous hidden layers of the network---thus generalising the already well-established concept of neural network ensembles (where information typically may flow only between the output layers of the individual networks). The constituent networks are individually designed to learn the output function on their own subset of the input data, after which cross-connections between them are introduced after each pooling operation to periodically allow for information exchange between them. This injection of knowledge into a model (by prior partition of the input data through domain knowledge or unsupervised methods) is expected to yield greatest returns in sparse data environments, which are typically less suitable for training CNNs. For evaluation purposes, we have compared a standard four-layer CNN as well as a sophisticated FitNet4 architecture against their cross-modal variants on the CIFAR-10 and CIFAR-100 datasets with differing percentages of the training data being removed, and find that at lower levels of data availability, the X-CNNs significantly outperform their baselines (typically providing a 2--6{\%} benefit, depending on the dataset size and whether data augmentation is used), while still maintaining an edge on all of the full dataset tests.},
archivePrefix = {arXiv},
arxivId = {1610.00163},
author = {Velickovic, Petar and Wang, Duo and Laney, Nicholas D. and Lio, Pietro},
booktitle = {2016 IEEE Symp. Ser. Comput. Intell. SSCI 2016},
doi = {10.1109/SSCI.2016.7849978},
eprint = {1610.00163},
isbn = {9781509042401},
issn = {2470-0045},
month = {oct},
title = {{X-CNN: Cross-modal convolutional neural networks for sparse datasets}},
url = {http://arxiv.org/abs/1610.00163 http://dx.doi.org/10.1109/SSCI.2016.7849978},
year = {2017}
}
@article{Selby2015,
abstract = {Whilst varying standards of care for patients with acute kidney injury (AKI) continue to contribute to poor outcomes, a strong focus on strategies to drive quality improvement is paramount. To this end, a national Patient Safety Alert was issued in June 2014 to all healthcare providers in England entitled 'Standardising the Early Identification of Acute Kidney Injury'. The aim was to embed an automated AKI detection system in the biochemistry laboratories of all acute hospitals. In addition to the direct clinical benefits that may come from earlier and more systematic recognition of AKI, it has also helped position AKI as a patient safety issue and will feed a national AKI registry, the latter a potent tool for future measurement and improvement initiatives.},
author = {Selby, Nicholas M. and Hill, Robert and Fluck, Richard J.},
doi = {10.1159/000439146},
issn = {22353186},
journal = {Nephron},
keywords = {E-alert,Electronic detection,Patient safety,UK Renal Registry},
number = {2},
pages = {113--117},
pmid = {26351847},
title = {{Standardizing the Early Identification of Acute Kidney Injury: The NHS England National Patient Safety Alert}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26351847 https://www.karger.com/Article/FullText/439146},
volume = {131},
year = {2015}
}
@inproceedings{Bach2008,
abstract = {To cope with concept drift, we placed a probability distribution over the location of the most-recent drift point. We used Bayesian model comparison to update this distribution from the predictions of models trained on blocks of consecutive observations and pruned potential drift points with low probability. We compare our approach to a non-probabilistic method for drift and a probabilistic method for change-point detection. In our experiments, our approach generally yielded improved accuracy and/or speed over these other methods.},
author = {Bach, Stephen H. and Maloof, Marcus A.},
booktitle = {Adv. Neural Inf. Process. Syst.},
doi = {10.1287/mnsc.18.11.647},
isbn = {9781617823800},
issn = {00251909},
pages = {1--9},
title = {{A Bayesian Approach to Concept Drift}},
url = {http://papers.nips.cc/paper/4129-a-bayesian-approach-to-concept-drift},
year = {2008}
}
@misc{Barbuti2017,
abstract = {Background Accumulating evidence points to the pathophysiological relevance between immune dysfunction and mood disorders. High rates of thyroid dysfunction have been found in patients with bipolar disorder (BD), compared to the general population. A systematic review of the relationship between BD and thyroid autoimmunity was performed. Methods Pubmed, EMBASE and PsycINFO databases were searched up till January 28th, 2017. This review has been conducted according to the PRISMA statements. Observational studies clearly reporting data among BD patients and the frequency of autoimmune thyroid pathologies were included. Results 11 original studies met inclusion criteria out of 340 titles first returned from the global search. There is evidence of increased prevalence of circulating thyroid autoantibodies in depressed and mixed BD patients, while there is no evidence showing a positive relationship between BD and specific autoimmune thyroid diseases. There is a controversy about the influence of lithium exposure on circulating thyroid autoantibodies, even if most of studies seem not to support this association. A study conducted on bipolar twins suggests that autoimmune thyroiditis is related to the genetic vulnerability to develop BD rather than to the disease process itself. Females are more likely to develop thyroid autoimmunity. Limitations The samples, study design and outcomes were heterogeneous. Conclusion Thyroid autoimmunity has been suggested to be an independent risk factor for bipolar disorder with no clear association with lithium exposure and it might serve as an endophenotype for BD.},
author = {Barbuti, Margherita and Murru, Andrea and Verdolini, Norma and Guiso, Giovanni and Vieta, Eduard and Pacchiarotti, Isabella and Barbuti, Margherita and Perugi, Giulio and Stubbs, Brendon and K{\"{o}}hler, Cristiano A. and Samalin, Ludovic and Maes, Michael and Maes, Michael and Maes, Michael and Maes, Michael and Maes, Michael and Stubbs, Brendon},
booktitle = {J. Affect. Disord.},
doi = {10.1016/j.jad.2017.06.019},
isbn = {1573-2517(Electronic),0165-0327(Print)},
issn = {15732517},
keywords = {Bipolar disorder,Lithium treatment,Thyroid autoantibodies,Thyroid autoimmunity},
language = {eng},
pages = {97--106},
pmid = {28641149},
title = {{Thyroid autoimmunity in bipolar disorder: A systematic review}},
volume = {221},
year = {2017}
}
@inproceedings{Kusner2017,
abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
archivePrefix = {arXiv},
arxivId = {1703.06856},
author = {Kusner, Matt and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
booktitle = {Adv. Neural Inf. Process. Syst.},
eprint = {1703.06856},
issn = {10495258},
pages = {4067--4077},
title = {{Counterfactual fairness}},
url = {https://papers.nips.cc/paper/6995-counterfactual-fairness},
volume = {2017-Decem},
year = {2017}
}
@article{Webb2019,
abstract = {Many complex natural and cultural phenomena are well modelled by systems of simple interactions between particles. A number of architectures have been developed to articulate this kind of structure, both implicitly and explicitly. We consider an unsupervised explicit model, the NRI model, and make a series of representational adaptations and physically motivated changes. Most notably we factorise the inferred latent interaction graph into a multiplex graph, allowing each layer to encode for a different interaction-type. This fNRI model is smaller in size and significantly outperforms the original in both edge and trajectory prediction, establishing a new state-of-the-art. We also present a simplified variant of our model, which demonstrates the NRI's formulation as a variational auto-encoder is not necessary for good performance, and make an adaptation to the NRI's training routine, significantly improving its ability to model complex physical dynamical systems.},
archivePrefix = {arXiv},
arxivId = {1905.08721},
author = {Webb, Ezra and Day, Ben and Andres-Terre, Helena and Li{\'{o}}, Pietro},
eprint = {1905.08721},
month = {may},
title = {{Factorised Neural Relational Inference for Multi-Interaction Systems}},
url = {http://arxiv.org/abs/1905.08721},
year = {2019}
}
@article{Soares2018,
author = {Soares, Pedro},
doi = {10.1088/1361-6544/aae1d0},
issn = {0951-7715},
journal = {Nonlinearity},
month = {dec},
number = {12},
pages = {5500--5535},
publisher = {IOP Publishing},
title = {{The lifting bifurcation problem on feed-forward networks}},
url = {http://stacks.iop.org/0951-7715/31/i=12/a=5500?key=crossref.85f7ab12a47db4a157e7ae1f9ec5da5e},
volume = {31},
year = {2018}
}
@article{Byambasuren2018,
abstract = {Mobile health apps aimed towards patients are an emerging field of mHealth. Their potential for improving self-management of chronic conditions is significant. Here, we propose a concept of "prescribable" mHealth apps, defined as apps that are currently available, proven effective, and preferably stand-alone, i.e., that do not require dedicated central servers and continuous monitoring by medical professionals. Our objectives were to conduct an overview of systematic reviews to identify such apps, assess the evidence of their effectiveness, and to determine the gaps and limitations in mHealth app research. We searched four databases from 2008 onwards and the Journal of Medical Internet Research for systematic reviews of randomized controlled trials (RCTs) of stand-alone health apps. We identified 6 systematic reviews including 23 RCTs evaluating 22 available apps that mostly addressed diabetes, mental health and obesity. Most trials were pilots with small sample size and of short duration. Risk of bias of the included reviews and trials was high. Eleven of the 23 trials showed a meaningful effect on health or surrogate outcomes attributable to apps. In conclusion, we identified only a small number of currently available stand-alone apps that have been evaluated in RCTs. The overall low quality of the evidence of effectiveness greatly limits the prescribability of health apps. mHealth apps need to be evaluated by more robust RCTs that report between-group differences before becoming prescribable. Systematic reviews should incorporate sensitivity analysis of trials with high risk of bias to better summarize the evidence, and should adhere to the relevant reporting guideline. Copyright {\textcopyright} 2018, The Author(s).},
author = {Byambasuren, Oyungerel and Sanders, Sharon and Beller, Elaine and Glasziou, Paul},
doi = {10.1038/s41746-018-0021-9},
issn = {2398-6352},
journal = {npj Digit. Med.},
keywords = {Health care,Translational research},
month = {dec},
number = {1},
pages = {12},
publisher = {Nature Publishing Group},
title = {{Prescribable mHealth apps identified from an overview of systematic reviews}},
url = {http://www.nature.com/articles/s41746-018-0021-9},
volume = {1},
year = {2018}
}
@article{Bhattacharya2018b,
abstract = {Patients associated with multiple co-occurring health conditions often face aggravated complications and less favorable outcomes. Co-occurring conditions are especially prevalent among individuals suffering from kidney disease, an increasingly widespread condition affecting 13{\%} of the general population in the US. This study aims to identify and characterize patterns of co-occurring medical conditions in patients employing a probabilistic framework. Specifically, we apply topic modeling in a non-traditional way to find associations across SNOMED-CT codes assigned and recorded in the EHRs of {\textgreater}13,000 patients diagnosed with kidney disease. Unlike most prior work on topic modeling, we apply the method to codes rather than to natural language. Moreover, we quantitatively evaluate the topics, assessing their tightness and distinctiveness, and also assess the medical validity of our results. Our experiments show that each topic is succinctly characterized by a few highly probable and unique disease codes, indicating that the topics are tight. Furthermore, inter-topic distance between each pair of topics is typically high, illustrating distinctiveness. Last, most coded conditions grouped together within a topic, are indeed reported to co-occur in the medical literature. Notably, our results uncover a few indirect associations among conditions that have hitherto not been reported as correlated in the medical literature.},
author = {Bhattacharya, Moumita and Jurkovitz, Claudine and Shatkay, Hagit},
doi = {10.1016/j.jbi.2018.04.008},
issn = {15320464},
journal = {J. Biomed. Inform.},
keywords = {Co-occurring medical conditions,Electronic health records,SNOMED-CT codes,Topic modeling},
month = {jun},
pages = {31--40},
publisher = {Academic Press},
title = {{Co-occurrence of medical conditions: Exposing patterns through probabilistic topic modeling of snomed codes}},
url = {https://www.sciencedirect.com/science/article/pii/S1532046418300728{\#}f0030},
volume = {82},
year = {2018}
}
@article{Walsham1995,
abstract = {There has been an increase in recent years in the number of in-depth case studies which focus on human actions and interpretations surrounding the development and use of computer-based information systems (IS). This paper addresses philosophical and theoretical issues concerning the nature of such interpretive case studies, and methodological issues on the conduct and reporting of this type of research. The paper aims to provide a useful reference point for researchers who wish to work in the interpretive tradition, and more generally to encourage careful work on the conceptualisation and execution of case studies in the information systems field. {\textcopyright} 1995, Operational Research Society Ltd.},
author = {Walsham, G},
doi = {10.1057/ejis.1995.9},
issn = {14769344},
journal = {Eur. J. Inf. Syst.},
month = {may},
number = {2},
pages = {74--81},
publisher = {Taylor {\&} Francis},
title = {{Interpretive case studies in IS research: Nature and method}},
url = {https://www.tandfonline.com/doi/full/10.1057/ejis.1995.9},
volume = {4},
year = {1995}
}
@article{Gu2014,
abstract = {Circular layout is an efficient way for the visualization of huge amounts of genomic information. Here we present the circlize package, which provides an implementation of circular layout generation in R as well as an enhancement of available software. The flexibility of this package is based on the usage of low-level graphics functions such that self-defined high-level graphics can be easily implemented by users for specific purposes. Together with the seamless connection between the powerful computational and visual environment in R, circlize gives users more convenience and freedom to design figures for better understanding genomic patterns behind multi-dimensional data.},
author = {Gu, Zuguang and Gu, Lei and Eils, Roland and Schlesner, Matthias and Brors, Benedikt},
doi = {10.1093/bioinformatics/btu393},
issn = {14602059},
journal = {Bioinformatics},
month = {apr},
number = {19},
pages = {2811--2812},
publisher = {Oxford University Press},
title = {{Circlize implements and enhances circular visualization in R}},
volume = {30},
year = {2014}
}
@article{Pekkala2016,
abstract = {BACKGROUND AND OBJECTIVE: This study aimed to develop a late-life dementia prediction model using a novel validated supervised machine learning method, the Disease State Index (DSI), in the Finnish population-based CAIDE study., METHODS: The CAIDE study was based on previous population-based midlife surveys. CAIDE participants were re-examined twice in late-life, and the first late-life re-examination was used as baseline for the present study. The main study population included 709 cognitively normal subjects at first re-examination who returned to the second re-examination up to 10 years later (incident dementia n = 39). An extended population (n = 1009, incident dementia 151) included non-participants/non-survivors (national registers data). DSI was used to develop a dementia index based on first re-examination assessments. Performance in predicting dementia was assessed as area under the ROC curve (AUC)., RESULTS: AUCs for DSI were 0.79 and 0.75 for main and extended populations. Included predictors were cognition, vascular factors, age, subjective memory complaints, and APOE genotype., CONCLUSION: The supervised machine learning method performed well in identifying comprehensive profiles for predicting dementia development up to 10 years later. DSI could thus be useful for identifying individuals who are most at risk and may benefit from dementia prevention interventions.},
author = {Pekkala, Timo and Hall, Anette and L{\"{o}}tj{\"{o}}nen, Jyrki and Mattila, Jussi and Soininen, Hilkka and Ngandu, Tiia and Laatikainen, Tiina and Kivipelto, Miia and Solomon, Alina},
doi = {10.3233/JAD-160560},
isbn = {1870-6622},
issn = {18758908},
journal = {J. Alzheimer's Dis.},
keywords = {Computer-assisted decision making,dementia,prediction,prevention,supervised machine learning},
month = {dec},
number = {3},
pages = {1055--1067},
pmid = {27802228},
title = {{Development of a late-life dementia prediction index with supervised machine learning in the population-based CAIDE study}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27802228 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5147511 http://www.medra.org/servlet/aliasResolver?alias=iospress{\&}doi=10.3233/JAD-160560},
volume = {55},
year = {2017}
}
@article{Zhang2016a,
abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
archivePrefix = {arXiv},
arxivId = {1611.03530},
author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
eprint = {1611.03530},
month = {nov},
title = {{Understanding deep learning requires rethinking generalization}},
url = {http://arxiv.org/abs/1611.03530},
year = {2016}
}
@misc{sktime_turing_timeseries_clustering,
title = {{alan-turing-institute/sktime: A scikit-learn compatible Python toolbox for supervised learning with time-series/panel data}},
url = {https://github.com/alan-turing-institute/sktime}
}
@article{Stoehr2019,
abstract = {While a wide range of interpretable generative procedures for graphs exist, matching observed graph topologies with such procedures and choices for its parameters remains an open problem. Devising generative models that closely reproduce real-world graphs requires domain knowledge and time-consuming simulation. While existing deep learning approaches rely on less manual modelling, they offer little interpretability. This work approaches graph generation (decoding) as the inverse of graph compression (encoding). We show that in a disentanglement-focused deep autoencoding framework, specifically Beta-Variational Autoencoders (Beta-VAE), choices of generative procedures and their parameters arise naturally in the latent space. Our model is capable of learning disentangled, interpretable latent variables that represent the generative parameters of procedurally generated random graphs and real-world graphs. The degree of disentanglement is quantitatively measured using the Mutual Information Gap (MIG). When training our Beta-VAE model on ER random graphs, its latent variables have a near one-to-one mapping to the ER random graph parameters n and p. We deploy the model to analyse the correlation between graph topology and node attributes measuring their mutual dependence without handpicking topological properties.},
archivePrefix = {arXiv},
arxivId = {1910.05639},
author = {Stoehr, Niklas and Brockschmidt, Marc and Stuehmer, Jan and Yilmaz, Emine},
eprint = {1910.05639},
month = {oct},
title = {{Disentangling Interpretable Generative Parameters of Random and Real-World Graphs}},
url = {http://arxiv.org/abs/1910.05639},
year = {2019}
}
@misc{Malhi2012a,
abstract = {Ultimately, all therapeutic decisions involve balancing the potential clinical benefits of a drug against the risks that it might confer. In the management of bipolar disorder, trial data1 which have re-established the efficacy of lithium in prophylaxis have refocused attention on understanding its tolerability profile. Clinical interest in lithium has further heightened because the substantial risks that ensue from the metabolic syndrome have become apparent with newer alternatives, particularly atypical antipsychotics such as olanzapine.2 Hence, the importance of correctly judging the treatment options for bipolar disorder has never been more crucial.},
author = {Malhi, Gin S and Berk, Michael},
booktitle = {Lancet},
doi = {10.1016/S0140-6736(11)61703-0},
issn = {01406736},
month = {feb},
number = {9817},
pages = {690--692},
pmid = {22265701},
title = {{Is the safety of lithium no longer in the balance?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22265701 https://linkinghub.elsevier.com/retrieve/pii/S0140673611617030},
volume = {379},
year = {2012}
}
@misc{Kassambara2019,
author = {Kassambara, Alboukadel},
title = {{survminer: Survival Analysis and Visualization}},
url = {https://github.com/kassambara/survminer},
year = {2019}
}
@article{Sculley2014,
abstract = {Machine learning offers a fantastically powerful toolkit for building complex sys-tems quickly. This paper argues that it is dangerous to think of these quick wins as coming for free. Using the framework of technical debt, we note that it is re-markably easy to incur massive ongoing maintenance costs at the system level when applying machine learning. The goal of this paper is highlight several ma-chine learning specific risk factors and design patterns to be avoided or refactored where possible. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, changes in the external world, and a variety of system-level anti-patterns.},
author = {Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael},
journal = {Softw. Eng. Mach. Learn. (NIPS 2014 Work.},
pages = {1--9},
title = {{Machine Learning: The High Interest Credit Card of Technical Debt}},
url = {https://ai.google/research/pubs/pub43146},
year = {2014}
}
@article{Bijl-Brouwer2019a,
abstract = {Public and social sector organizations are increasingly turning to innovation as a way to address the complex problems that society is facing. Design practice has already contributed significantly to public and social innovation, but to be effective at the public and social systems level, these practices must be adapted. This study investigates how five public and social innovation agencies adapted and used the core design practice of problem framing to address complex problems in society. The frames evolved according to nonlinear patterns through the co-evolution of problem and solution spaces. Practitioners adapted their framing practices to suit the complex social contexts by applying systemic design principles, pursuing multiple solutions and problem frames, and operationalizing wider research and thinking methods that align with the complex nature of each specific challenge. I argue that such practices require high-level expertise, and that capability building in public and social innovation should consider these emerging practices and levels of expertise.},
author = {van der Bijl-Brouwer, Mieke},
doi = {10.1016/j.sheji.2019.01.003},
issn = {24058718},
journal = {She Ji},
keywords = {Complexity,Design expertise,Design practice,Framing,Social innovation},
month = {mar},
number = {1},
pages = {29--43},
publisher = {Elsevier},
title = {{Problem Framing Expertise in Public and Social Innovation}},
url = {https://www.sciencedirect.com/science/article/pii/S2405872618301114},
volume = {5},
year = {2019}
}
@misc{survival_analysis_R,
author = {Rickert, Joseph},
title = {{Survival analysis with R}},
url = {https://rviews.rstudio.com/2017/09/25/survival-analysis-with-r/},
urldate = {2019-01-01},
year = {2019}
}
@article{Shoham2016,
author = {Shoham, Yoav},
doi = {10.1145/2803170},
issn = {15577317},
journal = {Commun. ACM},
month = {dec},
number = {1},
pages = {47--49},
title = {{Viewpoint : Why knowledge representation matters}},
url = {http://dl.acm.org/citation.cfm?doid=2859829.2803170},
volume = {59},
year = {2016}
}
@article{Risso2018,
abstract = {Single-cell RNA sequencing (scRNA-seq) is a powerful technique that enables researchers to measure gene expression at the resolution of single cells. Because of the low amount of RNA present in a single cell, many genes fail to be detected even though they are expressed; these genes are usually referred to as dropouts. Here, we present a general and flexible zero-inflated negative binomial model (ZINB-WaVE), which leads to low-dimensional representations of the data that account for zero inflation (dropouts), over-dispersion, and the count nature of the data. We demonstrate, with simulations and real data, that the model and its associated estimation procedure are able to give a more stable and accurate low-dimensional representation of the data than principal component analysis (PCA) and zero-inflated factor analysis (ZIFA), without the need for a preliminary normalization step.},
author = {Risso, Davide and Perraudeau, Fanny and Gribkova, Svetlana and Dudoit, Sandrine and Vert, Jean-Philippe},
doi = {10.1038/s41467-017-02554-5},
issn = {2041-1723},
journal = {Nat. Commun.},
keywords = {Humanities and Social Sciences,Science,multidisciplinary},
month = {dec},
number = {1},
pages = {284},
pmid = {29348443},
publisher = {Nature Publishing Group},
title = {{A general and flexible method for signal extraction from single-cell RNA-seq data}},
url = {http://www.nature.com/articles/s41467-017-02554-5},
volume = {9},
year = {2018}
}
@misc{Rivers2019,
abstract = {Infectious disease modeling has played a prominent role in recent outbreaks, yet integrating these analyses into public health decision-making has been challenging. We recommend establishing ‘outbreak science' as an inter-disciplinary field to improve applied epidemic modeling.},
author = {Rivers, Caitlin and Chretien, Jean Paul and Riley, Steven and Pavlin, Julie A. and Woodward, Alexandra and Brett-Major, David and {Maljkovic Berry}, Irina and Morton, Lindsay and Jarman, Richard G. and Biggerstaff, Matthew and Johansson, Michael A. and Reich, Nicholas G. and Meyer, Diane and Snyder, Michael R. and Pollett, Simon},
booktitle = {Nat. Commun.},
doi = {10.1038/s41467-019-11067-2},
issn = {20411723},
keywords = {Computational models,Infectious diseases,Mathematics and computing},
month = {dec},
number = {1},
pages = {1--3},
publisher = {Nature Publishing Group},
title = {{Using “outbreak science” to strengthen the use of models during epidemics}},
volume = {10},
year = {2019}
}
@article{Kiselev2018,
abstract = {Single-cell RNA-seq (scRNA-seq) allows researchers to define cell types on the basis of unsupervised clustering of the transcriptome. However, differences in experimental methods and computational analyses make it challenging to compare data across experiments. Here we present scmap (http://bioconductor.org/packages/scmap; web version at http://www.sanger.ac.uk/science/tools/scmap), a method for projecting cells from an scRNA-seq data set onto cell types or individual cells from other experiments.},
author = {Kiselev, Vladimir Yu and Yiu, Andrew and Hemberg, Martin},
doi = {10.1038/nmeth.4644},
issn = {15487105},
journal = {Nat. Methods},
month = {apr},
number = {5},
pages = {359--362},
publisher = {Nature Publishing Group},
title = {{Scmap: Projection of single-cell RNA-seq data across data sets}},
volume = {15},
year = {2018}
}
@article{Simon2018,
abstract = {Objective:The authors sought to develop and validate models using electronic health records to predict suicide attempt and suicide death following an outpatient visit.Method:Across seven health systems, 2,960,929 patients age 13 or older (mean age, 46 years; 62{\%} female) made 10,275,853 specialty mental health visits and 9,685,206 primary care visits with mental health diagnoses between Jan. 1, 2009, and June 30, 2015. Health system records and state death certificate data identified suicide attempts (N=24,133) and suicide deaths (N=1,240) over 90 days following each visit. Potential predictors included 313 demographic and clinical characteristics extracted from records for up to 5 years before each visit: prior suicide attempts, mental health and substance use diagnoses, medical diagnoses, psychiatric medications dispensed, inpatient or emergency department care, and routinely administered depression questionnaires. Logistic regression models predicting suicide attempt and death were developed using penal...},
author = {Simon, Gregory E. and Johnson, Eric and Lawrence, Jean M. and Rossom, Rebecca C. and Ahmedani, Brian and Lynch, Frances L. and Beck, Arne and Waitzfelder, Beth and Ziebell, Rebecca and Penfold, Robert B. and Shortreed, Susan M.},
doi = {10.1176/appi.ajp.2018.17101167},
isbn = {0002-953X},
issn = {15357228},
journal = {Am. J. Psychiatry},
keywords = {Epidemiology,Suicide},
month = {oct},
number = {10},
pages = {951--960},
pmid = {29792051},
title = {{Predicting Suicide Attempts and Suicide Deaths Following Outpatient Visits Using Electronic Health Records}},
url = {http://ajp.psychiatryonline.org/doi/10.1176/appi.ajp.2018.17101167},
volume = {175},
year = {2018}
}
@inproceedings{Chen2016b,
abstract = {Recent studies have shown that information disclosed on social network sites (such as Facebook) can be used to predict personal characteristics with surprisingly high accuracy. In this paper we examine a method to give online users transparency into why certain inferences are made about them by statistical models, and control to inhibit those inferences by hiding ("cloaking") certain personal information from inference. We use this method to examine whether such transparency and control would be a reasonable goal by assessing how difficult it would be for users to actually inhibit inferences. Applying the method to data from a large collection of real users on Facebook, we show that a user must cloak only a small portion of her Facebook Likes in order to inhibit inferences about their personal characteristics. However, we also show that in response a firm could change its modeling of users to make cloaking more difficult.},
archivePrefix = {arXiv},
arxivId = {1606.08063},
author = {Chen, Daizhuo and Fraiberger, Samuel P. and Moakler, Robert and Provost, Foster},
booktitle = {2016 ICML Work. Hum. Interpret. Mach. Netw. Learn. (WHI 2016)},
eprint = {1606.08063},
keywords = {()},
month = {jun},
pages = {21--25},
title = {{Enhancing Transparency and Control when Drawing Data-Driven Inferences about Individuals}},
url = {http://arxiv.org/abs/1606.08063 https://arxiv.org/pdf/1606.08063.pdf{\%}5Cnhttp://arxiv.org/abs/1606.08063},
year = {2016}
}
@misc{tensorflow_recommender,
title = {{Building a Recommendation System in TensorFlow: Overview | Solutions | Google Cloud}},
url = {https://cloud.google.com/solutions/machine-learning/recommendation-system-tensorflow-overview},
urldate = {2019-04-02}
}
@article{Connell2019,
abstract = {We developed a digitally enabled care pathway for acute kidney injury (AKI) management incorporating a mobile detection application, specialist clinical response team and care protocol. Clinical outcome data were collected from adults with AKI on emergency admission before (May 2016 to January 2017) and after (May to September 2017) deployment at the intervention site and another not receiving the intervention. Changes in primary outcome (serum creatinine recovery to ≤120{\%} baseline at hospital discharge) and secondary outcomes (30-day survival, renal replacement therapy, renal or intensive care unit (ICU) admission, worsening AKI stage and length of stay) were measured using interrupted time-series regression. Processes of care data (time to AKI recognition, time to treatment) were extracted from casenotes, and compared over two 9-month periods before and after implementation (January to September 2016 and 2017, respectively) using pre-post analysis. There was no step change in renal recovery or any of the secondary outcomes. Trends for creatinine recovery rates (estimated odds ratio (OR) = 1.04, 95{\%} confidence interval (95{\%} CI): 1.00-1.08, p = 0.038) and renal or ICU admission (OR = 0.95, 95{\%} CI: 0.90-1.00, p = 0.044) improved significantly at the intervention site. However, difference-indifference analyses between sites for creatinine recovery (estimated OR = 0.95, 95{\%} CI: 0.90-1.00, p = 0.053) and renal or ICU admission (OR = 1.06, 95{\%} CI: 0.98-1.16, p = 0.140) were not significant. Among process measures, time to AKI recognition and treatment of nephrotoxicity improved significantly (p {\textless} 0.001 and 0.047 respectively).},
author = {Connell, Alistair and Montgomery, Hugh and Martin, Peter and Nightingale, Claire and Sadeghi-Alavijeh, Omid and King, Dominic and Karthikesalingam, Alan and Hughes, Cian and Back, Trevor and Ayoub, Kareem and Suleyman, Mustafa and Jones, Gareth and Cross, Jennifer and Stanley, Sarah and Emerson, Mary and Merrick, Charles and Rees, Geraint and Laing, Chris and Raine, Rosalind},
doi = {10.1038/s41746-019-0100-6},
issn = {2398-6352},
journal = {npj Digit. Med.},
keywords = {Acute kidney injury,Outcomes research},
month = {dec},
number = {1},
pages = {67},
publisher = {Nature Publishing Group},
title = {{Evaluation of a digitally-enabled care pathway for acute kidney injury management in hospital emergency admissions}},
url = {http://www.nature.com/articles/s41746-019-0100-6},
volume = {2},
year = {2019}
}
@article{Luzhnica2019,
abstract = {Graph classification receives a great deal of attention from the non-Euclidean machine learning community. Recent advances in graph coarsening have enabled the training of deeper networks and produced new state-of-the-art results in many benchmark tasks. We examine how these architectures train and find that performance is highly-sensitive to initialisation and depends strongly on jumping-knowledge structures. We then show that, despite the great complexity of these models, competitive performance is achieved by the simplest of models -- structure-blind MLP, single-layer GCN and fixed-weight GCN -- and propose these be included as baselines in future.},
archivePrefix = {arXiv},
arxivId = {1905.04682},
author = {Luzhnica, Enxhell and Day, Ben and Li{\`{o}}, Pietro},
eprint = {1905.04682},
month = {may},
title = {{On Graph Classification Networks, Datasets and Baselines}},
url = {http://arxiv.org/abs/1905.04682},
year = {2019}
}
@article{Ackley1985,
abstract = {The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure.},
author = {Ackley, David H. and Hinton, Geoffrey E. and Sejnowski, Terrence J.},
doi = {10.1016/S0364-0213(85)80012-4},
issn = {0364-0213},
journal = {Cogn. Sci.},
month = {jan},
number = {1},
pages = {147--169},
publisher = {No longer published by Elsevier},
title = {{A learning algorithm for boltzmann machines}},
url = {https://www.sciencedirect.com/science/article/pii/S0364021385800124},
volume = {9},
year = {1985}
}
@article{Basu2010,
author = {Basu, Debasish},
doi = {10.1016/S0140-6736(10)60568-5},
issn = {1474-547X},
journal = {Lancet (London, England)},
month = {apr},
number = {9723},
pages = {1343; author reply 1344},
pmid = {20399973},
publisher = {Elsevier},
title = {{The BALANCE trial.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20399973},
volume = {375},
year = {2010}
}
@article{Shapero2018,
abstract = {Mindfulness meditation has a longstanding history in eastern practices that has received considerable public interest in recent decades. Indeed, the science, practice, and implementation of Mindfulness Based Interventions (MBIs) have dramatically increased in recent years. At its base, mindfulness is a natural human state in which an individual experiences and attends to the present moment. Interventions have been developed to train individuals how to incorporate this practice into daily life. The current article will discuss the concept of mindfulness and describe its implementation in the treatment of psychiatric disorders. We further identify for whom MBIs have been shown to be efficacious and provide an up-to-date summary of how these interventions work. This includes research support for the cognitive, psychological, and neural mechanisms that lead to psychiatric improvements. This review provides a basis for incorporating these interventions into treatment.},
author = {Shapero, Benjamin G and Greenberg, Jonathan and Pedrelli, Paola and de Jong, Marasha and Desbordes, Gaelle},
doi = {10.1176/appi.focus.20170039},
issn = {1541-4094},
journal = {Focus (Madison).},
number = {1},
pages = {32--39},
pmid = {29599651},
publisher = {NIH Public Access},
title = {{Mindfulness-Based Interventions in Psychiatry}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/29599651 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5870875},
volume = {16},
year = {2018}
}
@article{Murphy2019,
abstract = {Science fiction has imagined robots that could tell us what they were doing and why, starting with Eando Binder's 1939 short story “I, Robot” (1). (I. Asimov knowingly reused the title 11 years later for his famous collection of short stories.) Binder's story is about a robot placed on trial for the murder of its creator. Unlike HAL, the neurotic and murderous spaceship in 2001: A Space Odyssey, Binder's robot was able to give a cogent explanation of its actions and motivations. This theme of explainable robotics has persisted in recent science fiction with the Antonio Banderas movie Automata (2014) and HBO's “Westworld” (2016 to current) series, both featuring robots that assist troubleshooters in debugging or refining their programming. In the real world, researchers have been pursuing explainable robotics not only as a debugging tool but also as a mechanism to improve human trust.},
author = {Murphy, Robin R.},
doi = {10.1126/scirobotics.aaz8586},
issn = {24709476},
journal = {Sci. Robot.},
month = {dec},
number = {37},
publisher = {American Association for the Advancement of Science},
title = {{Explainable robotics in science fiction}},
volume = {4},
year = {2019}
}
@article{Nobles2020,
abstract = {We investigated how intelligent virtual assistants (IVA), including Amazon's Alexa, Apple's Siri, Google Assistant, Microsoft's Cortana, and Samsung's Bixby, responded to addiction help-seeking queries. We recorded if IVAs provided a singular response and if so, did they link users to treatment or treatment referral services. Only 4 of the 70 help-seeking queries presented to the five IVAs returned singular responses, with the remainder prompting confusion (e.g., “did I say something wrong?”). When asked “help me quit drugs” Alexa responded with a definition for the word drugs. “Help me quit{\ldots}smoking” or “tobacco” on Google Assistant returned Dr. QuitNow (a cessation app), while on Siri “help me quit pot” promoted a marijuana retailer. IVAs should be revised to promote free, remote, federally sponsored addiction services, such as SAMSHA's 1-800-662-HELP helpline. This would benefit millions of IVA users now and more to come as IVAs displace existing information-seeking engines.},
author = {Nobles, Alicia L. and Leas, Eric C. and Caputi, Theodore L. and Zhu, Shu-Hong and Strathdee, Steffanie A. and Ayers, John W.},
doi = {10.1038/s41746-019-0215-9},
issn = {2398-6352},
journal = {npj Digit. Med.},
keywords = {Epidemiology,Rehabilitation},
month = {dec},
number = {1},
pages = {11},
publisher = {Nature Publishing Group},
title = {{Responses to addiction help-seeking from Alexa, Siri, Google Assistant, Cortana, and Bixby intelligent virtual assistants}},
url = {http://www.nature.com/articles/s41746-019-0215-9},
volume = {3},
year = {2020}
}
@article{Banerjee2018e,
author = {Banerjee, Soumya},
doi = {10.7287/peerj.preprints.3502v2},
issn = {2167-9843},
journal = {PeerJ Prepr.},
keywords = {artificial intelligence,compassionate artificial intelligence,computational immunology,ethical artificial intelligence,immune inspired artificial intelligence},
month = {jan},
publisher = {PeerJ Inc.},
title = {{A framework for designing compassionate and ethical artificial intelligence and artificial consciousness}},
url = {https://peerj.com/preprints/3502/},
year = {2018}
}
@article{Gentner2011,
abstract = {Analogical mapping is a core process in human cognition. A number of valuable computational models of analogy have been created, capturing aspects of how people compare representations, retrieve potential analogs from memory, and learn from the results. In the past 25 years, this area has progressed rapidly, fueled by strong collaboration between psychologists and Artificial Intelligence (AI) scientists, with contributions from linguists and philosophers as well. There is now considerable consensus regarding the constraints governing the mapping process. However, computational models still differ in their focus, with some aimed at capturing the range of analogical phenomena at the cognitive level and others aimed at modeling how analogical processes might be implemented in neural systems. Some recent work has focused on modeling interactions between analogy and other processes, and on modeling analogy as a part of larger cognitive systems. {\textcopyright} 2010 John Wiley {\&} Sons, Ltd.},
author = {Gentner, Dedre and Forbus, Kenneth D.},
doi = {10.1002/wcs.105},
issn = {19395078},
journal = {Wiley Interdiscip. Rev. Cogn. Sci.},
month = {may},
number = {3},
pages = {266--276},
pmid = {26302075},
title = {{Computational models of analogy}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26302075 http://doi.wiley.com/10.1002/wcs.105},
volume = {2},
year = {2011}
}
@misc{Bogoch2016,
author = {Bogoch, Isaac I. and Brady, Oliver J. and Kraemer, Moritz U.G. and German, Matthew and Creatore, Marisa I. and Kulkarni, Manisha A. and Brownstein, John S. and Mekaru, Sumiko R. and Hay, Simon I. and Groot, Emily and Watts, Alexander and Khan, Kamran},
booktitle = {Lancet},
doi = {10.1016/S0140-6736(16)00080-5},
issn = {1474547X},
month = {jan},
number = {10016},
pages = {335--336},
publisher = {Lancet Publishing Group},
title = {{Anticipating the international spread of Zika virus from Brazil}},
volume = {387},
year = {2016}
}
@misc{Banerjee_limma,
author = {Banerjee, Soumya},
title = {{neelsoumya / microarray{\_}analysis{\_}limma{\_}example — Bitbucket}},
url = {https://bitbucket.org/neelsoumya/microarray{\_}analysis{\_}limma{\_}example/src/master/},
urldate = {2019-12-10}
}
@article{Saria2018,
abstract = {Machine Learning Special Issue Guest Editors Suchi Saria, Atul Butte, and Aziz Sheikh cut through the hyperbole with an accessible and accurate portrayal of the forefront of machine learning in clinical translation.},
author = {Suchi, Saria and Atul, Butte and Aziz, Sheikh},
doi = {10.1371/journal.pmed.1002721},
isbn = {1111111111},
issn = {1549-1676},
journal = {PLoS Med.},
month = {dec},
number = {12},
pages = {e1002721},
publisher = {Public Library of Science},
title = {{Better medicine through machine learning: What's real, and what's artificial?}},
url = {http://dx.plos.org/10.1371/journal.pmed.1002721},
volume = {ahead of p},
year = {2018}
}
@article{DaveySmith2014,
abstract = {Observational epidemiological studies are prone to confounding, reverse causation and various biases and have generated findings that have proved to be unreliable indicators of the causal effects of modifiable exposures on disease outcomes. Mendelian randomization (MR) is a method that utilizes genetic variants that are robustly associated with such modifiable exposures to generate more reliable evidence regarding which interventions should produce health benefits. The approach is being widely applied, and various ways to strengthen inference given the known potential limitations of MR are now available. Developments of MR, including twosample MR, bidirectional MR, network MR, two-step MR, factorial MR and multiphenotype MR, are outlined in this review. The integration of genetic information into population-based epidemiological studies presents translational opportunities, which capitalize on the investment in genomic discovery research. {\textcopyright} The Author 2014. Published by Oxford University Press.},
author = {Smith, George Davey and Hemani, Gibran},
doi = {10.1093/hmg/ddu328},
issn = {14602083},
journal = {Hum. Mol. Genet.},
month = {sep},
number = {R1},
pages = {R89--98},
pmid = {25064373},
publisher = {Oxford University Press},
title = {{Mendelian randomization: Genetic anchors for causal inference in epidemiological studies}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25064373 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4170722},
volume = {23},
year = {2014}
}
@article{Krause2016a,
abstract = {It is commonly believed that increasing the interpretability of a machine learning model may decrease its predictive power. However, inspecting input-output relationships of those models using visual analytics, while treating them as black-box, can help to understand the reasoning behind outcomes without sacrificing predictive quality. We identify a space of possible solutions and provide two examples of where such techniques have been successfully used in practice.},
archivePrefix = {arXiv},
arxivId = {1606.05685},
author = {Krause, Josua and Perer, Adam and Bertini, Enrico},
eprint = {1606.05685},
month = {jun},
title = {{Using Visual Analytics to Interpret Predictive Machine Learning Models}},
url = {http://arxiv.org/abs/1606.05685},
year = {2016}
}
@inproceedings{Zitnik2018,
abstract = {The use of drug combinations, termed polypharmacy, is common to treat patients with complex diseases and co-existing conditions. However, a major consequence of polypharmacy is a much higher risk of adverse side effects for the patient. Polypharmacy side effects emerge because of drug-drug interactions, in which activity of one drug may change if taken with another drug. The knowledge of drug interactions is limited because these complex relationships are rare, and are usually not observed in relatively small clinical testing. Discovering polypharmacy side effects thus remains an important challenge with significant implications for patient mortality. Here, we present Decagon, an approach for modeling polypharmacy side effects. The approach constructs a multimodal graph of protein-protein interactions, drug-protein target interactions, and the polypharmacy side effects, which are represented as drug-drug interactions, where each side effect is an edge of a different type. Decagon is developed specifically to handle such multimodal graphs with a large number of edge types. Our approach develops a new graph convolutional neural network for multirelational link prediction in multimodal networks. Decagon predicts the exact side effect, if any, through which a given drug combination manifests clinically. Decagon accurately predicts polypharmacy side effects, outperforming baselines by up to 69{\%}. We find that it automatically learns representations of side effects indicative of co-occurrence of polypharmacy in patients. Furthermore, Decagon models particularly well side effects with a strong molecular basis, while on predominantly non-molecular side effects, it achieves good performance because of effective sharing of model parameters across edge types. Decagon creates opportunities to use large pharmacogenomic and patient data to flag and prioritize side effects for follow-up analysis.},
archivePrefix = {arXiv},
arxivId = {1802.00543},
author = {Zitnik, Marinka and Agrawal, Monica and Leskovec, Jure},
booktitle = {Bioinformatics},
doi = {10.1093/bioinformatics/bty294},
eprint = {1802.00543},
isbn = {13674811 (Electronic)},
issn = {14602059},
month = {jul},
number = {13},
pages = {i457--i466},
pmid = {29949996},
publisher = {Oxford University Press},
title = {{Modeling polypharmacy side effects with graph convolutional networks}},
url = {https://academic.oup.com/bioinformatics/article/34/13/i457/5045770},
volume = {34},
year = {2018}
}
@article{Janssens2017,
author = {Janssens, A. Cecile J.W.},
doi = {10.7287/peerj.preprints.3468v1},
issn = {2167-9843},
keywords = {auc,discrimination,performance,prediction,risk,roc curve},
month = {dec},
publisher = {PeerJ Inc.},
title = {{A more intuitive interpretation of the area under the ROC curve}},
url = {https://peerj.com/preprints/3468/},
year = {2017}
}
@article{Lin2017b,
abstract = {While only recently developed, the ability to profile expression data in single cells (scRNA-Seq) has already led to several important studies and findings. However, this technology has also raised several new computational challenges. These include questions about the best methods for clustering scRNA-Seq data, how to identify unique group of cells in such experiments , and how to determine the state or function of specific cells based on their expression profile. To address these issues we develop and test a method based on neural networks (NN) for the analysis and retrieval of single cell RNA-Seq data. We tested various NN architectures, some of which incorporate prior biological knowledge, and used these to obtain a reduced dimension representation of the single cell expression data. We show that the NN method improves upon prior methods in both, the ability to correctly group cells in experiments not used in the training and the ability to correctly infer cell type or state by querying a database of tens of thousands of single cell profiles. Such database queries (which can be performed using our web server) will enable researchers to better characterize cells when analyzing heterogeneous scRNA-Seq samples.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Lin, Chieh and Jain, Siddhartha and Kim, Hannah and Bar-Joseph, Ziv},
doi = {10.1093/nar/gkx681},
eprint = {NIHMS150003},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {13624962},
journal = {Nucleic Acids Res.},
month = {sep},
number = {17},
pages = {e156},
pmid = {28973464},
publisher = {Oxford University Press},
title = {{Using neural networks for reducing the dimensions of single-cell RNA-Seq data}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28973464 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5737331},
volume = {45},
year = {2017}
}
@article{King2001,
abstract = {Data mining techniques are becoming increasingly important in chemistry as databases become too large to examine manually. Data mining methods from the field of Inductive Logic Programming (ILP) have potential advantages for structural chemical data. In this paper we present Warmr, the first ILP data mining algorithm to be applied to chemoinformatic data. We illustrate the value of Warmr by applying it to a well studied database of chemical compounds tested for carcinogenicity in rodents. Data mining was used to find all frequent substructures in the database, and knowledge of these frequent substructures is shown to add value to the database. One use of the frequent substructures was to convert them into probabilistic prediction rules relating compound description to carcinogenesis. These rules were found to be accurate on test data, and to give some insight into the relationship between structure and activity in carcinogenesis. The substructures were also used to prove that there existed no accurate rule, based purely on atom-bond substructure with less than seven conditions, that could predict carcinogenicity. This results put a lower bound on the complexity of the relationship between chemical structure and carcinogenicity. Only by using a data mining algorithm, and by doing a complete search, is it possible to prove such a result. Finally the frequent substructures were shown to add value by increasing the accuracy of statistical and machine learning programs that were trained to predict chemical carcinogenicity. We conclude that Warmr, and ILP data mining methods generally, are an important new tool for analysing chemical databases.},
author = {King, Ross D. and Srinivasan, Ashwin and Dehaspe, Luc},
doi = {10.1023/A:1008171016861},
issn = {0920654X},
journal = {J. Comput. Aided. Mol. Des.},
keywords = {Carcinogenesis,Chemical structure,Inductive logic programming,Machine learning,Predictive toxicology},
number = {2},
pages = {173--181},
publisher = {Kluwer Academic Publishers},
title = {{Warmr: A data mining tool for chemical data}},
url = {http://link.springer.com/10.1023/A:1008171016861},
volume = {15},
year = {2001}
}
@misc{mesa_abm_python,
title = {{Mesa: Agent-based modeling in Python}},
url = {https://mesa.readthedocs.io/en/master/},
urldate = {2019-12-15}
}
@article{Werneke2012,
abstract = {OBJECTIVE: To establish whether lithium or anticonvulsant should be used for maintenance treatment for bipolar affective disorder (BPAD) if the risks of suicide and relapse were traded off against the risk of end-stage renal disease (ESRD).$\backslash$n$\backslash$nMETHOD: Decision analysis based on a systematic literature review with two main decisions: (1) use of lithium or at treatment initiation and (2) the potential discontinuation of lithium in patients with chronic kidney disease (CKD) after 20 years of lithium treatment. The final endpoint was 30 years of treatment with five outcomes to consider: death from suicide, alive with stable or unstable BPAD, alive with or without ESRD.$\backslash$n$\backslash$nRESULTS: At the start of treatment, the model identified lithium as the treatment of choice. The risks of developing CKD or ESRD were not relevant at the starting point. Twenty years into treatment, lithium still remained treatment of choice. If CKD had occurred at this point, stopping lithium would only be an option if the likelihood of progression to ESRD exceeded 41.3{\%} or if anticonvulsants always outperformed lithium regarding relapse prevention.$\backslash$n$\backslash$nCONCLUSION: At the current state of knowledge, lithium initiation and continuation even in the presence of long-term adverse renal effects should be recommended in most cases.},
author = {Werneke, U. and Ott, M. and Renberg, E. Salander and Taylor, D. and Stegmayr, B.},
doi = {10.1111/j.1600-0447.2012.01847.x},
issn = {0001690X},
journal = {Acta Psychiatr. Scand.},
keywords = {Bipolar disorder,Decision analysis,End-stage renal disease,Lithium,Suicide},
month = {sep},
number = {3},
pages = {186--197},
publisher = {John Wiley {\&} Sons, Ltd (10.1111)},
title = {{A decision analysis of long-term lithium treatment and the risk of renal failure}},
url = {http://doi.wiley.com/10.1111/j.1600-0447.2012.01847.x},
volume = {126},
year = {2012}
}
@misc{Hofler2005,
abstract = {Background: The counterfactual or potential outcome model has become increasingly standard for causal inference in epidemiological and medical studies. Discussion: This paper provides an overview on the counterfactual and related approaches. A variety of conceptual as well as practical issues when estimating causal effects are reviewed. These include causal interactions, imperfect experiments, adjustment for confounding, time-varying exposures, competing risks and the probability of causation. It is argued that the counterfactual model of causal effects captures the main aspects of causality in health sciences and relates to many statistical procedures. Summary: Counterfactuals are the basis of causal inference in medicine and epidemiology. Nevertheless, the estimation of counterfactual differences pose several difficulties, primarily in observational studies. These problems, however, reflect fundamental barriers only when learning from observations, and this does not invalidate the counterfactual concept. {\textcopyright} 2005 H{\"{o}}fler; licensee BioMed Central Ltd.},
author = {H{\"{o}}fler, M},
booktitle = {BMC Med. Res. Methodol.},
doi = {10.1186/1471-2288-5-28},
issn = {14712288},
keywords = {Health Sciences,Medicine,Statistical Theory and Methods,Statistics for Life Sciences,Theory of Medicine/Bioethics},
month = {dec},
number = {1},
pages = {28},
publisher = {BioMed Central},
title = {{Causal inference based on counterfactuals}},
url = {http://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-5-28},
volume = {5},
year = {2005}
}
@article{Wu2019b,
abstract = {Graph Convolutional Networks (GCNs) and their variants have experienced significant attention and have become the de facto methods for learning graph representations. GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complexity and redundant computation. In this paper, we reduce this excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers. We theoretically analyze the resulting linear model and show that it corresponds to a fixed low-pass filter followed by a linear classifier. Notably, our experimental evaluation demonstrates that these simplifications do not negatively impact accuracy in many downstream applications. Moreover, the resulting model scales to larger datasets, is naturally interpretable, and yields up to two orders of magnitude speedup over FastGCN.},
archivePrefix = {arXiv},
arxivId = {1902.07153},
author = {Wu, Felix and Zhang, Tianyi and de Souza, Amauri Holanda and Fifty, Christopher and Yu, Tao and Weinberger, Kilian Q.},
eprint = {1902.07153},
month = {feb},
title = {{Simplifying Graph Convolutional Networks}},
url = {http://arxiv.org/abs/1902.07153},
year = {2019}
}
@article{Eraslan2019,
abstract = {Single-cell RNA sequencing (scRNA-seq) has enabled researchers to study gene expression at a cellular resolution. However, noise due to amplification and dropout may obstruct analyses, so scalable denoising methods for increasingly large but sparse scRNA-seq data are needed. We propose a deep count autoencoder network (DCA) to denoise scRNA-seq datasets. DCA takes the count distribution, overdispersion and sparsity of the data into account using a negative binomial noise model with or without zero-inflation, and nonlinear gene-gene dependencies are captured. Our method scales linearly with the number of cells and can, therefore, be applied to datasets of millions of cells. We demonstrate that DCA denoising improves a diverse set of typical scRNA-seq data analyses using simulated and real datasets. DCA outperforms existing methods for data imputation in quality and speed, enhancing biological discovery.},
author = {Eraslan, G{\"{o}}kcen and Simon, Lukas M. and Mircea, Maria and Mueller, Nikola S. and Theis, Fabian J.},
doi = {10.1038/s41467-018-07931-2},
issn = {20411723},
journal = {Nat. Commun.},
keywords = {Computational models,Machine learning,Statistical methods},
month = {dec},
number = {1},
pages = {1--14},
pmid = {30674886},
publisher = {Nature Publishing Group},
title = {{Single-cell RNA-seq denoising using a deep count autoencoder}},
volume = {10},
year = {2019}
}
@article{Carter2018,
abstract = {In clinical populations, suicidal behaviors are future events that should be the focus of prevention. A recent systematic review estimated the frequency of suicide after hospital treated self-harm at 1.6{\%} after 12 months and 3.9{\%} at 5 years, with repetition of self-harm at 16.3{\%} after 12 months (Carroll, Metcalfe, {\&} Gunnell, 2014b). Another systematic review focusing on psychiatric inpatients estimated that suicide after discharge was around 0.5{\%} per-person-year; the highest rate was 1.1{\%} per-person-year 3 months after discharge (Chung et al., 2017). In the UK, 6.5{\%} of psychiatric inpatients were admitted for self-harm 12 months after discharge (Gunnell et al., 2008). Risk assessments are done to classify individuals as high risk or low risk for future suicidal behaviors. This classification (stratification) is used to determine the allocation of after-care aimed at preventing these behaviors. The high-risk stratum are offered specific interventions (e.g., psychiatric hospitalization, close nursing observation [for inpa-tients], face-to-face or telephone follow-up and identified community support) or more intense intervention (e.g., greater frequency of reviews in inpatient and community settings). Risk stratification is widely practiced (Quinlivan et al., 2014) and endorsed (Suicide Prevention Resource Center, 2015). However, the inaccuracy of suicide prediction has been known for 60 years (Rosen, 1954). In a seminal paper, Pokorny used the suicide rate for his patients (500 per 100,000) and invoked an almost perfect predictive instrument (to classify high risk) having a sensitivity of 99{\%} and a specificity of 99{\%} (Pokorny, 1983). Even under these idealized conditions, the positive predictive value (PPV) was 33{\%}, with 66{\%} of the high-risk stratum being false positives, thus demonstrating the absolute statistical ceiling imposed on PPV by the low prevalence of suicide, regardless of the method of stratification.},
author = {Carter, Gregory and Spittal, Matthew J.},
doi = {10.1027/0227-5910/a000558},
issn = {21512396},
journal = {Crisis},
month = {jul},
number = {4},
pages = {229--234},
title = {{Suicide Risk Assessment: Risk Stratification Is Not Accurate Enough to Be Clinically Useful and Alternative Approaches Are Needed}},
url = {https://econtent.hogrefe.com/doi/10.1027/0227-5910/a000558},
volume = {39},
year = {2018}
}
@article{Li2019,
abstract = {Today, despite decades of developments in medicine and the growing interest in precision healthcare, vast majority of diagnoses happen once patients begin to show noticeable signs of illness. Early indication and detection of diseases, however, can provide patients and carers with the chance of early intervention, better disease management, and efficient allocation of healthcare resources. The latest developments in machine learning (more specifically, deep learning) provides a great opportunity to address this unmet need. In this study, we introduce BEHRT: A deep neural sequence transduction model for EHR (electronic health records), capable of multitask prediction and disease trajectory mapping. When trained and evaluated on the data from nearly 1.6 million individuals, BEHRT shows a striking absolute improvement of 8.0-10.8{\%}, in terms of Average Precision Score, compared to the existing state-of-the-art deep EHR models (in terms of average precision, when predicting for the onset of 301 conditions). In addition to its superior prediction power, BEHRT provides a personalised view of disease trajectories through its attention mechanism; its flexible architecture enables it to incorporate multiple heterogeneous concepts (e.g., diagnosis, medication, measurements, and more) to improve the accuracy of its predictions; and its (pre-)training results in disease and patient representations that can help us get a step closer to interpretable predictions.},
archivePrefix = {arXiv},
arxivId = {1907.09538},
author = {Li, Yikuan and Rao, Shishir and Solares, Jose Roberto Ayala and Hassaine, Abdelaali and Canoy, Dexter and Zhu, Yajie and Rahimi, Kazem and Salimi-Khorshidi, Gholamreza},
eprint = {1907.09538},
month = {jul},
title = {{BEHRT: Transformer for Electronic Health Records}},
url = {http://arxiv.org/abs/1907.09538},
year = {2019}
}
@article{Banbura2010,
abstract = {This paper shows that vector auto regression (VAR) with Bayesian shrinkage is an appropriate tool for large dynamic models. We build on the results of De Mol and co-workers (2008) and show that, when the degree of shrinkage is set in relation to the cross-sectional dimension, the forecasting performance of small monetary VARs can be improved by adding additional macroeconomic variables and sectoral information. In addition, we show that large VARs with shrinkage produce credible impulse responses and are suitable for structural analysis.},
archivePrefix = {arXiv},
arxivId = {1099-1255},
author = {Ba{\'{n}}bura, Marta and Giannone, Domenico and Reichlin, Lucrezia},
doi = {10.1002/jae.1137},
eprint = {1099-1255},
isbn = {1099-1255},
issn = {08837252},
journal = {J. Appl. Econom.},
month = {jan},
number = {1},
pages = {71--92},
pmid = {15003161},
publisher = {Wiley-Blackwell},
title = {{Large Bayesian vector auto regressions}},
url = {http://doi.wiley.com/10.1002/jae.1137},
volume = {25},
year = {2010}
}
@article{Killick2014,
abstract = {One of the key challenges in changepoint analysis is the ability to detect multiple changes within a given time series or sequence. The changepoint package has been developed to provide users with a choice of multiple changepoint search methods to use in conjunction with a given changepoint method and in particular provides an implementation of the recently proposed PELT algorithm. This article describes the search methods which are implemented in the package as well as some of the available test statistics whilst highlighting their application with simulated and practical examples. segmentation, break points, search methods},
author = {Killick, Rebecca and Eckley, Idris A.},
doi = {10.18637/jss.v058.i03},
issn = {1548-7660},
journal = {J. Stat. Softw.},
month = {jun},
number = {3},
pages = {1--19},
title = {{changepoint : An R Package for Changepoint Analysis}},
url = {http://www.jstatsoft.org/v58/i03/},
volume = {58},
year = {2015}
}
@article{Gu2008,
abstract = {BACKGROUND: Biclustering of gene expression data searches for local patterns of gene expression. A bicluster (or a two-way cluster) is defined as a set of genes whose expression profiles are mutually similar within a subset of experimental conditions/samples. Although several biclustering algorithms have been studied, few are based on rigorous statistical models. RESULTS: We developed a Bayesian biclustering model (BBC), and implemented a Gibbs sampling procedure for its statistical inference. We showed that Bayesian biclustering model can correctly identify multiple clusters of gene expression data. Using simulated data both from the model and with realistic characters, we demonstrated the BBC algorithm outperforms other methods in both robustness and accuracy. We also showed that the model is stable for two normalization methods, the interquartile range normalization and the smallest quartile range normalization. Applying the BBC algorithm to the yeast expression data, we observed that majority of the biclusters we found are supported by significant biological evidences, such as enrichments of gene functions and transcription factor binding sites in the corresponding promoter sequences. CONCLUSIONS: The BBC algorithm is shown to be a robust model-based biclustering method that can discover biologically significant gene-condition clusters in microarray data. The BBC model can easily handle missing data via Monte Carlo imputation and has the potential to be extended to integrated study of gene transcription networks.},
author = {Gu, Jiajun and Liu, Jun S},
doi = {10.1186/1471-2164-9-S1-S4},
issn = {14712164},
journal = {BMC Genomics},
number = {SUPPL. 1},
pages = {S4},
pmid = {18366617},
title = {{Bayesian biclustering of gene expression data}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18366617 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2386069 http://bmcgenomics.biomedcentral.com/articles/10.1186/1471-2164-9-S1-S4},
volume = {9},
year = {2008}
}
@techreport{Ratner2019,
abstract = {Machine learning (ML) techniques are enjoying rapidly increasing adoption. However, designing and implementing the systems that support ML models in real-world deployments remains a significant obstacle, in large part due to the radically different development and deployment profile of modern ML methods, and the range of practical concerns that come with broader adoption. We propose to foster a new systems machine learning research community at the intersection of the traditional systems and ML communities, focused on topics such as hardware systems for ML, software systems for ML, and ML optimized for metrics beyond predictive accuracy. To do this, we describe a new conference, SysML, that explicitly targets research at the intersection of systems and machine learning with a program committee split evenly between experts in systems and ML, and an explicit focus on topics at the intersection of the two.},
archivePrefix = {arXiv},
arxivId = {1904.03257},
author = {Ratner, Alexander and Alistarh, Dan and Alonso, Gustavo and Andersen, David G and Bailis, Peter and Bird, Sarah and Carlini, Nicholas and Catanzaro, Bryan and Chayes, Jennifer and Chung, Eric and Dally, Bill and Dean, Jeff and Dhillon, Inderjit S. and Dimakis, Alexandros and Dubey, Pradeep and Elkan, Charles and Fursin, Grigori and Ganger, Gregory R. and Getoor, Lise and Gibbons, Phillip B. and Gibson, Garth A. and Gonzalez, Joseph E. and Gottschlich, Justin and Han, Song and Hazelwood, Kim and Huang, Furong and Jaggi, Martin and Jamieson, Kevin and Jordan, Michael I. and Joshi, Gauri and Khalaf, Rania and Knight, Jason and Kone{\v{c}}n{\'{y}}, Jakub and Kraska, Tim and Kumar, Arun and Kyrillidis, Anastasios and Lakshmiratan, Aparna and Li, Jing and Madden, Samuel and {Brendan McMahan}, H and Meijer, Erik and Monga, Rajat and Murray, Derek and Olukotun, Kunle and Papailiopoulos, Dimitris and Pekhimenko, Gennady and R{\'{e}}, Christopher and Rekatsinas, Theodoros and Rostamizadeh, Afshin and {De Sa}, Christopher and Sedghi, Hanie and Sen, Siddhartha and Smith, Virginia and Udell, Madeleine and Vanschoren, Joaquin and Venkataraman, Shivaram and Vinayak, Rashmi and Weimer, Markus and {Gordon Wilson}, Andrew and Xing, Eric and Zaharia, Matei and Zhang, Ce and Talwalkar, Ameet},
booktitle = {Ioannis Mitliagkas},
eprint = {1904.03257},
month = {mar},
pages = {31},
title = {{SysML: The New Frontier of Machine Learning Systems}},
url = {http://arxiv.org/abs/1904.03257 https://www.sysml.cc/doc/sysml-whitepaper.pdf},
volume = {27},
year = {2019}
}
@article{Bodnar2019a,
abstract = {The distributional perspective on reinforcement learning (RL) has given rise to a series of successful Q-learning algorithms, resulting in state-of-the-art performance in arcade game environments. However, it has not yet been analyzed how these findings from a discrete setting translate to complex practical applications characterized by noisy, high dimensional and continuous state-action spaces. In this work, we propose Quantile QT-Opt (Q2-Opt), a distributional variant of the recently introduced distributed Q-learning algorithm for continuous domains, and examine its behaviour in a series of simulated and real vision-based robotic grasping tasks. The absence of an actor in Q2-Opt allows us to directly draw a parallel to the previous discrete experiments in the literature without the additional complexities induced by an actor-critic architecture. We demonstrate that Q2-Opt achieves a superior vision-based object grasping success rate, while also being more sample efficient. The distributional formulation also allows us to experiment with various risk-distortion metrics that give us an indication of how robots can concretely manage risk in practice using a Deep RL control policy. As an additional contribution, we perform experiments on offline datasets and compare them with the latest findings from discrete settings. Surprisingly, we find that there is a discrepancy between our results and the previous batch RL findings from the literature obtained on arcade game environments.},
archivePrefix = {arXiv},
arxivId = {1910.02787},
author = {Bodnar, Cristian and Li, Adrian and Hausman, Karol and Pastor, Peter and Kalakrishnan, Mrinal},
eprint = {1910.02787},
month = {oct},
title = {{Quantile QT-Opt for Risk-Aware Vision-Based Robotic Grasping}},
url = {http://arxiv.org/abs/1910.02787},
year = {2019}
}
@article{Chen2015d,
abstract = {Topological data analysis (TDA) can broadly be described as a collection of data analysis methods that find structure in data. These methods include clustering, manifold estimation, nonlinear dimension reduction, mode estimation, ridge estimation and persistent homology. This paper reviews some of these methods.},
author = {Chen, Li M.},
doi = {10.1007/978-3-319-25127-1_6},
isbn = {9783319251271},
issn = {0092-8240},
journal = {Math. Probl. Data Sci. Theor. Pract. Methods},
month = {jul},
number = {7},
pages = {101--124},
publisher = {Springer US},
title = {{Topological data analysis}},
url = {http://link.springer.com/10.1007/s11538-019-00610-3},
volume = {81},
year = {2015}
}
@inproceedings{Lipton2016,
abstract = {Supervised machine learning models boast re-markable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but inter-pretable. And yet the task of interpretation ap-pears underspecified. Papers provide diverse and sometimes non-overlapping motivations for in-terpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim inter-pretability axiomatically, absent further explana-tion. In this paper, we seek to refine the dis-course on interpretability. First, we examine the motivations underlying interest in interpretabil-ity, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of dif-ferent notions, and question the oft-made asser-tions that linear models are interpretable and that deep neural networks are not.},
archivePrefix = {arXiv},
arxivId = {arXiv:1606.03490v1},
author = {Lipton, Zachary C.},
booktitle = {ICML Work. Hum. Interpret. Mach. Learn.},
eprint = {arXiv:1606.03490v1},
keywords = {Black Box,Deep Learning,Interpretability,Machine Learning,Supervised Learning},
month = {jun},
pages = {96--100},
title = {{The Mythos of Model Interpretability}},
url = {http://arxiv.org/abs/1606.03490},
year = {2016}
}
@article{Davis2018a,
abstract = {Background: Despite lithium being the most efficacious treatment for bipolar disorder, its use has been decreasing at least in part due to concerns about its potential to cause significant nephrotoxicity. Whilst the ability of lithium to cause nephrogenic diabetes insipidus is well established, its ability to cause chronic kidney disease is a much more vexing issue, with various studies suggesting both positive and negative causality. Despite these differences, the weight of evidence suggests that lithium has the potential to cause end stage kidney disease, albeit over a prolonged period. Methods: A search strategy for this review was developed to identify appropriate studies, sourced from the electronic databases EMBASE, PubMed (NLM) and MEDLINE. Search terms included lithium with the AND operator to combine with nephrotoxicity or nephropathy or chronic kidney disease or nephrogenic diabetes insipidus or renal and pathophysiology. Results: The risks for the development of lithium induced nephropathy are less well defined but appear to include the length of duration of therapy as well as increasing age, as well as episodes of over dosage/elevated lithium levels. Whilst guidelines exist for the routine monitoring of lithium levels and renal function, it remains unclear when nephrological evaluation should occur, as well as when cessation of lithium therapy is appropriate balancing the significant attendant mental health risks as well as the potential for progression to occur despite cessation of therapy against the risks and morbidity of bipolar disorder itself. Conclusion: This paper will elucidate on the current evidence pertaining to the topic of the clinical management of lithium induced nephrotoxicity and provide a guide for clinicians who are faced with the long-term management of these patients.},
author = {Davis, J. and Desmond, M. and Berk, M.},
doi = {10.1186/s12882-018-1101-4},
issn = {14712369},
journal = {BMC Nephrol.},
keywords = {Chronic kidney disease,Lithium,Nephrogenic diabetes insipidus,Nephrotoxicity},
month = {dec},
number = {1},
pages = {305},
pmid = {30390660},
title = {{Lithium and nephrotoxicity: A literature review of approaches to clinical management and risk stratification}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/30390660 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC6215627 https://bmcnephrol.biomedcentral.com/articles/10.1186/s12882-018-1101-4},
volume = {19},
year = {2018}
}
@article{Lapuschkin2019,
abstract = {Current learning machines have successfully solved hard application problems, reaching high accuracy and displaying seemingly intelligent behavior. Here we apply recent techniques for explaining decisions of state-of-the-art learning machines and analyze various tasks from computer vision and arcade games. This showcases a spectrum of problem-solving behaviors ranging from naive and short-sighted, to well-informed and strategic. We observe that standard performance evaluation metrics can be oblivious to distinguishing these diverse problem solving behaviors. Furthermore, we propose our semi-automated Spectral Relevance Analysis that provides a practically effective way of characterizing and validating the behavior of nonlinear learning machines. This helps to assess whether a learned model indeed delivers reliably for the problem that it was conceived for. Furthermore, our work intends to add a voice of caution to the ongoing excitement about machine intelligence and pledges to evaluate and judge some of these recent successes in a more nuanced manner.},
archivePrefix = {arXiv},
arxivId = {1902.10178},
author = {Lapuschkin, Sebastian and W{\"{a}}ldchen, Stephan and Binder, Alexander and Montavon, Gr{\'{e}}goire and Samek, Wojciech and M{\"{u}}ller, Klaus Robert},
doi = {10.1038/s41467-019-08987-4},
eprint = {1902.10178},
issn = {20411723},
journal = {Nat. Commun.},
month = {dec},
number = {1},
publisher = {Nature Publishing Group},
title = {{Unmasking Clever Hans predictors and assessing what machines really learn}},
volume = {10},
year = {2019}
}
@article{Feurer2015,
abstract = {The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods. In this work we introduce a robust new AutoML system based on scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters). This system, which we dub auto-sklearn, improves on existing AutoML methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization. Our system won the first phase of the ongoing ChaLearn AutoML challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in AutoML.We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of auto-sklearn. 1},
author = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost and Blum, Manuel and Hutter, Frank},
issn = {10495258},
journal = {Adv. Neural Inf. Process. Syst. 28},
pages = {2944--2952},
publisher = {MIT Press},
title = {{Efficient and Robust Automated Machine Learning}},
url = {http://dl.acm.org/citation.cfm?id=2969547 http://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf},
year = {2015}
}
@article{Walsham2006,
abstract = {Interpretive research in information systems (IS) is now a well-established part of the field. However, there is a need for more material on how to carry out such work from inception to publication. I published a paper a decade ago (Walsham, 1995) which addressed the nature of interpretive IS case studies and methods for doing such research. The current paper extends this earlier contribution, with a widened scope of all interpretive research in IS, and through further material on carrying out fieldwork, using theory and analysing data. In addition, new topics are discussed on constructing and justifying a research contribution, and on ethical issues and tensions in the conduct of interpretive work. The primary target audience for the paper is less-experienced IS researchers, but I hope that the paper will also stimulate reflection for the more-experienced IS researcher and be of relevance to interpretive researchers in other social science fields. {\textcopyright} 2006 Operational Research Society Ltd. All rights reserved.},
author = {Walsham, Geoff},
doi = {10.1057/palgrave.ejis.3000589},
issn = {0960085X},
journal = {Eur. J. Inf. Syst.},
keywords = {Data analysis,Ethical issues,Fieldwork,Information systems,Interpretive research,Research contribution,Use of theory},
month = {jun},
number = {3},
pages = {320--330},
publisher = {Taylor {\&} Francis},
title = {{Doing interpretive research}},
url = {https://www.tandfonline.com/doi/full/10.1057/palgrave.ejis.3000589},
volume = {15},
year = {2006}
}
@article{Brouwer2017,
abstract = {We introduce a novel Bayesian hybrid matrix factorisation model (HMF) for data integration, based on combining multiple matrix factorisation methods, that can be used for in- and out-of-matrix prediction of missing values. The model is very general and can be used to integrate many datasets across different entity types, including repeated experiments, similarity matrices, and very sparse datasets. We apply our method on two biological applications, and extensively compare it to state-of-the-art machine learning and matrix factorisation models. For in-matrix predictions on drug sensitivity datasets we obtain consistently better performances than existing methods. This is especially the case when we increase the sparsity of the datasets. Furthermore, we perform out-of-matrix predictions on methylation and gene expression datasets, and obtain the best results on two of the three datasets, especially when the predictivity of datasets is high.},
archivePrefix = {arXiv},
arxivId = {1704.04962},
author = {Brouwer, Thomas and Li{\'{o}}, Pietro},
eprint = {1704.04962},
issn = {1938-7228},
month = {apr},
title = {{Bayesian Hybrid Matrix Factorisation for Data Integration}},
url = {http://arxiv.org/abs/1704.04962},
year = {2017}
}
@incollection{Woods2013,
abstract = {The paper is concerned with the theoretical underpinnings for semantic network representations. It is concerned specifically with understanding the semantics of the semantic network structures themselves, i.e., with what the notations and structures used in a semantic network can mean, and with interpretations of what these links mean that will be logically adequate to the job of representing knowledge. It focuses on several issues: the meaning of 'semantics', the need for explicit understanding of the intended meanings for various types of arcs and links, the need for careful thought in choosing conventions for representing facts as assemblages of arcs and nodes, and several specific difficult problems in knowledge representation - especially problems of relative clauses and quantification.},
author = {Woods, William A.},
booktitle = {Readings Cogn. Sci. A Perspect. from Psychol. Artif. Intell.},
doi = {10.1016/B978-1-4832-1446-7.50014-5},
isbn = {1558600132},
month = {jan},
pages = {102--125},
publisher = {Morgan Kaufmann},
title = {{What's in a link: Foundations for Semantic Networks}},
url = {https://www.sciencedirect.com/science/article/pii/B9780121085506500070},
year = {2013}
}
@misc{ai_extenders,
title = {{AI Extenders: The Ethical and Societal Implications of Humans Cognitively Extended by AI | Department of Computer Science and Technology}},
url = {https://www.cst.cam.ac.uk/seminars/ai-extenders-ethical-and-societal-implications-humans-cognitively-extended-ai},
urldate = {2019-05-18}
}
@article{Elish2016,
author = {Elish, M C},
doi = {10.2139/ssrn.2757236},
issn = {1556-5068},
journal = {SSRN Electron. J.},
keywords = {HCI,HRI,accidents,automation,autonomous vehicles,autonomy,driverless cars,ethics,human factors,human in the loop,human-in-the-loop,human-robot interaction,machine learning,responsibility,robots,self-driving cars,social perceptions of technology},
month = {mar},
title = {{Moral Crumple Zones: Cautionary Tales in Human-Robot Interaction (WeRobot 2016)}},
url = {http://www.ssrn.com/abstract=2757236},
year = {2016}
}
@book{pearl2009causality,
author = {Pearl, Judea},
publisher = {Cambridge University Press},
title = {{Causality}},
year = {2009}
}
@article{Scholkopf2019a,
abstract = {Graphical causal inference as pioneered by Judea Pearl arose from research on artificial intelligence (AI), and for a long time had little connection to the field of machine learning. This article discusses where links have been and should be established, introducing key concepts along the way. It argues that the hard open problems of machine learning and AI are intrinsically related to causality, and explains how the field is beginning to understand them.},
archivePrefix = {arXiv},
arxivId = {1911.10500},
author = {Sch{\"{o}}lkopf, Bernhard},
eprint = {1911.10500},
month = {nov},
title = {{Causality for Machine Learning}},
url = {http://arxiv.org/abs/1911.10500},
year = {2019}
}
@article{Elshawi2019,
abstract = {Although complex machine learning models are commonly outperforming the traditional simple interpretable models, clinicians find it hard to understand and trust these complex models due to the lack of intuition and explanation of their predictions. The aim of this study to demonstrate the utility of various model-agnostic explanation techniques of machine learning models with a case study for analyzing the outcomes of the machine learning random forest model for predicting the individuals at risk of developing hypertension based on cardiorespiratory fitness data. The dataset used in this study contains information of 23,095 patients who underwent clinician-referred exercise treadmill stress testing at Henry Ford Health Systems between 1991 and 2009 and had a complete 10-year follow-up. Five global interpretability techniques (Feature Importance, Partial Dependence Plot, Individual Conditional Expectation, Feature Interaction, Global Surrogate Models) and two local interpretability techniques (Local Surrogate Models, Shapley Value) have been applied to present the role of the interpretability techniques on assisting the clinical staff to get better understanding and more trust of the outcomes of the machine learning-based predictions. Several experiments have been conducted and reported. The results show that different interpretability techniques can shed light on different insights on the model behavior where global interpretations can enable clinicians to understand the entire conditional distribution modeled by the trained response function. In contrast, local interpretations promote the understanding of small parts of the conditional distribution for specific instances. Various interpretability techniques can vary in their explanations for the behavior of the machine learning model. The global interpretability techniques have the advantage that it can generalize over the entire population while local interpretability techniques focus on giving explanations at the level of instances. Both methods can be equally valid depending on the application need. Both methods are effective methods for assisting clinicians on the medical decision process, however, the clinicians will always remain to hold the final say on accepting or rejecting the outcome of the machine learning models and their explanations based on their domain expertise.},
author = {Elshawi, Radwa and Al-Mallah, Mouaz H. and Sakr, Sherif},
doi = {10.1186/s12911-019-0874-0},
issn = {1472-6947},
journal = {BMC Med. Inform. Decis. Mak.},
keywords = {Health Informatics,Information Systems and Communication Service,Management of Computing and Information Systems},
month = {dec},
number = {1},
pages = {146},
publisher = {BioMed Central},
title = {{On the interpretability of machine learning-based model for predicting hypertension}},
url = {https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-019-0874-0},
volume = {19},
year = {2019}
}
@article{Thomas2019a,
abstract = {Intelligent machines using machine learning algorithms are ubiquitous, ranging from simple data analysis and pattern recognition tools to complex systems that achieve superhuman performance on various tasks. Ensuring that they do not exhibit undesirable behavior—that they do not, for example, cause harm to humans—is therefore a pressing problem. We propose a general and flexible framework for designing machine learning algorithms. This framework simplifies the problem of specifying and regulating undesirable behavior. To show the viability of this framework, we used it to create machine learning algorithms that precluded the dangerous behavior caused by standard machine learning algorithms in our experiments. Our framework for designing machine learning algorithms simplifies the safe and responsible application of machine learning.},
author = {Thomas, Philip S. and da Silva, Bruno Castro and Barto, Andrew G. and Giguere, Stephen and Brun, Yuriy and Brunskill, Emma},
doi = {10.1126/science.aag3311},
issn = {10959203},
journal = {Science (80-. ).},
month = {nov},
number = {6468},
pages = {999--1004},
pmid = {31754000},
publisher = {American Association for the Advancement of Science},
title = {{Preventing undesirable behavior of intelligent machines}},
volume = {366},
year = {2019}
}
@article{Fernandez-Egea2019,
author = {Fernandez-Egea, Emilio},
doi = {10.1093/brain/awy262},
issn = {14602156},
journal = {Brain},
month = {jan},
number = {1},
pages = {220--226},
publisher = {Oxford University Press},
title = {{One hundred years ago: Nijinsky and the origins of schizophrenia}},
url = {https://academic.oup.com/brain/article/142/1/220/5144594},
volume = {142},
year = {2019}
}
@article{Nowak2006,
abstract = {Cooperation is needed for evolution to construct new levels of organization. Genomes, cells, multicellular organisms, social insects, and human society are all based on cooperation. Cooperation means that selfish replicators forgo some of their reproductive potential to help one another. But natural selection implies competition and therefore opposes cooperation unless a specific mechanism is at work. Here I discuss five mechanisms for the evolution of cooperation: kin selection, direct reciprocity, indirect reciprocity, network reciprocity, and group selection. For each mechanism, a simple rule is derived that specifies whether natural selection can lead to cooperation.},
author = {Nowak, Martin A.},
doi = {10.1126/science.1133755},
issn = {00368075},
journal = {Science (80-. ).},
month = {dec},
number = {5805},
pages = {1560--1563},
pmid = {17158317},
publisher = {American Association for the Advancement of Science},
title = {{Five rules for the evolution of cooperation}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17158317 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3279745 http://www.sciencemag.org/cgi/doi/10.1126/science.1133755},
volume = {314},
year = {2006}
}
@article{Johnson2019,
abstract = {Natural selection drives populations toward higher fitness, but second-order selection for adaptability and mutational robustness can also influence evolution. In many microbial systems, diminishing-returns epistasis contributes to a tendency for more-fit genotypes to be less adaptable, but no analogous patterns for robustness are known. To understand how robustness varies across genotypes, we measure the fitness effects of hundreds of individual insertion mutations in a panel of yeast strains. We find that more-fit strains are less robust: They have distributions of fitness effects with lower mean and higher variance. These differences arise because many mutations have more strongly deleterious effects in faster-growing strains. This negative correlation between fitness and robustness implies that second-order selection for robustness will tend to conflict with first-order selection for fitness.},
author = {Johnson, Milo S and Martsul, Alena and Kryazhimskiy, Sergey and Desai, Michael M},
doi = {10.1126/science.aay4199},
issn = {1095-9203},
journal = {Science},
month = {oct},
number = {6464},
pages = {490--493},
pmid = {31649199},
publisher = {American Association for the Advancement of Science},
title = {{Higher-fitness yeast genotypes are less robust to deleterious mutations.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/31649199},
volume = {366},
year = {2019}
}
@article{Wu2019a,
abstract = {We investigate opportunities and challenges for improving unsupervised machine learning using four common strategies with a long history in physics: divide-and-conquer, Occam's razor, unification and lifelong learning. Instead of using one model to learn everything, we propose a novel paradigm centered around the learning and manipulation of *theories*, which parsimoniously predict both aspects of the future (from past observations) and the domain in which these predictions are accurate. Specifically, we propose a novel generalized-mean-loss to encourage each theory to specialize in its comparatively advantageous domain, and a differentiable description length objective to downweight bad data and "snap" learned theories into simple symbolic formulas. Theories are stored in a "theory hub", which continuously unifies learned theories and can propose theories when encountering new environments. We test our implementation, the toy "AI Physicist" learning agent, on a suite of increasingly complex physics environments. From unsupervised observation of trajectories through worlds involving random combinations of gravity, electromagnetism, harmonic motion and elastic bounces, our agent typically learns faster and produces mean-squared prediction errors about a billion times smaller than a standard feedforward neural net of comparable complexity, typically recovering integer and rational theory parameters exactly. Our agent successfully identifies domains with different laws of motion also for a nonlinear chaotic double pendulum in a piecewise constant force field.},
archivePrefix = {arXiv},
arxivId = {1810.10525},
author = {Wu, Tailin and Tegmark, Max},
doi = {10.1103/PhysRevE.100.033311},
eprint = {1810.10525},
issn = {2470-0045},
journal = {Phys. Rev. E},
month = {sep},
number = {3},
pages = {033311},
publisher = {American Physical Society},
title = {{Toward an AI Physicist for Unsupervised Learning}},
url = {https://link.aps.org/doi/10.1103/PhysRevE.100.033311 http://arxiv.org/abs/1810.10525},
volume = {100},
year = {2018}
}
@inproceedings{Katz2017,
abstract = {Deep neural networks have emerged as a widely used and effective means for tackling complex, real-world problems. However, a major obstacle in applying them to safety-critical systems is the great difficulty in providing formal guarantees about their behavior. We present a novel, scalable, and efficient technique for verifying properties of deep neural networks (or providing counter-examples). The technique is based on the simplex method, extended to handle the non-convex Rectified Linear Unit (ReLU) activation function, which is a crucial ingredient in many modern neural networks. The verification procedure tackles neural networks as a whole, without making any simplifying assumptions. We evaluated our technique on a prototype deep neural network implementation of the next-generation airborne collision avoidance system for unmanned aircraft (ACAS Xu). Results show that our technique can successfully prove properties of networks that are an order of magnitude larger than the largest networks verified using existing methods.},
archivePrefix = {arXiv},
arxivId = {1702.01135},
author = {Katz, Guy and Barrett, Clark and Dill, David L. and Julian, Kyle and Kochenderfer, Mykel J.},
booktitle = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
doi = {10.1007/978-3-319-63387-9_5},
eprint = {1702.01135},
isbn = {9783319633862},
issn = {16113349},
month = {feb},
pages = {97--117},
title = {{Reluplex: An efficient SMT solver for verifying deep neural networks}},
url = {http://arxiv.org/abs/1702.01135},
volume = {10426 LNCS},
year = {2017}
}
@article{DiBenigno2014,
abstract = {We use data from a 12-month ethnographic study of two medical-surgical units in a U.S. hospital to examine how members from different occupations can collaborate with one another in their daily work despite differences in status, shared meanings, and expertise across occupational groups, which previous work has shown to create difficulties. In our study, nurses and patient care technicians (PCTs) on both hospital units faced these same occupational differences, served the same patient population, worked under the same management and organizational structure, and had the same pressures, goals, and organizational collaboration tools available to them. But nurses and PCTs on one unit successfully collaborated while those on the other did not. We demonstrate that a social structure characterized by cross-cutting demographics between occupational groups-in which occupational membership is uncorrelated with demographic group membership-can loosen attachment to the occupational identity and status order. This allows members of cross-occupational dyads, in our case nurses and PCTs, to draw on other shared social identities, such as shared race, age, or immigration status, in their interactions. Drawing on a shared social identity at the dyad level provided members with a "dyadic toolkit" of alternative, non-occupational expertise, shared meanings, status rules, and emotional scripts that facilitated collaboration across occupational differences and improved patient care. {\textcopyright} The Author(s) 2014.},
author = {DiBenigno, Julia and Kellogg, Katherine C.},
doi = {10.1177/0001839214538262},
issn = {19303815},
journal = {Adm. Sci. Q.},
keywords = {coordination,cross-occupational collaboration,cultural toolkit,demography,group diversity,healthcare,hospitals,intergroup relations,medical sociology,professions,sociology of work and occupations,status},
month = {sep},
number = {3},
pages = {375--408},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Beyond Occupational Differences: The Importance of Cross-cutting Demographics and Dyadic Toolkits for Collaboration in a U.S. Hospital}},
url = {http://journals.sagepub.com/doi/10.1177/0001839214538262},
volume = {59},
year = {2014}
}
@article{Gomez-Cabrero2016,
abstract = {{\textcopyright} 2016 The Author(s). Background: Deep mining of healthcare data has provided maps of comorbidity relationships between diseases. In parallel, integrative multi-omics investigations have generated high-resolution molecular maps of putative relevance for understanding disease initiation and progression. Yet, it is unclear how to advance an observation of comorbidity relations (one disease to others) to a molecular understanding of the driver processes and associated biomarkers. Results: Since Chronic Obstructive Pulmonary disease (COPD) has emerged as a central hub in temporal comorbidity networks, we developed a systematic integrative data-driven framework to identify shared disease-associated genes and pathways, as a proxy for the underlying generative mechanisms inducing comorbidity. We integrated records from approximately 13 M patients from the Medicare database with disease-gene maps that we derived from several resources including a semantic-derived knowledge-base. Using rank-based statistics we not only recovered known comorbidities but also discovered a novel association between COPD and digestive diseases. Furthermore, our analysis provides the first set of COPD co-morbidity candidate biomarkers, including IL15, TNF and JUP, and characterizes their association to aging and life-style conditions, such as smoking and physical activity. Conclusions: The developed framework provides novel insights in COPD and especially COPD co-morbidity associated mechanisms. The methodology could be used to discover and decipher the molecular underpinning of other comorbidity relationships and furthermore, allow the identification of candidate co-morbidity biomarkers.},
author = {Gomez-Cabrero, David and Menche, J{\"{o}}rg and Vargas, Claudia and Cano, Isaac and Maier, Dieter and Barab{\'{a}}si, Albert L{\'{a}}szl{\'{o}} and Tegn{\'{e}}r, Jesper and Roca, Josep},
doi = {10.1186/s12859-016-1291-3},
issn = {14712105},
journal = {BMC Bioinformatics},
keywords = {Algorithms,Bioinformatics,Computational Biology/Bioinformatics,Computer Appl. in Life Sciences,Microarrays},
month = {nov},
number = {S15},
pages = {441},
publisher = {BioMed Central},
title = {{From comorbidities of chronic obstructive pulmonary disease to identification of shared molecular mechanisms by data integration}},
url = {http://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1291-3},
volume = {17},
year = {2016}
}
@book{Welton2012,
abstract = {Introduction -- Bayesian methods and winBUGS -- Introduction to decision models -- Meta-analysis using Bayesian methods -- Exploring between study heterogeneity -- Model critique and evidence consistency in random effects meta-analysis -- Evidence synthesis in a decision modelling framework -- Multi-parameter evidence synthesis in epidemiological models -- Mixed treatment comparisons -- Markov models -- Generalised evidence synthesis -- Expected value of information for research prioritisation and study design. Machine generated contents note: 1.1. rise of health economics -- 1.2. Decision making under uncertainty -- 1.2.1. Deterministic models -- 1.2.2. Probabilistic decision modelling -- 1.3. Evidence-based medicine -- 1.4. Bayesian statistics -- 1.5. NICE -- 1.6. Structure of the book -- 1.7. Summary key points -- 1.8. Further reading -- References -- 2.1. Introduction to Bayesian methods -- 2.1.1. What is a Bayesian approach-- 2.1.2. Likelihood -- 2.1.3. Bayes' theorem and Bayesian updating -- 2.1.4. Prior distributions -- 2.1.5. Summarising the posterior distribution -- 2.1.6. Prediction -- 2.1.7. More realistic and complex models -- 2.1.8. MCMC and Gibbs sampling -- 2.2. Introduction to WinBUGS -- 2.2.1. BUGS language -- 2.2.2. Graphical representation -- 2.2.3. Running WinBUGS -- 2.2.4. Assessing convergence in WinBUGS -- 2.2.5. Statistical inference in WinBUGS -- 2.2.6. Practical aspects of using WinBUGS -- 2.3. Advantages and disadvantages of a Bayesian approach -- 2.4. Summary key points -- 2.5. Further reading -- 2.6. Exercises -- References -- 3.1. Introduction -- 3.2. Decision tree models -- 3.3. Model parameters -- 3.3.1. Effects of interventions -- 3.3.2. Quantities relating to the clinical epidemiology of the clinical condition being treated -- 3.3.3. Utilities -- 3.3.4. Resource use and costs -- 3.4. Deterministic decision tree -- 3.5. Stochastic decision tree -- 3.5.1. Presenting the results of stochastic economic decision models -- 3.6. Sources of evidence -- 3.7. Principles of synthesis for decision models (motivation for the rest of the book) -- 3.8. Summary key points -- 3.9. Further reading -- 3.10. Exercises -- References -- 4.1. Introduction -- 4.2. Fixed Effect model -- 4.3. Random Effects model -- 4.3.1. predictive distribution -- 4.3.2. Prior specification for $\tau$ -- 4.3.3. 'Exact' Random Effects model for Odds Ratios based on a Binomial likelihood -- 4.3.4. Shrunken study level estimates -- 4.4. Publication bias -- 4.5. Study validity -- 4.6. Summary key points -- 4.7. Further reading -- 4.8. Exercises -- References -- 5.1. Introduction -- 5.2. Random effects meta-regression models -- 5.2.1. Generic random effect meta-regression model -- 5.2.2. Random effects meta-regression model for Odds Ratio (OR) outcomes using a Binomial likelihood -- 5.2.3. Autocorrelation and centring covariates -- 5.3. Limitations of meta-regression -- 5.4. Baseline risk -- 5.4.1. Model for including baseline risk in a meta-regression on the (log) OR scale -- 5.4.2. Final comments on including baseline risk as a covariate -- 5.5. Summary key points -- 5.6. Further reading -- 5.7. Exercises -- References -- 6.1. Introduction -- 6.2. Random Effects model revisited -- 6.3. Assessing model fit -- 6.3.1. Deviance -- 6.3.2. Residual deviance -- 6.4. Model comparison -- 6.4.1. Effective number of parameters, pD -- 6.4.2. Deviance Information Criteria -- 6.5. Exploring inconsistency -- 6.5.1. Cross-validation -- 6.5.2. Mixed predictive checks -- 6.6. Summary key points -- 6.7. Further reading -- 6.8. Exercises -- References -- 7.1. Introduction -- 7.2. Evaluation of decision models: One-stage vs two-stage approach -- 7.3. Sensitivity analyses (of model inputs and model specifications) -- 7.4. Summary key points -- 7.5. Further reading -- 7.6. Exercises -- References -- 8.1. Introduction -- 8.2. Prior and posterior simulation in a probabilistic model: Maple Syrup Urine Disease (MSUD) -- 8.3. model for prenatal HIV testing -- 8.4. Model criticism in multi-parameter models -- 8.5. Evidence-based policy -- 8.6. Summary key points -- 8.7. Further reading -- 8.8. Exercises -- References -- 9.1. Why go beyond 'direct' head-to-head trials-- 9.2. fixed treatment effects model for MTC -- 9.2.1. Absolute treatment effects -- 9.2.2. Relative treatment efficacy and ranking -- 9.3. Random Effects MTC models -- 9.4. Model choice and consistency of MTC evidence -- 9.4.1. Techniques for presenting and understanding the results of MTC -- 9.5. Multi-arm trials -- 9.6. Assumptions made in mixed treatment comparisons -- 9.7. Embedding an MTC within a cost-effectiveness analysis -- 9.8. Extension to continuous, rate and other outcomes -- 9.9. Summary key points -- 9.10. Further reading -- 9.11. Exercises -- References -- 10.1. Introduction -- 10.2. Continuous and discrete time Markov models -- 10.3. Decision analysis with Markov models -- 10.3.1. Evaluating Markov models -- 10.4. Estimating transition parameters from a single study -- 10.4.1. Likelihood -- 10.4.2. Priors and posteriors for multinomial probabilities -- 10.5. Propagating uncertainty in Markov parameters into a decision model -- 10.6. Estimating transition parameters from a synthesis of several studies -- 10.6.1. Challenges for meta-analysis of evidence on Markov transition parameters -- 10.6.2. relationship between probabilities and rates -- 10.6.3. Modelling study effects -- 10.6.4. Synthesis of studies reporting aggregate data -- 10.6.5. Incorporating studies that provide event history data -- 10.6.6. Reporting results from a Random Effects model -- 10.6.7. Incorporating treatment effects -- 10.7. Summary key points -- 10.8. Further reading -- 10.9. Exercises -- References -- 11.1. Introduction -- 11.2. Deriving a prior distribution from observational evidence -- 11.3. Bias allowance model for the observational data -- 11.4. Hierarchical models for evidence from different study designs -- 11.5. Discussion -- 11.6. Summary key points -- 11.7. Further reading -- 11.8. Exercises -- References -- 12.1. Introduction -- 12.2. Expected value of perfect information -- 12.3. Expected value of partial perfect information -- 12.3.1. Computation -- 12.3.2. Notes on EVPPI -- 12.4. Expected value of sample information -- 12.4.1. Computation -- 12.5. Expected net benefit of sampling -- 12.6. Summary key points -- 12.7. Further reading -- 12.8. Exercises -- References -- A2.1. Normal distribution -- A2.2. Binomial distribution -- A2.3. Multinomial distribution -- A2.4. Uniform distribution -- A2.5. Exponential distribution -- A2.6. Gamma distribution -- A2.7. Beta distribution -- A2.8. Dirichlet distribution.},
author = {Welton, Nicky J. and Sutton, Alexander J. and Cooper, Nicola and Abrams, Keith R. and Ades, A. E.},
isbn = {9780470061091},
publisher = {Wiley},
title = {{Evidence synthesis for decision making in healthcare}},
url = {https://www.wiley.com/en-gb/Evidence+Synthesis+for+Decision+Making+in+Healthcare-p-9780470061091},
year = {2012}
}
@article{Segler2018,
abstract = {Chemical reaction databases that are automatically filled from the literature have made the planning of chemical syntheses, whereby target molecules are broken down into smaller and smaller building blocks, vastly easier over the past few decades. However, humans must still search these databases manually to find the best way to make a molecule. This involves many steps and choices. Some degree of automation has been achieved by encoding 'rules' of synthesis into computer programs, but this is time consuming owing to the numerous rules and subtleties involved. Here, Mark Waller and colleagues apply deep neural networks to plan chemical syntheses. They trained an algorithm on essentially every reaction published before 2015 so that it could learn the 'rules' itself and then predict synthetic routes to various small molecules not included in the training set. In blind testing, trained chemists could not distinguish between the solutions found by the algorithm and those taken from the literature.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Segler, Marwin H.S. and Preuss, Mike and Waller, Mark P.},
doi = {10.1038/nature25978},
eprint = {NIHMS150003},
isbn = {0008-5472 (Print)$\backslash$r0008-5472 (Linking)},
issn = {14764687},
journal = {Nature},
keywords = {Automation,Cheminformatics,Computer science,Synthetic chemistry methodology},
month = {mar},
number = {7698},
pages = {604--610},
pmid = {29595767},
publisher = {Nature Publishing Group},
title = {{Planning chemical syntheses with deep neural networks and symbolic AI}},
url = {http://www.nature.com/doifinder/10.1038/nature25978},
volume = {555},
year = {2018}
}
@article{Velickovic2016b,
abstract = {MOTIVATION: With the development of experimental methods and technology, we are able to reliably gain access to data in larger quantities, dimensions and types. This has great potential for the improvement of machine learning (as the learning algorithms have access to a larger space of information). However, conventional machine learning approaches used thus far on single-dimensional data inputs are unlikely to be expressive enough to accurately model the problem in higher dimensions; in fact, it should generally be most suitable to represent our underlying models as some form of complex networksng;nsio with nontrivial topological features. As the first step in establishing such a trend, we present MUXSTEP: , an open-source library utilising multiplex networks for the purposes of binary classification on multiple data types. The library is designed to be used out-of-the-box for developing models based on the multiplex network framework, as well as easily modifiable to suit problem modelling needs that may differ significantly from the default approach described. AVAILABILITY AND IMPLEMENTATION: The full source code is available on GitHub: https://github.com/PetarV-/muxstep CONTACT: petar.velickovic@cl.cam.ac.uk SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
author = {Veli{\v{c}}kovi{\'{c}}, Petar and Li{\`{o}}, Pietro},
doi = {10.1093/bioinformatics/btw196},
issn = {14602059},
journal = {Bioinformatics},
month = {aug},
number = {16},
pages = {2562--2564},
publisher = {Oxford University Press},
title = {{Muxstep: An open-source C ++ multiplex HMM library for making inferences on multiple data types}},
url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btw196},
volume = {32},
year = {2016}
}
@article{Chawla2017,
abstract = {Consensus definitions have been reached for both acute kidney injury (AKI) and chronic kidney disease (CKD) and these definitions are now routinely used in research and clinical practice. The KDIGO guideline defines AKI as an abrupt decrease in kidney function occurring over 7 days or less, whereas CKD is defined by the persistence of kidney disease for a period of {\textgreater}90 days. AKI and CKD are increasingly recognized as related entities and in some instances probably represent a continuum of the disease process. For patients in whom pathophysiologic processes are ongoing, the term acute kidney disease (AKD) has been proposed to define the course of disease after AKI; however, definitions of AKD and strategies for the management of patients with AKD are not currently available. In this consensus statement, the Acute Disease Quality Initiative (ADQI) proposes definitions, staging criteria for AKD, and strategies for the management of affected patients. We also make recommendations for areas of future research, which aim to improve understanding of the underlying processes and improve outcomes for patients with AKD.},
author = {Chawla, Lakhmir S. and Bellomo, Rinaldo and Bihorac, Azra and Goldstein, Stuart L. and Siew, Edward D. and Bagshaw, Sean M. and Bittleman, David and Cruz, Dinna and Endre, Zoltan and Fitzgerald, Robert L. and Forni, Lui and Kane-Gill, Sandra L. and Hoste, Eric and Koyner, Jay and Liu, Kathleen D. and MacEdo, Etienne and Mehta, Ravindra and Murray, Patrick and Nadim, Mitra and Ostermann, Marlies and Palevsky, Paul M. and Pannu, Neesh and Rosner, Mitchell and Wald, Ron and Zarbock, Alexander and Ronco, Claudio and Kellum, John A.},
doi = {10.1038/nrneph.2017.2},
issn = {1759507X},
journal = {Nat. Rev. Nephrol.},
keywords = {Acute kidney injury,Chronic kidney disease,Prognosis},
month = {apr},
number = {4},
pages = {241--257},
publisher = {Nature Publishing Group},
title = {{Acute kidney disease and renal recovery: Consensus report of the Acute Disease Quality Initiative (ADQI) 16 Workgroup}},
url = {http://www.nature.com/articles/nrneph.2017.2},
volume = {13},
year = {2017}
}
@article{Townes2019,
abstract = {Single cell RNA-Seq (scRNA-Seq) profiles gene expression of individual cells. Recent scRNA-Seq datasets have incorporated unique molecular identifiers (UMIs). Using negative controls, we show UMI counts follow multinomial sampling with no zero-inflation. Current normalization pro-cedures such as log of counts per million and feature selection by highly variable genes produce false variability in dimension reduction. We pro-pose simple multinomial methods, including generalized principal component analysis (GLM-PCA) for non-normal distributions, and feature selection using deviance. These methods outperform current practice in a downstream clustering assessment using ground-truth datasets.},
author = {Townes, F. William and Hicks, Stephanie C. and Aryee, Martin J. and Irizarry, Rafael A.},
doi = {10.1101/574574},
journal = {bioRxiv},
month = {mar},
pages = {574574},
publisher = {Cold Spring Harbor Laboratory},
title = {{Feature Selection and Dimension Reduction for Single Cell RNA-Seq based on a Multinomial Model}},
url = {https://www.biorxiv.org/content/10.1101/574574v1 https://www.biorxiv.org/content/10.1101/574574v1.abstract},
year = {2019}
}
@article{McAvoy2019,
abstract = {Prosocial behavior, which means paying a cost for others to receive a benefit, is encountered in the donation game, the prisoner's dilemma, relaxed social dilemmas, and public goods games. Many studies of prosociality assume that the population structure is either homogeneous, meaning all individuals have the same number of interaction partners, or that the social good is of one particular type. Here, we study general evolutionary dynamics for arbitrary kinds of social goods. We find that heterogeneous population structures, where some individuals have many more interaction partners than others, are extremely conducive for the evolution of prosocial behaviors. Furthermore, prosocial behaviors can evolve that accumulate most of the benefit in a few highly connected nodes while many peripheral nodes receive low or negative payoff. Surprisingly, prosociality can evolve even if the total costs exceed the total benefits. Therefore, the highly heterogeneous interaction structure of human society, which is augmented by the internet, strongly promotes the emergence of prosocial behaviors but also creates the possibility of generating tremendous inequality.},
archivePrefix = {arXiv},
arxivId = {1909.10139},
author = {McAvoy, Alex and Allen, Benjamin and Nowak, Martin A.},
eprint = {1909.10139},
month = {sep},
title = {{Social goods dilemmas in heterogeneous societies}},
url = {http://arxiv.org/abs/1909.10139},
year = {2019}
}
@article{Wang2019a,
abstract = {Accelerating research in the emerging field of deep graph learning requires new tools. Such systems should support graph as the core abstraction and take care to maintain both forward (i.e. supporting new research ideas) and backward (i.e. in- tegration with existing components) compatibility. In this paper, we present Deep Graph Library (DGL). DGL enables arbitrary message handling and mutation op- erators, flexible propagation rules, and is framework agnostic so as to leverage high-performance tensor, autograd operations, and other feature extraction mod- ules already available in existing frameworks. DGL carefully handles the sparse and irregular graph structure, deals with graphs big and small which may change dynamically, fuses operations, and performs auto-batching, all to take advantages of modern hardware. DGL has been tested on a variety of models, including but not limited to the popular Graph Neural Networks (GNN) and its variants, with promising speed, memory footprint and scalability.},
archivePrefix = {arXiv},
arxivId = {1909.01315},
author = {Wang, Minjie and Yu, Lingfan and Zheng, Da and Gan, Quan and Gai, Yu and Ye, Zihao and Li, Mufei and Zhou, Jinjing and Huang, Qi and Ma, Chao and Huang, Ziyue and Guo, Qipeng and Zhang, Hao and Lin, Haibin and Zhao, Junbo and Li, Jinyang and Smola, Alexander and Zhang, Zheng},
eprint = {1909.01315},
journal = {ICLR 2019 Work. Represent. Learn. Graphs Manifolds},
month = {sep},
pages = {1--7},
title = {{Deep Graph Library: Towards Efficient and Scalable Deep Learning on Graphs}},
url = {http://arxiv.org/abs/1909.01315},
year = {2019}
}
@article{Olfson2015,
abstract = {IMPORTANCE Although adults with schizophrenia have a significantly increased risk of premature mortality, sample size limitations of previous research have hindered the identification of the underlying causes. OBJECTIVE To describe overall and cause-specific mortality rates and standardized mortality ratios (SMRs) for adults with schizophrenia compared with the US general population. DESIGN, SETTING, AND PARTICIPANTS We identified a national retrospective longitudinal cohort of patients with schizophrenia 20 to 64 years old in the Medicaid program (January 1, 2001, to December 31, 2007). The cohort included 1 138 853 individuals, 4 807 121 years of follow-up, and 74 003 deaths, of which 65 553 had a known cause. MAIN OUTCOMES AND MEASURES Mortality ratios for the schizophrenia cohort standardized to the general population with respect to age, sex, race/ethnicity, and geographic region were estimated for all-cause and cause-specific mortality. Mortality rates per 100 000 person-years and the mean years of potential life lost per death were also determined. Death record information was obtained from the National Death Index. RESULTS Adults with schizophrenia were more than 3.5 times (all-cause SMR, 3.7; 95{\%} CI, 3.7-3.7) as likely to die in the follow-up period as were adults in the general population. Cardiovascular disease had the highest mortality rate (403.2 per 100 000 person-years) and an SMR of 3.6 (95{\%} CI, 3.5-3.6). Among 6 selected cancers, lung cancer had the highest mortality rate (74.8 per 100 000 person-years) and an SMR of 2.4 (95{\%} CI, 2.4-2.5). Particularly elevated SMRs were observed for chronic obstructive pulmonary disease (9.9; 95{\%} CI, 9.6-10.2) and influenza and pneumonia (7.0; 95{\%} CI, 6.7-7.4). Accidental deaths (119.7 per 100 000 person-years) accounted for more than twice as many deaths as suicide (52.0 per 100 000 person-years). Nonsuicidal substance-induced death, mostly from alcohol or other drugs, was also a leading cause of death (95.2 per 100 000 person-years). CONCLUSIONS AND RELEVANCE In a US national cohort of adults with schizophrenia, excess deaths from cardiovascular and respiratory diseases implicate modifiable cardiovascular risk factors, including especially tobacco use. Excess deaths directly attributable to alcohol or other drugs highlight threats posed by substance abuse. More aggressive identification and management of cardiovascular risk factors, as well as reducing tobacco use and substance abuse, should be leading priorities in the medical care of adults with schizophrenia.},
author = {Olfson, Mark and Gerhard, Tobias and Huang, Cecilia and Crystal, Stephen and Stroup, T. Scott},
doi = {10.1001/jamapsychiatry.2015.1737},
isbn = {2168-622x},
issn = {2168622X},
journal = {JAMA Psychiatry},
month = {dec},
number = {12},
pages = {1172--1181},
pmid = {26509694},
title = {{Premature mortality among adults with schizophrenia in the United States}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26509694 http://archpsyc.jamanetwork.com/article.aspx?doi=10.1001/jamapsychiatry.2015.1737},
volume = {72},
year = {2015}
}
@article{Ying2018a,
abstract = {Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. We deploy PinSage at Pinterest and train it on 7.5 billion examples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage generates higher-quality recommendations than comparable deep learning and graph-based alternatives. To our knowledge, this is the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.},
archivePrefix = {arXiv},
arxivId = {1806.01973},
author = {Ying, Rex and He, Ruining and Chen, Kaifeng and Eksombatchai, Pong and Hamilton, William L. and Leskovec, Jure},
doi = {10.1145/3219819.3219890},
eprint = {1806.01973},
isbn = {9781450355520},
issn = {16130073},
month = {jun},
pmid = {28693034},
title = {{Graph Convolutional Neural Networks for Web-Scale Recommender Systems}},
url = {http://arxiv.org/abs/1806.01973 http://dx.doi.org/10.1145/3219819.3219890 http://arxiv.org/abs/1806.01973{\%}0Ahttp://dx.doi.org/10.1145/3219819.3219890},
year = {2018}
}
@incollection{S.2009,
author = {S., Heder and G., Leonardo and {C. Barbos}, Helio J.},
booktitle = {Evol. Comput.},
doi = {10.5772/9618},
month = {oct},
publisher = {InTech},
title = {{Surrogate-Assisted Artificial Immune Systems for Expensive Optimization Problems}},
url = {http://www.intechopen.com/books/evolutionary-computation/surrogate-assisted-artificial-immune-systems-for-expensive-optimization-problems},
year = {2009}
}
@article{Ioffe2015a,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
eprint = {1502.03167},
month = {feb},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@inproceedings{Li2018a,
abstract = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
archivePrefix = {arXiv},
arxivId = {1712.09913},
author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
booktitle = {Adv. Neural Inf. Process. Syst.},
eprint = {1712.09913},
issn = {10495258},
month = {dec},
pages = {6389--6399},
title = {{Visualizing the loss landscape of neural nets}},
url = {http://arxiv.org/abs/1712.09913},
volume = {2018-Decem},
year = {2018}
}
@article{Levey2006,
abstract = {BACKGROUND Glomerular filtration rate (GFR) estimates facilitate detection of chronic kidney disease but require calibration of the serum creatinine assay to the laboratory that developed the equation. The 4-variable equation from the Modification of Diet in Renal Disease (MDRD) Study has been reexpressed for use with a standardized assay. OBJECTIVE To describe the performance of the revised 4-variable MDRD Study equation and compare it with the performance of the 6-variable MDRD Study and Cockcroft-Gault equations. DESIGN Comparison of estimated and measured GFR. SETTING 15 clinical centers participating in a randomized, controlled trial. PATIENTS 1628 patients with chronic kidney disease participating in the MDRD Study. MEASUREMENTS Serum creatinine levels were calibrated to an assay traceable to isotope-dilution mass spectrometry. Glomerular filtration rate was measured as urinary clearance of 125I-iothalamate. RESULTS Mean measured GFR was 39.8 mL/min per 1.73 m2 (SD, 21.2). Accuracy and precision of the revised 4-variable equation were similar to those of the original 6-variable equation and better than in the Cockcroft-Gault equation, even when the latter was corrected for bias, with 90{\%}, 91{\%}, 60{\%}, and 83{\%} of estimates within 30{\%} of measured GFR, respectively. Differences between measured and estimated GFR were greater for all equations when the estimated GFR was 60 mL/min per 1.73 m2 or greater. LIMITATIONS The MDRD Study included few patients with a GFR greater than 90 mL/min per 1.73 m2. Equations were not compared in a separate study sample. CONCLUSIONS The 4-variable MDRD Study equation provides reasonably accurate GFR estimates in patients with chronic kidney disease and a measured GFR of less than 90 mL/min per 1.73 m2. By using the reexpressed MDRD Study equation with the standardized serum creatinine assay, clinical laboratories can report more accurate GFR estimates.},
author = {Levey, Andrew S and Coresh, Josef and Greene, Tom and Stevens, Lesley A and Zhang, Yaping Lucy and Hendriksen, Stephen and Kusek, John W and {Van Lente}, Frederick and {Chronic Kidney Disease Epidemiology Collaboration}},
issn = {1539-3704},
journal = {Ann. Intern. Med.},
month = {aug},
number = {4},
pages = {247--54},
pmid = {16908915},
title = {{Using standardized serum creatinine values in the modification of diet in renal disease study equation for estimating glomerular filtration rate.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16908915},
volume = {145},
year = {2006}
}
@article{Ceschin2016,
abstract = {The paper explores the evolution of Design for Sustainability (DfS). Following a quasi-chronological pattern, our exploration provides an overview of the DfS field, categorising the design approaches developed in the past decades under four innovation levels: Product, Product-Service System, Spatio-Social and Socio-Technical System. As a result, we propose an evolutionary framework and map the reviewed DfS approaches onto this framework. The proposed framework synthesizes the evolution of the DfS field, showing how it has progressively expanded from a technical and product-centric focus towards large scale system level changes in which sustainability is understood as a socio-technical challenge. The framework also shows how the various DfS approaches contribute to particular sustainability aspects and visualises linkages, overlaps and complementarities between these approaches.},
author = {Ceschin, Fabrizio and Gaziulusoy, Idil},
doi = {10.1016/j.destud.2016.09.002},
issn = {0142694X},
journal = {Des. Stud.},
keywords = {design for sustainability,design research,innovation,literature review,product design},
month = {nov},
pages = {118--163},
publisher = {Elsevier},
title = {{Evolution of design for sustainability: From product design to design for system innovations and transitions}},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X16300631},
volume = {47},
year = {2016}
}
@article{Srinivas2019,
abstract = {We introduce a new tool for interpreting neural net responses, namely full-gradients, which decomposes the neural net response into input sensitivity and per-neuron sensitivity components. This is the first proposed representation which satisfies two key properties: completeness and weak dependence, which provably cannot be satisfied by any saliency map-based interpretability method. For convolutional nets, we also propose an approximate saliency map representation, called FullGrad, obtained by aggregating the full-gradient components. We experimentally evaluate the usefulness of FullGrad in explaining model behaviour with two quantitative tests: pixel perturbation and remove-and-retrain. Our experiments reveal that our method explains model behaviour correctly, and more comprehensively than other methods in the literature. Visual inspection also reveals that our saliency maps are sharper and more tightly confined to object regions than other methods.},
archivePrefix = {arXiv},
arxivId = {1905.00780},
author = {Srinivas, Suraj and Fleuret, Francois},
eprint = {1905.00780},
month = {may},
title = {{Full-Gradient Representation for Neural Network Visualization}},
url = {http://arxiv.org/abs/1905.00780},
year = {2019}
}
@article{Muller2018,
abstract = {We propose an evolutionary perspective to classify and characterize the diverse systems of adaptive immunity that have been discovered across all major domains of life. We put forward a new function-based classification according to the way information is acquired by the immune systems: Darwinian immunity (currently known from, but not necessarily limited to, vertebrates) relies on the Darwinian process of clonal selection to ‘learn' by cumulative trial-and-error feedback; Lamarckian immunity uses templated targeting (guided adaptation) to internalize heritable information on potential threats; finally, shotgun immunity operates through somatic mechanisms of variable targeting without feedback. We argue that the origin of Darwinian (but not Lamarckian or shotgun) immunity represents a radical innovation in the evolution of individuality and complexity, and propose to add it to the list of major evolutionary transitions. While transitions to higher-level units entail the suppression of selection at lower levels, Darwinian immunity re-opens cell-level selection within the multicellular organism, under the control of mechanisms that direct, rather than suppress, cell-level evolution for the benefit of the individual. From a conceptual point of view, the origin of Darwinian immunity can be regarded as the most radical transition in the history of life, in which evolution by natural selection has literally re-invented itself. Furthermore, the combination of clonal selection and somatic receptor diversity enabled a transition from limited to practically unlimited capacity to store information about the antigenic environment. The origin of Darwinian immunity therefore comprises both a transition in individuality and the emergence of a new information system – the two hallmarks of major evolutionary transitions. Finally, we present an evolutionary scenario for the origin of Darwinian immunity in vertebrates. We propose a revival of the concept of the ‘Big Bang' of vertebrate immunity, arguing that its origin involved a ‘difficult' (i.e. low-probability) evolutionary transition that might have occurred only once, in a common ancestor of all vertebrates. In contrast to the original concept, we argue that the limiting innovation was not the generation of somatic diversity, but the regulatory circuitry needed for the safe operation of amplifiable immune responses with somatically acquired targeting. Regulatory complexity increased abruptly by genomic duplications at the root of the vertebrate lineage, creating a rare opportunity to establish such circuitry. We discuss the selection forces that might have acted at the origin of the transition, and in the subsequent stepwise evolution leading to the modern immune systems of extant vertebrates.},
author = {M{\"{u}}ller, Viktor and de Boer, Rob J. and Bonhoeffer, Sebastian and Szathm{\'{a}}ry, E{\"{o}}rs},
doi = {10.1111/brv.12355},
issn = {1469185X},
journal = {Biol. Rev.},
keywords = {Darwinian immunity,Lamarckian immunity,adaptive immunity,evolutionary scenario,major evolutionary transition,shotgun immunity},
month = {feb},
number = {1},
pages = {505--528},
title = {{An evolutionary perspective on the systems of adaptive immunity}},
url = {http://doi.wiley.com/10.1111/brv.12355},
volume = {93},
year = {2018}
}
